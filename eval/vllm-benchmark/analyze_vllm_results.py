#!/usr/bin/env python3
"""
VLLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
VLLM benchmark_serving.py ì¶œë ¥ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""
import json
import os
import sys
import glob
from pathlib import Path
from typing import Dict, List, Any, Optional
import statistics
from datetime import datetime

def load_vllm_benchmark_results(path: str) -> Dict[str, Any]:
    """VLLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¡œë“œ (íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬)"""
    results = {}
    json_files = []
    
    # ì…ë ¥ì´ íŒŒì¼ì¸ì§€ ë””ë ‰í† ë¦¬ì¸ì§€ í™•ì¸
    if os.path.isfile(path):
        if path.endswith('.json'):
            json_files = [path]
        else:
            print(f"âš ï¸  JSON íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {path}")
            return results
    elif os.path.isdir(path):
        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° JSON íŒŒì¼ë“¤ ì°¾ê¸°
        json_files = glob.glob(f"{path}/**/*.json", recursive=True)
        if not json_files:
            # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œë„ ì°¾ê¸°
            json_files = glob.glob(f"{path}/*.json")
    else:
        print(f"âš ï¸  ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
        return results
    
    if not json_files:
        print(f"âš ï¸  JSON ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
        return results
    
    print(f"ğŸ“ ë°œê²¬ëœ JSON íŒŒì¼ë“¤: {len(json_files)}ê°œ")
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # íŒŒì¼ëª…ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì¶”ì¶œ
            scenario_name = Path(json_file).stem
            results[scenario_name] = data
            print(f"âœ… {scenario_name}: ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {json_file}: ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨ - {e}")
    
    return results

def analyze_vllm_performance(results: Dict[str, Any]) -> None:
    """VLLM ì„±ëŠ¥ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "="*70)
    print("ğŸ“Š VLLM ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼")
    print("="*70)
    
    summary = []
    
    for scenario_name, data in results.items():
        print(f"\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤: {scenario_name}")
        print("-" * 50)
        
        # ê¸°ë³¸ í†µê³„
        completed_requests = data.get('completed', 0)
        total_requests = data.get('total_requests', completed_requests)
        success_rate = completed_requests / total_requests if total_requests > 0 else 0
        
        print(f"âœ… ì„±ê³µë¥ : {success_rate:.1%} ({completed_requests}/{total_requests})")
        print(f"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {data.get('duration', 0):.2f}ì´ˆ")
        
        # ì²˜ë¦¬ëŸ‰ ë©”íŠ¸ë¦­
        request_throughput = data.get('request_throughput', 0)
        input_throughput = data.get('input_throughput', 0)
        output_throughput = data.get('output_throughput', 0)
        
        print(f"ğŸ”„ ìš”ì²­ ì²˜ë¦¬ëŸ‰: {request_throughput:.2f} req/s")
        print(f"ğŸ“¥ ì…ë ¥ í† í° ì²˜ë¦¬ëŸ‰: {input_throughput:.1f} tok/s")
        print(f"ğŸ“¤ ì¶œë ¥ í† í° ì²˜ë¦¬ëŸ‰: {output_throughput:.1f} tok/s")
        
        # ì§€ì—°ì‹œê°„ ë©”íŠ¸ë¦­ (TTFT - Time to First Token)
        ttft_mean = data.get('mean_ttft_ms', 0)
        ttft_median = data.get('median_ttft_ms', 0)
        ttft_p95 = data.get('p95_ttft_ms', 0)
        ttft_p99 = data.get('p99_ttft_ms', 0)
        
        print(f"ğŸš€ TTFT í‰ê· : {ttft_mean:.1f}ms")
        print(f"ğŸš€ TTFT ì¤‘ì•™ê°’: {ttft_median:.1f}ms")
        print(f"ğŸš€ TTFT P95: {ttft_p95:.1f}ms")
        print(f"ğŸš€ TTFT P99: {ttft_p99:.1f}ms")
        
        # TPOT (Time per Output Token)
        tpot_mean = data.get('mean_tpot_ms', 0)
        tpot_median = data.get('median_tpot_ms', 0)
        tpot_p95 = data.get('p95_tpot_ms', 0)
        tpot_p99 = data.get('p99_tpot_ms', 0)
        
        print(f"âš¡ TPOT í‰ê· : {tpot_mean:.1f}ms")
        print(f"âš¡ TPOT ì¤‘ì•™ê°’: {tpot_median:.1f}ms")
        print(f"âš¡ TPOT P95: {tpot_p95:.1f}ms")
        print(f"âš¡ TPOT P99: {tpot_p99:.1f}ms")
        
        # ITL (Inter-token Latency)
        itl_mean = data.get('mean_itl_ms', 0)
        itl_p95 = data.get('p95_itl_ms', 0)
        
        if itl_mean > 0:
            print(f"ğŸ”— ITL í‰ê· : {itl_mean:.1f}ms")
            print(f"ğŸ”— ITL P95: {itl_p95:.1f}ms")
        
        # E2E Latency (End-to-End)
        e2el_mean = data.get('mean_e2el_ms', 0)
        e2el_median = data.get('median_e2el_ms', 0)
        e2el_p95 = data.get('p95_e2el_ms', 0)
        e2el_p99 = data.get('p99_e2el_ms', 0)
        
        print(f"ğŸ”„ E2E í‰ê· : {e2el_mean:.1f}ms")
        print(f"ğŸ”„ E2E ì¤‘ì•™ê°’: {e2el_median:.1f}ms")
        print(f"ğŸ”„ E2E P95: {e2el_p95:.1f}ms")
        print(f"ğŸ”„ E2E P99: {e2el_p99:.1f}ms")
        
        # í† í° í†µê³„
        input_tokens = data.get('total_input_tokens', 0)
        output_tokens = data.get('total_output_tokens', 0)
        
        if input_tokens > 0 or output_tokens > 0:
            print(f"ğŸ“Š ì´ ì…ë ¥ í† í°: {input_tokens:,}")
            print(f"ğŸ“Š ì´ ì¶œë ¥ í† í°: {output_tokens:,}")
            print(f"ğŸ“Š í‰ê·  ì…ë ¥ ê¸¸ì´: {input_tokens/completed_requests:.1f}" if completed_requests > 0 else "")
            print(f"ğŸ“Š í‰ê·  ì¶œë ¥ ê¸¸ì´: {output_tokens/completed_requests:.1f}" if completed_requests > 0 else "")
        
        # ìš”ì•½ ë°ì´í„° ìˆ˜ì§‘
        summary.append({
            'scenario': scenario_name,
            'success_rate': success_rate,
            'request_throughput': request_throughput,
            'output_throughput': output_throughput,
            'ttft_p95': ttft_p95,
            'tpot_mean': tpot_mean,
            'e2el_p95': e2el_p95
        })
    
    # ì „ì²´ ìš”ì•½ ë° ì„±ëŠ¥ í‰ê°€
    if summary:
        print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
        print("-" * 50)
        
        avg_success_rate = statistics.mean([s['success_rate'] for s in summary])
        avg_request_throughput = statistics.mean([s['request_throughput'] for s in summary])
        avg_output_throughput = statistics.mean([s['output_throughput'] for s in summary])
        avg_ttft_p95 = statistics.mean([s['ttft_p95'] for s in summary])
        avg_tpot_mean = statistics.mean([s['tpot_mean'] for s in summary])
        avg_e2el_p95 = statistics.mean([s['e2el_p95'] for s in summary])
        
        print(f"í‰ê·  ì„±ê³µë¥ : {avg_success_rate:.1%}")
        print(f"í‰ê·  ìš”ì²­ ì²˜ë¦¬ëŸ‰: {avg_request_throughput:.2f} req/s")
        print(f"í‰ê·  ì¶œë ¥ í† í° ì²˜ë¦¬ëŸ‰: {avg_output_throughput:.1f} tok/s")
        print(f"í‰ê·  TTFT P95: {avg_ttft_p95:.1f}ms")
        print(f"í‰ê·  TPOT: {avg_tpot_mean:.1f}ms")
        print(f"í‰ê·  E2E P95: {avg_e2el_p95:.1f}ms")
        
        # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
        grade, issues = evaluate_performance(
            avg_success_rate, avg_output_throughput, 
            avg_ttft_p95, avg_tpot_mean, avg_e2el_p95
        )
        
        print(f"\nğŸ† ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        if issues:
            print("âš ï¸  ê°œì„  í•„ìš” ì‚¬í•­:")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print("âœ¨ ëª¨ë“  ì„±ëŠ¥ ì§€í‘œê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤!")

def evaluate_performance(success_rate: float, throughput: float, 
                        ttft_p95: float, tpot_mean: float, e2el_p95: float) -> tuple:
    """ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€"""
    grade = "A"
    issues = []
    
    # ì„±ê³µë¥  ì²´í¬
    if success_rate < 0.95:
        grade = "C"
        issues.append(f"ë‚®ì€ ì„±ê³µë¥  ({success_rate:.1%})")
    elif success_rate < 0.98:
        grade = "B"
        issues.append(f"ë³´í†µ ì„±ê³µë¥  ({success_rate:.1%})")
    
    # ì²˜ë¦¬ëŸ‰ ì²´í¬
    if throughput < 10:
        grade = "C"
        issues.append(f"ë‚®ì€ ì²˜ë¦¬ëŸ‰ ({throughput:.1f} tok/s)")
    elif throughput < 50:
        grade = "B" if grade == "A" else grade
        issues.append(f"ë³´í†µ ì²˜ë¦¬ëŸ‰ ({throughput:.1f} tok/s)")
    
    # TTFT ì²´í¬
    if ttft_p95 > 500:
        grade = "C"
        issues.append(f"ë†’ì€ TTFT P95 ({ttft_p95:.1f}ms)")
    elif ttft_p95 > 200:
        grade = "B" if grade == "A" else grade
        issues.append(f"ë³´í†µ TTFT P95 ({ttft_p95:.1f}ms)")
    
    # TPOT ì²´í¬
    if tpot_mean > 100:
        grade = "C"
        issues.append(f"ë†’ì€ TPOT ({tpot_mean:.1f}ms)")
    elif tpot_mean > 50:
        grade = "B" if grade == "A" else grade
        issues.append(f"ë³´í†µ TPOT ({tpot_mean:.1f}ms)")
    
    # E2E ì§€ì—°ì‹œê°„ ì²´í¬
    if e2el_p95 > 5000:
        grade = "C"
        issues.append(f"ë†’ì€ E2E P95 ({e2el_p95:.1f}ms)")
    elif e2el_p95 > 2000:
        grade = "B" if grade == "A" else grade
        issues.append(f"ë³´í†µ E2E P95 ({e2el_p95:.1f}ms)")
    
    return grade, issues

def generate_summary_report(results: Dict[str, Any], output_file: str) -> None:
    """ìš”ì•½ ë¦¬í¬íŠ¸ JSON ìƒì„±"""
    summary_data = {
        "timestamp": datetime.now().isoformat(),
        "total_scenarios": len(results),
        "scenarios": {}
    }
    
    for scenario_name, data in results.items():
        summary_data["scenarios"][scenario_name] = {
            "success_rate": data.get('completed', 0) / max(data.get('total_requests', 1), 1),
            "request_throughput": data.get('request_throughput', 0),
            "output_throughput": data.get('output_throughput', 0),
            "ttft_p95_ms": data.get('p95_ttft_ms', 0),
            "tpot_mean_ms": data.get('mean_tpot_ms', 0),
            "e2el_p95_ms": data.get('p95_e2el_ms', 0)
        }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±: {output_file}")

def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python3 analyze_vllm_results.py <results_path> [output_summary.json]")
        print("ì˜ˆì‹œ: python3 analyze_vllm_results.py /results/benchmark.json summary.json")
        print("      python3 analyze_vllm_results.py /results summary.json")
        sys.exit(1)
    
    results_path = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    if not os.path.exists(results_path):
        print(f"âŒ ê²°ê³¼ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {results_path}")
        sys.exit(1)
    
    print(f"ğŸ“ ê²°ê³¼ ê²½ë¡œ: {results_path}")
    
    # ê²°ê³¼ ë¡œë“œ ë° ë¶„ì„
    results = load_vllm_benchmark_results(results_path)
    
    if not results:
        print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì„±ëŠ¥ ë¶„ì„
    analyze_vllm_performance(results)
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± (ì˜µì…˜)
    if output_file:
        generate_summary_report(results, output_file)

if __name__ == "__main__":
    main() 