{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VLLM Evaluation BenchmarkAutomated Model Evaluation &amp; Benchmarking","text":"<p>       A comprehensive Kubernetes-based evaluation system for VLLM serving models.       Continuous quality monitoring, performance benchmarking, and automated regression detection.     </p> Get Started CLI Guide View on GitHub"},{"location":"#key-features","title":"Key Features","text":"\ud83d\ude80 Multi-Framework Support <p>Unified interface for Evalchemy, NVIDIA Eval, VLLM Benchmark, and Deepeval frameworks</p> \u26a1 Performance Benchmarking <p>Comprehensive performance metrics: TTFT, TPOT, throughput analysis, and latency monitoring</p> \ud83d\udd04 Automated Workflows <p>Event-driven evaluation pipelines with Argo Workflows and Kubernetes orchestration</p> \ud83d\udcca Real-time Monitoring <p>Grafana dashboards, ClickHouse metrics storage, and Microsoft Teams notifications</p> \ud83c\udfaf Quality Assurance <p>Automated regression detection, quality metrics tracking, and continuous monitoring</p> \u2699\ufe0f Flexible Configuration <p>TOML-based profiles, environment-specific settings, and customizable evaluation parameters</p>"},{"location":"#evaluation-frameworks","title":"Evaluation Frameworks","text":"EvalchemyNVIDIA EvalVLLM BenchmarkDeepeval <p>Academic Benchmarks &amp; Standard Evaluations</p> <ul> <li>MMLU (Massive Multitask Language Understanding)</li> <li>HumanEval (Code Generation)</li> <li>ARC (AI2 Reasoning Challenge)</li> <li>HellaSwag (Commonsense Reasoning)</li> </ul> <pre><code>vllm-eval run evalchemy my-model --endpoint http://localhost:8000/v1\n</code></pre> <p>Mathematical Reasoning &amp; Coding</p> <ul> <li>AIME 2024 (Mathematical Problem Solving)</li> <li>LiveCodeBench (Live Coding Challenges)</li> <li>Advanced reasoning capabilities assessment</li> </ul> <pre><code>vllm-eval run nvidia my-model --benchmark aime --gpus 2\n</code></pre> <p>Performance &amp; Throughput Testing</p> <ul> <li>Time to First Token (TTFT)</li> <li>Time Per Output Token (TPOT)</li> <li>Concurrent request handling</li> <li>Latency analysis</li> </ul> <pre><code>vllm-eval run vllm-benchmark my-model --scenario performance\n</code></pre> <p>RAG &amp; Custom Metrics</p> <ul> <li>Context relevance evaluation</li> <li>Faithfulness assessment</li> <li>Answer relevancy scoring</li> <li>Custom metric development</li> </ul> <pre><code>vllm-eval run deepeval my-model --suite rag\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#cli-installation","title":"CLI Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/thakicloud/vllm-eval-public.git\ncd vllm-eval-public\n\n# Install CLI in development mode\npip install -e .\n\n# Run setup wizard\nvllm-eval setup\n\n# Check system status\nvllm-eval doctor\n</code></pre>"},{"location":"#first-evaluation","title":"First Evaluation","text":"<pre><code># Start your VLLM server\nvllm serve my-model --port 8000\n\n# Run Evalchemy evaluation\nvllm-eval run evalchemy my-model --endpoint http://localhost:8000/v1\n\n# View results\nvllm-eval results list\n</code></pre>"},{"location":"#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># Deploy with Kind\nmake kind-deploy\n\n# Install Helm charts\nmake helm-install\n\n# Submit evaluation workflow\nmake submit-workflow\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"Evaluation Frameworks\"\n        A[Evalchemy]\n        B[NVIDIA Eval]\n        C[VLLM Benchmark]\n        D[Deepeval]\n    end\n\n    subgraph \"CLI Interface\"\n        E[vllm-eval CLI]\n    end\n\n    subgraph \"Kubernetes Platform\"\n        F[Argo Workflows]\n        G[Argo Events]\n        H[Evaluation Runners]\n    end\n\n    subgraph \"Data &amp; Monitoring\"\n        I[ClickHouse]\n        J[Grafana]\n        K[Teams Notifications]\n    end\n\n    E --&gt; A\n    E --&gt; B  \n    E --&gt; C\n    E --&gt; D\n\n    G --&gt; F\n    F --&gt; H\n    H --&gt; A\n    H --&gt; B\n    H --&gt; C\n    H --&gt; D\n\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Model Development</p> <p>Run evaluations during model development to catch quality regressions early.</p> <pre><code>vllm-eval run all my-dev-model --config development\n</code></pre> <p>CI/CD Integration</p> <p>Integrate with build pipelines for automated quality gates.</p> <pre><code>vllm-eval run evalchemy $MODEL_NAME --config ci --batch-size 4\n</code></pre> <p>Production Monitoring</p> <p>Continuous monitoring of production models with alerts.</p> <pre><code># Kubernetes workflow triggered by GHCR image push\n# Automated evaluation \u2192 ClickHouse \u2192 Grafana \u2192 Teams\n</code></pre>"},{"location":"#getting-started","title":"Getting StartedReady to start evaluating your models?","text":"\ud83d\udcda User Guide <p>Learn how to configure evaluations, interpret results, and manage benchmarks</p> Read User Guide \u2192 \ud83d\udcbb Developer Guide <p>Set up development environment, contribute code, and extend functionality</p> Developer Setup \u2192 \ud83c\udfd7\ufe0f Architecture <p>Understand system design, components, and deployment architecture</p> System Overview \u2192 \ud83d\udd27 Operations <p>Deploy, monitor, troubleshoot, and scale the evaluation system</p> Operations Guide \u2192 <p>Get up and running with VLLM Evaluation Benchmark in minutes</p> Get Started Now"},{"location":"CONTRIBUTING/","title":"\uae30\uc5ec \uac00\uc774\ub4dc","text":"<p>VLLM \ubaa8\ub378 \uc131\ub2a5 \uc790\ub3d9 \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc5d0 \uae30\uc5ec\ud558\ub294 \ubc29\ubc95</p>"},{"location":"CONTRIBUTING/#_2","title":"\ud83d\ude4f \uae30\uc5ec\ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!","text":"<p>\uc774 \ud504\ub85c\uc81d\ud2b8\uc5d0 \uad00\uc2ec\uc744 \uac00\uc838\uc8fc\uc2dc\uace0 \uae30\uc5ec\ud558\uace0\uc790 \ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4. \uc774 \ubb38\uc11c\ub294 \ud6a8\uacfc\uc801\uc774\uace0 \uc77c\uad00\uc131 \uc788\ub294 \uae30\uc5ec\ub97c \uc704\ud55c \uac00\uc774\ub4dc\ub77c\uc778\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.</p>"},{"location":"CONTRIBUTING/#_3","title":"\ud83d\udccb \ubaa9\ucc28","text":"<ul> <li>\ud83d\udee0\ufe0f \uac1c\ubc1c \ud658\uacbd \uc124\uc815</li> <li>\ud83d\udd04 \uae30\uc5ec \ud504\ub85c\uc138\uc2a4</li> <li>\ud83c\udfa8 \ucf54\ub529 \ud45c\uc900</li> <li>\ud83e\uddea \ud14c\uc2a4\ud305 \uac00\uc774\ub4dc\ub77c\uc778</li> <li>\ud83d\udcda \ubb38\uc11c\ud654</li> <li>\ud83d\udc1b \uc774\uc288 \ub9ac\ud3ec\ud305</li> <li>\ud83d\udd0d Pull Request \uac00\uc774\ub4dc\ub77c\uc778</li> <li>\ud83d\udc65 \ucf54\ub4dc \ub9ac\ubdf0 \ud504\ub85c\uc138\uc2a4</li> </ul>"},{"location":"CONTRIBUTING/#_4","title":"\ud83d\udee0\ufe0f \uac1c\ubc1c \ud658\uacbd \uc124\uc815","text":""},{"location":"CONTRIBUTING/#_5","title":"\ud544\uc218 \uc694\uad6c\uc0ac\ud56d","text":"<ul> <li>Python: 3.9+</li> <li>Docker: 24.0+</li> <li>Kubernetes: 1.26+ (\ub85c\uceec \ud14c\uc2a4\ud2b8\uc6a9)</li> <li>Kind: 0.20+ (\ub85c\uceec \ud074\ub7ec\uc2a4\ud130)</li> <li>Helm: 3.12+</li> <li>Git: 2.30+</li> </ul>"},{"location":"CONTRIBUTING/#_6","title":"\ud658\uacbd \uad6c\uc131","text":"<pre><code># 1. \uc800\uc7a5\uc18c \ud074\ub860\ngit clone https://github.com/your-org/vllm-eval.git\ncd vllm-eval\n\n# 2. Python \uac00\uc0c1\ud658\uacbd \uc0dd\uc131 \ubc0f \ud65c\uc131\ud654\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# 3. \uac1c\ubc1c \uc758\uc874\uc131 \uc124\uce58\npip install -r requirements-dev.txt\npip install -r requirements-test.txt\n\n# 4. Pre-commit \ud6c5 \uc124\uce58\npre-commit install\n\n# 5. \ud14c\uc2a4\ud2b8 \uc2e4\ud589\uc73c\ub85c \uc124\uc815 \ud655\uc778\npytest eval/deepeval_tests/\n</code></pre>"},{"location":"CONTRIBUTING/#ide","title":"IDE \uc124\uc815","text":"<ul> <li>Interpreter: <code>./venv/bin/python</code></li> <li>Code style &amp; Linter: Ruff</li> <li>Test runner: pytest</li> </ul>"},{"location":"CONTRIBUTING/#_7","title":"\ud83d\udd04 \uae30\uc5ec \ud504\ub85c\uc138\uc2a4","text":""},{"location":"CONTRIBUTING/#1","title":"1. \uc774\uc288 \ud655\uc778 \ub610\ub294 \uc0dd\uc131","text":"<ul> <li>\uae30\uc874 \uc774\uc288\ub97c \uac80\uc0c9\ud574\ubcf4\uc138\uc694</li> <li>\uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc774\ub098 \ubc84\uadf8\ub294 \uc774\uc288\ub97c \uba3c\uc800 \uc0dd\uc131\ud574\uc8fc\uc138\uc694</li> <li>\uc774\uc288 \ud15c\ud50c\ub9bf\uc744 \uc0ac\uc6a9\ud574\uc8fc\uc138\uc694</li> </ul>"},{"location":"CONTRIBUTING/#2","title":"2. \ube0c\ub79c\uce58 \uc0dd\uc131","text":"<pre><code># \uba54\uc778 \ube0c\ub79c\uce58\uc5d0\uc11c \uc2dc\uc791\ngit checkout main\ngit pull origin main\n\n# \uae30\ub2a5 \ube0c\ub79c\uce58 \uc0dd\uc131\ngit checkout -b feature/your-feature-name\n# \ub610\ub294 \ubc84\uadf8 \uc218\uc815\ngit checkout -b fix/bug-description\n</code></pre>"},{"location":"CONTRIBUTING/#3","title":"3. \uac1c\ubc1c \ubc0f \ud14c\uc2a4\ud2b8","text":"<pre><code># \ucf54\ub4dc \ubcc0\uacbd \ud6c4 \ud14c\uc2a4\ud2b8\npytest\n\n# \ub9b0\ud305 \ubc0f \ud3ec\ub9f7\ud305\nruff check . --fix\nruff format .\n</code></pre>"},{"location":"CONTRIBUTING/#4","title":"4. \ucee4\ubc0b","text":"<pre><code># \ubcc0\uacbd\uc0ac\ud56d \ucee4\ubc0b (Conventional Commits \uaddc\uce59 \uc900\uc218)\ngit add .\ngit commit -m \"feat: add new evaluation metric for hallucination detection\"\n</code></pre>"},{"location":"CONTRIBUTING/#5-pull-request","title":"5. Pull Request \uc0dd\uc131","text":"<pre><code># \ube0c\ub79c\uce58 \ud478\uc2dc\ngit push origin feature/your-feature-name\n\n# GitHub\uc5d0\uc11c PR \uc0dd\uc131\n</code></pre>"},{"location":"CONTRIBUTING/#_8","title":"\ud83c\udfa8 \ucf54\ub529 \ud45c\uc900","text":""},{"location":"CONTRIBUTING/#python","title":"Python \ucf54\ub4dc \uc2a4\ud0c0\uc77c","text":"<p>Ruff\ub97c \uc0ac\uc6a9\ud55c \uc790\ub3d9 \ub9b0\ud305 \ubc0f \ud3ec\ub9f7\ud305\uc744 \ub530\ub985\ub2c8\ub2e4:</p> <pre><code># \u2705 \uc88b\uc740 \uc608\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom deepeval import BaseMetric\n\n\nclass HallucinationMetric(BaseMetric):\n    \"\"\"\ud658\uac01 \ud0d0\uc9c0\ub97c \uc704\ud55c \ucee4\uc2a4\ud140 \uba54\ud2b8\ub9ad.\n\n    Args:\n        threshold: \ud658\uac01 \ud310\uc815 \uc784\uacc4\uac12 (0.0-1.0)\n        model_name: \uc0ac\uc6a9\ud560 \ubaa8\ub378\uba85\n\n    Returns:\n        HallucinationScore: \ud658\uac01 \uc810\uc218 \uacb0\uacfc\n    \"\"\"\n\n    def __init__(self, threshold: float = 0.5, model_name: str = \"gpt-4\") -&gt; None:\n        self.threshold = threshold\n        self.model_name = model_name\n\n    def measure(self, test_case: Dict[str, str]) -&gt; float:\n        \"\"\"\ud658\uac01 \uc810\uc218\ub97c \uce21\uc815\ud569\ub2c8\ub2e4.\"\"\"\n        # \uad6c\ud604 \ub0b4\uc6a9\n        pass\n</code></pre>"},{"location":"CONTRIBUTING/#_9","title":"\uba85\uba85 \uaddc\uce59","text":"<ul> <li>\ud30c\uc77c\uba85: <code>snake_case.py</code></li> <li>\ud074\ub798\uc2a4\uba85: <code>PascalCase</code></li> <li>\ud568\uc218/\ubcc0\uc218\uba85: <code>snake_case</code></li> <li>\uc0c1\uc218\uba85: <code>UPPER_SNAKE_CASE</code></li> <li>Private \uba64\ubc84: <code>_leading_underscore</code></li> </ul>"},{"location":"CONTRIBUTING/#_10","title":"\ud0c0\uc785 \ud78c\ud305","text":"<p>\ubaa8\ub4e0 \ud568\uc218\uc5d0\ub294 \ud0c0\uc785 \ud78c\ud305\uc744 \ud3ec\ud568\ud574\uc57c \ud569\ub2c8\ub2e4:</p> <pre><code>from typing import Dict, List, Optional, Union\n\ndef evaluate_model(\n    model_endpoint: str,\n    dataset_path: str,\n    metrics: List[str],\n    config: Optional[Dict[str, Union[str, int]]] = None\n) -&gt; Dict[str, float]:\n    \"\"\"\ubaa8\ub378\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.\"\"\"\n    pass\n</code></pre>"},{"location":"CONTRIBUTING/#_11","title":"\uc5d0\ub7ec \ucc98\ub9ac","text":"<pre><code>import logging\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\nclass EvaluationError(Exception):\n    \"\"\"\ud3c9\uac00 \uacfc\uc815\uc5d0\uc11c \ubc1c\uc0dd\ud558\ub294 \uc5d0\ub7ec.\"\"\"\n    pass\n\ndef safe_evaluate(model_endpoint: str) -&gt; Optional[Dict[str, float]]:\n    \"\"\"\uc548\uc804\ud55c \ubaa8\ub378 \ud3c9\uac00.\"\"\"\n    try:\n        result = evaluate_model(model_endpoint)\n        return result\n    except ConnectionError as e:\n        logger.error(f\"\ubaa8\ub378 \uc5f0\uacb0 \uc2e4\ud328: {e}\")\n        return None\n    except EvaluationError as e:\n        logger.error(f\"\ud3c9\uac00 \uc2e4\ud328: {e}\")\n        raise\n</code></pre>"},{"location":"CONTRIBUTING/#_12","title":"\ud83e\uddea \ud14c\uc2a4\ud305 \uac00\uc774\ub4dc\ub77c\uc778","text":""},{"location":"CONTRIBUTING/#_13","title":"\ud14c\uc2a4\ud2b8 \uad6c\uc870","text":"<ul> <li>Deepeval \ud14c\uc2a4\ud2b8: <code>eval/deepeval_tests/</code> \ub514\ub809\ud1a0\ub9ac \uc548\uc5d0 Pytest \uae30\ubc18\uc758 \ud14c\uc2a4\ud2b8 \ucf54\ub4dc\uac00 \uc704\uce58\ud569\ub2c8\ub2e4.<ul> <li>\ucee4\uc2a4\ud140 \uba54\ud2b8\ub9ad \ud14c\uc2a4\ud2b8: <code>eval/deepeval_tests/test_custom_metric.py</code></li> <li>RAG \ud3c9\uac00 \ud14c\uc2a4\ud2b8: <code>eval/deepeval_tests/test_llm_rag.py</code></li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#_14","title":"\ud14c\uc2a4\ud2b8 \uc791\uc131 \uaddc\uce59","text":"<pre><code>import pytest\nfrom unittest.mock import Mock, patch\n\nfrom eval.deepeval_tests.metrics.rag_precision import RAGPrecisionMetric\n\n\nclass TestRAGPrecisionMetric:\n    \"\"\"RAG Precision \uba54\ud2b8\ub9ad \ud14c\uc2a4\ud2b8.\"\"\"\n\n    @pytest.fixture\n    def metric(self) -&gt; RAGPrecisionMetric:\n        \"\"\"\ud14c\uc2a4\ud2b8\uc6a9 \uba54\ud2b8\ub9ad \uc778\uc2a4\ud134\uc2a4.\"\"\"\n        return RAGPrecisionMetric(threshold=0.8)\n\n    def test_init_with_default_params(self) -&gt; None:\n        \"\"\"\uae30\ubcf8 \ud30c\ub77c\ubbf8\ud130\ub85c \ucd08\uae30\ud654 \ud14c\uc2a4\ud2b8.\"\"\"\n        metric = RAGPrecisionMetric()\n        assert metric.threshold == 0.7\n        assert metric.name == \"rag_precision\"\n\n    @patch('eval.deepeval_tests.metrics.rag_precision.openai_client')\n    def test_measure_high_precision(self, mock_client: Mock, metric: RAGPrecisionMetric) -&gt; None:\n        \"\"\"\ub192\uc740 \uc815\ubc00\ub3c4 \ucf00\uc774\uc2a4 \ud14c\uc2a4\ud2b8.\"\"\"\n        # Given\n        mock_client.return_value = {\"score\": 0.9}\n        test_case = {\n            \"query\": \"\ud14c\uc2a4\ud2b8 \uc9c8\ubb38\",\n            \"context\": \"\uad00\ub828 \ubb38\ub9e5\",\n            \"answer\": \"\uc815\ub2f5\"\n        }\n\n        # When\n        score = metric.measure(test_case)\n\n        # Then\n        assert score &gt; 0.8\n        mock_client.assert_called_once()\n</code></pre>"},{"location":"CONTRIBUTING/#_15","title":"\ud14c\uc2a4\ud2b8 \uc2e4\ud589","text":"<pre><code># \ubaa8\ub4e0 \ud14c\uc2a4\ud2b8 \uc2e4\ud589\npytest\n\n# \ud2b9\uc815 \ud14c\uc2a4\ud2b8 \ud30c\uc77c\npytest tests/unit/test_metrics.py\n\n# \ucee4\ubc84\ub9ac\uc9c0 \ud3ec\ud568\npytest --cov=eval --cov-report=html\n\n# \ubcd1\ub82c \uc2e4\ud589\npytest -n auto\n</code></pre>"},{"location":"CONTRIBUTING/#_16","title":"\ud83d\udcda \ubb38\uc11c\ud654","text":""},{"location":"CONTRIBUTING/#docstring","title":"Docstring \uaddc\uce59","text":"<p>Google \uc2a4\ud0c0\uc77c docstring\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <pre><code>def evaluate_dataset(\n    dataset_path: str, \n    metrics: List[str],\n    batch_size: int = 32\n) -&gt; Dict[str, float]:\n    \"\"\"\ub370\uc774\ud130\uc14b\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.\n\n    Args:\n        dataset_path: \ub370\uc774\ud130\uc14b \ud30c\uc77c \uacbd\ub85c\n        metrics: \uc0ac\uc6a9\ud560 \uba54\ud2b8\ub9ad \ub9ac\uc2a4\ud2b8\n        batch_size: \ubc30\uce58 \ud06c\uae30\n\n    Returns:\n        \uba54\ud2b8\ub9ad\ubcc4 \uc810\uc218 \ub515\uc154\ub108\ub9ac\n\n    Raises:\n        FileNotFoundError: \ub370\uc774\ud130\uc14b \ud30c\uc77c\uc774 \uc5c6\ub294 \uacbd\uc6b0\n        EvaluationError: \ud3c9\uac00 \uc911 \uc624\ub958 \ubc1c\uc0dd\n\n    Example:\n        &gt;&gt;&gt; results = evaluate_dataset(\"data.json\", [\"accuracy\", \"f1\"])\n        &gt;&gt;&gt; print(results[\"accuracy\"])\n        0.85\n    \"\"\"\n    pass\n</code></pre>"},{"location":"CONTRIBUTING/#readme","title":"README \ubc0f \ubb38\uc11c \uc5c5\ub370\uc774\ud2b8","text":"<p>\uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc744 \ucd94\uac00\ud560 \ub54c\ub294 \uad00\ub828 \ubb38\uc11c\ub3c4 \ud568\uaed8 \uc5c5\ub370\uc774\ud2b8\ud574\uc8fc\uc138\uc694:</p> <ul> <li><code>README.md</code>: \uc8fc\uc694 \uae30\ub2a5 \ubcc0\uacbd</li> <li><code>docs/</code>: \uc0c1\uc138 \uac00\uc774\ub4dc</li> <li>API \ubb38\uc11c: \uc0c8\ub85c\uc6b4 API \ucd94\uac00 \uc2dc</li> </ul>"},{"location":"CONTRIBUTING/#_17","title":"\ud83d\udc1b \uc774\uc288 \ub9ac\ud3ec\ud305","text":""},{"location":"CONTRIBUTING/#_18","title":"\ubc84\uadf8 \ub9ac\ud3ec\ud2b8","text":"<p>\uc774\uc288 \ud15c\ud50c\ub9bf\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc74c \uc815\ubcf4\ub97c \ud3ec\ud568\ud574\uc8fc\uc138\uc694:</p> <ul> <li>\ud658\uacbd \uc815\ubcf4: OS, Python \ubc84\uc804, \uc758\uc874\uc131 \ubc84\uc804</li> <li>\uc7ac\ud604 \ub2e8\uacc4: \ub2e8\uacc4\ubcc4 \uc0c1\uc138 \uc124\uba85</li> <li>\uc608\uc0c1 \ub3d9\uc791: \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud574\uc57c \ud558\ub294\uc9c0</li> <li>\uc2e4\uc81c \ub3d9\uc791: \uc2e4\uc81c\ub85c \ubb34\uc5c7\uc774 \uc77c\uc5b4\ub0ac\ub294\uc9c0</li> <li>\ub85c\uadf8: \uad00\ub828 \uc5d0\ub7ec \ub85c\uadf8\ub098 \uc2a4\ud0dd \ud2b8\ub808\uc774\uc2a4</li> </ul>"},{"location":"CONTRIBUTING/#_19","title":"\uae30\ub2a5 \uc694\uccad","text":"<ul> <li>\ub3d9\uae30: \uc65c \uc774 \uae30\ub2a5\uc774 \ud544\uc694\ud55c\uc9c0</li> <li>\uc81c\uc548: \uc5b4\ub5bb\uac8c \uad6c\ud604\ub418\uc5b4\uc57c \ud558\ub294\uc9c0</li> <li>\ub300\uc548: \uace0\ub824\ub41c \ub2e4\ub978 \ubc29\ubc95\ub4e4</li> <li>\uc601\ud5a5\ub3c4: \uae30\uc874 \uae30\ub2a5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5</li> </ul>"},{"location":"CONTRIBUTING/#pull-request","title":"\ud83d\udd0d Pull Request \uac00\uc774\ub4dc\ub77c\uc778","text":""},{"location":"CONTRIBUTING/#pr","title":"PR \uccb4\ud06c\ub9ac\uc2a4\ud2b8","text":"<ul> <li>[ ] \uc774\uc288\uc640 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4</li> <li>[ ] \ud14c\uc2a4\ud2b8\uac00 \ud1b5\uacfc\ud569\ub2c8\ub2e4</li> <li>[ ] \ucf54\ub4dc \ucee4\ubc84\ub9ac\uc9c0\uac00 \uc720\uc9c0\ub418\uac70\ub098 \uac1c\uc120\ub429\ub2c8\ub2e4</li> <li>[ ] \ubb38\uc11c\uac00 \uc5c5\ub370\uc774\ud2b8\ub418\uc5c8\uc2b5\ub2c8\ub2e4</li> <li>[ ] CHANGELOG\uac00 \uc5c5\ub370\uc774\ud2b8\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (\ud544\uc694\uc2dc)</li> <li>[ ] Breaking changes\uac00 \uba85\uc2dc\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (\ud574\ub2f9\uc2dc)</li> </ul>"},{"location":"CONTRIBUTING/#pr_1","title":"PR \ud15c\ud50c\ub9bf","text":"<pre><code>## \ubcc0\uacbd\uc0ac\ud56d \uc694\uc57d\n&lt;!-- \ubb34\uc5c7\uc744 \ubcc0\uacbd\ud588\ub294\uc9c0 \uac04\ub2e8\ud788 \uc124\uba85 --&gt;\n\n## \uad00\ub828 \uc774\uc288\n&lt;!-- \ud574\uacb0\ud558\ub294 \uc774\uc288 \ubc88\ud638 --&gt;\nCloses #123\n\n## \ubcc0\uacbd \uc720\ud615\n- [ ] \ubc84\uadf8 \uc218\uc815\n- [ ] \uc0c8 \uae30\ub2a5\n- [ ] Breaking change\n- [ ] \ubb38\uc11c \uc5c5\ub370\uc774\ud2b8\n\n## \ud14c\uc2a4\ud2b8\n&lt;!-- \uc5b4\ub5bb\uac8c \ud14c\uc2a4\ud2b8\ud588\ub294\uc9c0 \uc124\uba85 --&gt;\n\n## \uccb4\ud06c\ub9ac\uc2a4\ud2b8\n- [ ] \ud14c\uc2a4\ud2b8 \ucd94\uac00/\uc5c5\ub370\uc774\ud2b8\n- [ ] \ubb38\uc11c \uc5c5\ub370\uc774\ud2b8\n- [ ] \ub9b0\ud305 \ud1b5\uacfc\n</code></pre>"},{"location":"CONTRIBUTING/#_20","title":"\ucee4\ubc0b \uba54\uc2dc\uc9c0 \uaddc\uce59","text":"<p>Conventional Commits\uc744 \ub530\ub985\ub2c8\ub2e4:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Types: - <code>feat</code>: \uc0c8\ub85c\uc6b4 \uae30\ub2a5 - <code>fix</code>: \ubc84\uadf8 \uc218\uc815 - <code>docs</code>: \ubb38\uc11c \ubcc0\uacbd - <code>style</code>: \ucf54\ub4dc \uc2a4\ud0c0\uc77c \ubcc0\uacbd - <code>refactor</code>: \ub9ac\ud329\ud1a0\ub9c1 - <code>test</code>: \ud14c\uc2a4\ud2b8 \ucd94\uac00/\uc218\uc815 - <code>chore</code>: \uae30\ud0c0 \ubcc0\uacbd\uc0ac\ud56d</p> <p>Examples: <pre><code>feat(metrics): add hallucination detection metric\n\nfix(deepeval): resolve memory leak in evaluation loop\n\ndocs(readme): update installation instructions\n\ntest(rag): add integration tests for RAG metrics\n</code></pre></p>"},{"location":"CONTRIBUTING/#_21","title":"\ud83d\udc65 \ucf54\ub4dc \ub9ac\ubdf0 \ud504\ub85c\uc138\uc2a4","text":""},{"location":"CONTRIBUTING/#_22","title":"\ub9ac\ubdf0\uc5b4 \uac00\uc774\ub4dc\ub77c\uc778","text":"<p>\ub9ac\ubdf0 \ud3ec\uc778\ud2b8: - \ucf54\ub4dc \ud488\uc9c8 \ubc0f \uac00\ub3c5\uc131 - \ud14c\uc2a4\ud2b8 \ucee4\ubc84\ub9ac\uc9c0 - \uc131\ub2a5 \uc601\ud5a5 - \ubcf4\uc548 \uace0\ub824\uc0ac\ud56d - API \uc77c\uad00\uc131</p> <p>\ub9ac\ubdf0 \uc608\uc2dc: <pre><code>### \ud83d\udca1 \uc81c\uc548\n`evaluate_model` \ud568\uc218\uc5d0\uc11c \uc5d0\ub7ec \ucc98\ub9ac\ub97c \ucd94\uac00\ud558\ub294 \uac83\uc774 \uc88b\uaca0\uc2b5\ub2c8\ub2e4.\n\n### \ud83d\udc1b \uc774\uc288\nL45: \uc774 \ubd80\ubd84\uc5d0\uc11c \uba54\ubaa8\ub9ac \ub204\uc218\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n### \u2705 \uc2b9\uc778\nLGTM! \ud14c\uc2a4\ud2b8\ub3c4 \uc798 \uc791\uc131\ub418\uc5c8\ub124\uc694.\n</code></pre></p>"},{"location":"CONTRIBUTING/#_23","title":"\uc791\uc131\uc790 \uac00\uc774\ub4dc\ub77c\uc778","text":"<ul> <li>\ub9ac\ubdf0 \ud53c\ub4dc\ubc31\uc5d0 \uc2e0\uc18d\ud558\uac8c \uc751\ub2f5</li> <li>\ubcc0\uacbd\uc0ac\ud56d\uc5d0 \ub300\ud55c \uba85\ud655\ud55c \uc124\uba85</li> <li>\ud14c\uc2a4\ud2b8 \uacb0\uacfc \uacf5\uc720</li> <li>\uc758\uacac \ubd88\uc77c\uce58 \uc2dc \uac74\uc124\uc801 \ud1a0\ub860</li> </ul>"},{"location":"CONTRIBUTING/#_24","title":"\ud83c\udff7\ufe0f \ub9b4\ub9ac\uc2a4 \ud504\ub85c\uc138\uc2a4","text":""},{"location":"CONTRIBUTING/#_25","title":"\ubc84\uc804 \uad00\ub9ac","text":"<ul> <li>Semantic Versioning \uc0ac\uc6a9: <code>MAJOR.MINOR.PATCH</code></li> <li>Pre-release: <code>1.0.0-alpha.1</code>, <code>1.0.0-beta.1</code>, <code>1.0.0-rc.1</code></li> </ul>"},{"location":"CONTRIBUTING/#_26","title":"\ub9b4\ub9ac\uc2a4 \ub2e8\uacc4","text":"<ol> <li>Feature freeze: \uc0c8 \uae30\ub2a5 \ucd94\uac00 \uc911\ub2e8</li> <li>\ud14c\uc2a4\ud305: \uc804\uccb4 \ud14c\uc2a4\ud2b8 \uc2a4\uc704\ud2b8 \uc2e4\ud589</li> <li>\ubb38\uc11c \uc5c5\ub370\uc774\ud2b8: README, CHANGELOG \uc5c5\ub370\uc774\ud2b8</li> <li>\ud0dc\uadf8 \uc0dd\uc131: <code>git tag v1.0.0</code></li> <li>\ub9b4\ub9ac\uc2a4 \ub178\ud2b8: GitHub \ub9b4\ub9ac\uc2a4 \ud398\uc774\uc9c0 \uc791\uc131</li> </ol>"},{"location":"CONTRIBUTING/#_27","title":"\ud83d\udd12 \ubcf4\uc548 \uac00\uc774\ub4dc\ub77c\uc778","text":""},{"location":"CONTRIBUTING/#_28","title":"\ubcf4\uc548 \uc774\uc288 \ub9ac\ud3ec\ud305","text":"<p>\ubcf4\uc548 \ucde8\uc57d\uc810\uc740 public \uc774\uc288\uac00 \uc544\ub2cc \uc774\uba54\uc77c\ub85c \uc2e0\uace0\ud574\uc8fc\uc138\uc694: - \uc774\uba54\uc77c: security@company.com - PGP Key: [\uacf5\uac1c\ud0a4 \ub9c1\ud06c]</p>"},{"location":"CONTRIBUTING/#_29","title":"\ubcf4\uc548 \uccb4\ud06c\ub9ac\uc2a4\ud2b8","text":"<ul> <li>\ubbfc\uac10\ud55c \uc815\ubcf4 \ud558\ub4dc\ucf54\ub529 \uae08\uc9c0</li> <li>\uc758\uc874\uc131 \ucde8\uc57d\uc810 \uc815\uae30 \uac80\uc0ac</li> <li>API \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc778\uc99d/\uc778\uac00 \ud655\uc778</li> <li>\ub85c\uadf8\uc5d0 \ubbfc\uac10\ud55c \uc815\ubcf4 \ucd9c\ub825 \uae08\uc9c0</li> </ul>"},{"location":"CONTRIBUTING/#_30","title":"\ud83c\udf89 \uc778\uc815\uacfc \uac10\uc0ac","text":"<p>\ubaa8\ub4e0 \uae30\uc5ec\uc790\ub294 README\uc758 Contributors \uc139\uc158\uc5d0 \ub4f1\ub85d\ub429\ub2c8\ub2e4. \ub610\ud55c \ub9b4\ub9ac\uc2a4 \ub178\ud2b8\uc5d0\uc11c \uae30\uc5ec \ub0b4\uc6a9\uc774 \uc5b8\uae09\ub429\ub2c8\ub2e4.</p>"},{"location":"CONTRIBUTING/#_31","title":"\ud83d\udcde \ub3c4\uc6c0 \ubc0f \ubb38\uc758","text":"<ul> <li>\uc77c\ubc18 \uc9c8\ubb38: GitHub Discussions</li> <li>\uae30\uc220 \uc9c0\uc6d0: #vllm-eval Slack \ucc44\ub110</li> <li>\uae34\uae09 \ubb38\uc758: tech-support@company.com</li> </ul> <p>\ub2e4\uc2dc \ud55c\ubc88 \uae30\uc5ec\ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4! \ud83d\ude4f</p> <p>\uc9c8\ubb38\uc774\ub098 \uc81c\uc548\uc0ac\ud56d\uc774 \uc788\uc73c\uc2dc\uba74 \uc5b8\uc81c\ub4e0\uc9c0 \uc5f0\ub77d\ud574\uc8fc\uc138\uc694.</p>"},{"location":"how-to-CICD/","title":"\ud83d\ude80 \ubc30\ud3ec \ubc0f \ubaa8\ub2c8\ud130\ub9c1 \uac00\uc774\ub4dc","text":"<p>\uc774 \ubb38\uc11c\ub294 VLLM \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc758 \ubca4\uce58\ub9c8\ud06c\ub97c \ubc30\ud3ec\ud558\uace0 \ubaa8\ub2c8\ud130\ub9c1\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 Argo CD\ub97c \ud65c\uc6a9\ud55c GitOps \uae30\ubc18\uc758 \uc790\ub3d9\ud654\ub41c \ubc30\ud3ec \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ucd95\ud558\ub294 \uac83\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-CICD/#gitops-argo-cd","title":"GitOps \uae30\ubc18 \ubc30\ud3ec (Argo CD)","text":"<p>Argo CD\ub97c \uc0ac\uc6a9\ud558\uba74 Kubernetes \ub9e4\ub2c8\ud398\uc2a4\ud2b8\ub97c Git \uc800\uc7a5\uc18c\uc5d0\uc11c \uad00\ub9ac\ud558\uba70, Git\uc758 \ubcc0\uacbd \uc0ac\ud56d\uc774 \ud074\ub7ec\uc2a4\ud130\uc5d0 \uc790\ub3d9\uc73c\ub85c \ub3d9\uae30\ud654\ub418\ub3c4\ub85d \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubc30\ud3ec \uacfc\uc815\uc744 \uc790\ub3d9\ud654\ud558\uace0, \ubaa8\ub4e0 \ubcc0\uacbd \uc0ac\ud56d\uc744 \ucd94\uc801\ud558\uba70, \uc548\uc815\uc801\uc778 \ub864\ubc31\uc744 \ubcf4\uc7a5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"how-to-CICD/#cicd","title":"CI/CD \ud750\ub984","text":"<p>\uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ucf54\ub4dc\uc640 Kubernetes \ub9e4\ub2c8\ud398\uc2a4\ud2b8\uac00 \ub3d9\uc77c\ud55c Git \uc800\uc7a5\uc18c\uc5d0\uc11c \uad00\ub9ac\ub429\ub2c8\ub2e4.</p> <pre><code>graph TD\n    A[\ud83d\udc68\u200d\ud83d\udcbb Developer: \ucf54\ub4dc Push] --&gt; B{CI: GitHub Actions};\n    subgraph \"\uc560\ud50c\ub9ac\ucf00\uc774\uc158 &amp; \ub9e4\ub2c8\ud398\uc2a4\ud2b8 Git \uc800\uc7a5\uc18c\"\n        A --&gt; B;\n        B --&gt; C[\ud83d\udd27 1. Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc &amp; 'latest'\ub85c \ud478\uc2dc];\n        C --&gt; D[\ud83d\udcc4 2. k8s/ \ud3f4\ub354\uc758 \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \uc5c5\ub370\uc774\ud2b8 &amp; Push];\n        D --&gt; E{CD: Argo CD};\n    end\n    E --&gt; F[\ud83d\udd04 K8s \ud074\ub7ec\uc2a4\ud130\uc5d0 \uc790\ub3d9 \ub3d9\uae30\ud654];</code></pre>"},{"location":"how-to-CICD/#1-argo-cd-helm","title":"1\ub2e8\uacc4: \ud074\ub7ec\uc2a4\ud130\uc5d0 Argo CD \uc124\uce58 (Helm \uc0ac\uc6a9)","text":"<p>\uba3c\uc800, \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130\uc5d0 Argo CD\ub97c \uc124\uce58\ud569\ub2c8\ub2e4. Helm\uc744 \uc0ac\uc6a9\ud558\uba74 \uac04\ud3b8\ud558\uac8c \uc124\uce58\ud558\uace0 \uad00\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code># 1. Argo CD \ub124\uc784\uc2a4\ud398\uc774\uc2a4 \uc0dd\uc131\nkubectl create namespace argocd\n\n# 2. Argo CD Helm \uc800\uc7a5\uc18c \ucd94\uac00\nhelm repo add argo https://argoproj.github.io/argo-helm\n\n# 3. Helm \ucc28\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec Argo CD \uc124\uce58\nhelm install argocd argo/argo-cd --namespace argocd\n</code></pre> <p>\uc124\uce58\uac00 \uc644\ub8cc\ub41c \ud6c4, \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd08\uae30 \uad00\ub9ac\uc790 \ube44\ubc00\ubc88\ud638\ub97c \ud655\uc778\ud558\uace0 Argo CD UI\uc5d0 \uc811\uc18d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p><pre><code># \ucd08\uae30 \ube44\ubc00\ubc88\ud638 \ud655\uc778\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n\n# Argo CD \uc11c\ubc84\uc5d0 \ud3ec\ud2b8 \ud3ec\uc6cc\ub529 (\ub85c\uceec\uc5d0\uc11c 8080 \ud3ec\ud2b8\ub85c \uc811\uc18d)\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre> \uc774\uc81c \uc6f9 \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c <code>https://localhost:8080</code>\uc73c\ub85c \uc811\uc18d\ud558\uc5ec \uc0ac\uc6a9\uc790 \uc774\ub984 <code>admin</code>\uacfc \uc704\uc5d0\uc11c \uc5bb\uc740 \ube44\ubc00\ubc88\ud638\ub85c \ub85c\uadf8\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"how-to-CICD/#2-ci-github-actions","title":"2\ub2e8\uacc4: CI \ud30c\uc774\ud504\ub77c\uc778 \uc124\uc815 (GitHub Actions)","text":"<p>\uc774 \ud504\ub85c\uc81d\ud2b8\uc758 CI/CD \ud30c\uc774\ud504\ub77c\uc778\uc740 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ucf54\ub4dc\uc640 Kubernetes \ub9e4\ub2c8\ud398\uc2a4\ud2b8\ub97c \ub3d9\uc77c\ud55c Git \uc800\uc7a5\uc18c\uc5d0\uc11c \uad00\ub9ac\ud569\ub2c8\ub2e4. <code>.github/workflows/evalchemy-build.yml</code>\uacfc \uac19\uc740 \ube4c\ub4dc \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub294 \ubca4\uce58\ub9c8\ud06c \uad00\ub828 \ucf54\ub4dc(\uc608: <code>eval/</code> \ub514\ub809\ud1a0\ub9ac)\uac00 \ubcc0\uacbd\ub418\uba74 \ub2e4\uc74c \uc791\uc5c5\uc744 \uc790\ub3d9\uc73c\ub85c \uc218\ud589\ud569\ub2c8\ub2e4.</p> <ol> <li>\ubcc0\uacbd\ub41c \ucf54\ub4dc\uc5d0 \ud574\ub2f9\ud558\ub294 Docker \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc\ud558\uc5ec <code>:latest</code> \ud0dc\uadf8\ub85c \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac(GHCR)\uc5d0 \ud478\uc2dc\ud569\ub2c8\ub2e4.</li> <li>\ub3d9\uc77c\ud55c \uc800\uc7a5\uc18c \ub0b4\uc758 <code>k8s/</code> \ub514\ub809\ud1a0\ub9ac\uc5d0 \uc788\ub294 \uad00\ub828 Kubernetes \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \ud30c\uc77c(\uc608: <code>evalchemy-job.yaml</code>)\uc744 \uc218\uc815\ud569\ub2c8\ub2e4.<ul> <li><code>latest</code> \ud0dc\uadf8\ub97c \uc0ac\uc6a9\ud558\ubbc0\ub85c \uc774\ubbf8\uc9c0 \ud0dc\uadf8\ub294 \ubcc0\uacbd\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</li> <li>\ub300\uc2e0, \ub9e4\ub2c8\ud398\uc2a4\ud2b8\uc758 <code>annotations</code>\uc5d0 \ud604\uc7ac \ucee4\ubc0b \ud574\uc2dc\ub97c \ucd94\uac00\ud558\uc5ec Argo CD\uac00 \ubcc0\uacbd \uc0ac\ud56d\uc744 \uac10\uc9c0\ud558\uace0 \uc0c8\ub85c\uc6b4 \uc774\ubbf8\uc9c0\ub97c \ubc30\ud3ec\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4.</li> </ul> </li> <li>\uc218\uc815\ub41c \ub9e4\ub2c8\ud398\uc2a4\ud2b8\ub97c <code>[skip ci]</code> \uba54\uc2dc\uc9c0\uc640 \ud568\uaed8 \ub2e4\uc2dc \uc800\uc7a5\uc18c\uc5d0 \ud478\uc2dc\ud558\uc5ec \ubb34\ud55c CI \ub8e8\ud504\ub97c \ubc29\uc9c0\ud569\ub2c8\ub2e4.</li> </ol>"},{"location":"how-to-CICD/#_2","title":"\ud544\uc694\ud55c \uc124\uc815","text":"<p>\uc6cc\ud06c\ud50c\ub85c\uc6b0\uac00 \uc800\uc7a5\uc18c\uc5d0 \ub2e4\uc2dc \ud478\uc2dc\ud558\ub824\uba74 GitHub Actions\uc758 \uae30\ubcf8 <code>GITHUB_TOKEN</code>\uc5d0 \uc4f0\uae30 \uad8c\ud55c\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc800\uc7a5\uc18c <code>Settings &gt; Actions &gt; General</code>\uc5d0\uc11c <code>Workflow permissions</code>\ub97c <code>Read and write permissions</code>\uc73c\ub85c \uc124\uc815\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uc544\ub798\ub294 <code>evalchemy-build.yml</code> \uc6cc\ud06c\ud50c\ub85c\uc6b0\uc5d0\uc11c \ub9e4\ub2c8\ud398\uc2a4\ud2b8\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \ubd80\ubd84\uc758 \uc608\uc2dc\uc785\ub2c8\ub2e4.</p> <pre><code># .github/workflows/evalchemy-build.yml\n# ...\n  - name: \ud83d\udd04 Update K8s manifest to trigger Argo CD\n    run: |\n      # yq\uc640 \uac19\uc740 \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud558\uc5ec annotation \uc5c5\ub370\uc774\ud2b8\n      yq e '.spec.template.metadata.annotations.\"git-commit\" = \"${{ github.sha }}\"' -i k8s/evalchemy-job.yaml\n\n  - name: \ud83d\ude80 Commit and push manifest changes\n    run: |\n      git config --global user.name 'github-actions'\n      git config --global user.email 'github-actions@github.com'\n      git add k8s/evalchemy-job.yaml\n      git commit -m \"Update evalchemy manifest for commit ${{ github.sha }} [skip ci]\"\n      git push\n# ...\n</code></pre> <ol> <li> <p><code>configs/evalchemy.json</code> \uc5c5\ub370\uc774\ud2b8: <code>tasks</code> \ubaa9\ub85d\uc5d0 \uc2e4\ud589\ud560 \ud0dc\uc2a4\ud06c\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4. (\uc218\uc815 \ud6c4 \uc774\ubbf8\uc9c0 \uc7ac\ube4c\ub4dc \ud544\uc694)     <pre><code>{\n  \"tasks\": [\n    \"custom_task_1\",\n    \"custom_task_2\"\n   ]\n}\n</code></pre></p> </li> <li> <p><code>k8s/evalchemy-job.yaml</code> \uc218\uc815: <code>image</code> \ud544\ub4dc\ub97c \uc0c8\ub85c \ube4c\ub4dc\ud55c \uc774\ubbf8\uc9c0 \uc8fc\uc18c\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4.</p> </li> </ol> <pre><code># k8s/evalchemy-job.yaml\nspec:\n  template:\n    spec:\n      containers:\n        - name: evalchemy\n          image: your-registry/vllm-eval-evalchemy:custom-task-v1\n</code></pre> <p>\uc774 \ubc29\uc2dd\uc744 \ud1b5\ud574 \ucf54\ub4dc \ud478\uc2dc\ub9cc\uc73c\ub85c \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589 \ubc0f \uc5c5\ub370\uc774\ud2b8\uac00 \uc644\uc804\ud788 \uc790\ub3d9\ud654\ub429\ub2c8\ub2e4.</p>"},{"location":"how-to-CICD/#_3","title":"\uc218\ub3d9 \ubc30\ud3ec (\ub85c\uceec \ud14c\uc2a4\ud2b8 \ubc0f \uac1c\ubc1c\uc6a9)","text":"<p>\uac04\ub2e8\ud55c \ud14c\uc2a4\ud2b8\ub098 \uac1c\ubc1c \ud658\uacbd\uc5d0\uc11c\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc218\ub3d9\uc73c\ub85c \ubc30\ud3ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ol> <li> <p>Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ubc0f \ud478\uc2dc:     <pre><code>docker build -t your-registry/vllm-eval-evalchemy:custom-task-v1 -f docker/evalchemy.Dockerfile .\ndocker push your-registry/vllm-eval-evalchemy:custom-task-v1\n</code></pre></p> </li> <li> <p><code>eval_config.json</code> \uc5c5\ub370\uc774\ud2b8: \uc774\ubbf8\uc9c0\uc5d0 \ud3ec\ud568\ub41c <code>eval/evalchemy/configs/eval_config.json</code> \ud30c\uc77c\uc5d0\uc11c \uc2e4\ud589\ud560 \ud0dc\uc2a4\ud06c\ub97c \ud65c\uc131\ud654\ud569\ub2c8\ub2e4. (\uc218\uc815 \ud6c4 \uc774\ubbf8\uc9c0 \uc7ac\ube4c\ub4dc \ud544\uc694)     <pre><code>{\n  \"custom_task_1\": { \"enabled\": true },\n  \"custom_task_2\": { \"enabled\": true }\n}\n</code></pre></p> </li> <li> <p><code>k8s/evalchemy-job.yaml</code> \uc218\uc815: <code>image</code> \ud544\ub4dc\ub97c \uc0c8\ub85c \ube4c\ub4dc\ud55c \uc774\ubbf8\uc9c0 \uc8fc\uc18c\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4.     <pre><code># k8s/evalchemy-job.yaml\nspec:\n  template:\n    spec:\n      containers:\n        - name: evalchemy\n          image: your-registry/vllm-eval-evalchemy:custom-task-v1\n</code></pre></p> </li> <li> <p>Kubernetes Job \uc2e4\ud589:     <pre><code>kubectl apply -f k8s/evalchemy-job.yaml\n</code></pre></p> </li> </ol>"},{"location":"how-to-CICD/#_4","title":"\ubaa8\ub2c8\ud130\ub9c1 \ubc0f \uc54c\ub9bc \uc124\uc815","text":"<p>\ubc30\ud3ec \ubc29\uc2dd\uacfc \uad00\uacc4\uc5c6\uc774, \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub294 Grafana\uc640 Prometheus\ub97c \ud1b5\ud574 \ubaa8\ub2c8\ud130\ub9c1\ud558\uace0 \uc54c\ub9bc\uc744 \uc124\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li> <p>Grafana \ub300\uc2dc\ubcf4\ub4dc: \uc0c8 \ubca4\uce58\ub9c8\ud06c(<code>custom_task_1</code>)\uc758 <code>exact_match</code> \uc810\uc218\ub97c \uc2dc\uac01\ud654\ud558\ub294 \ud328\ub110 \uc608\uc2dc\uc785\ub2c8\ub2e4.     <pre><code>// charts/grafana/dashboards/custom-benchmark.json\n{\n  \"title\": \"Custom Task 1 Score (exact_match)\",\n  \"type\": \"stat\",\n  \"targets\": [{\n    \"expr\": \"avg(vllm_eval_exact_match{task='custom_task_1'}) by (model_tag)\",\n    \"legendFormat\": \"{{model_tag}}\"\n  }]\n}\n</code></pre></p> </li> <li> <p>Prometheus \uc54c\ub9bc \uaddc\uce59: <code>custom_task_1</code>\uc758 \uc810\uc218\uac00 \uc804\ub0a0 \ub300\ube44 10% \uc774\uc0c1 \ud558\ub77d\ud558\uba74 \uc54c\ub9bc\uc744 \ubcf4\ub0c5\ub2c8\ub2e4. (Alertmanager \uc124\uc815 \ud30c\uc77c\uc5d0 \ucd94\uac00)     <pre><code># \uc608\uc2dc: alert-rules.yaml\n- alert: CustomTaskRegression\n  expr: |\n    avg_over_time(vllm_eval_exact_match{task='custom_task_1'}[1h]) &lt; \n    avg_over_time(vllm_eval_exact_match{task='custom_task_1'}[24h] offset 24h) * 0.9\n  for: 10m\n  annotations:\n    summary: \"Custom Task 1 \uc131\ub2a5 \uc800\ud558 \uac10\uc9c0\"\n</code></pre></p> </li> </ul>"},{"location":"how-to-add-benchmark/","title":"\ud83c\udfaf \ubca4\uce58\ub9c8\ud06c \ucd94\uac00 \uac00\uc774\ub4dc","text":"<p>\uc774 \ubb38\uc11c\ub294 VLLM \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc5d0 \uc0c8\ub85c\uc6b4 \ubca4\uce58\ub9c8\ud06c\ub97c \ucd94\uac00\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#_2","title":"\ud83d\udccb \ubaa9\ucc28","text":"<ul> <li>\ud83c\udfaf \uac1c\uc694</li> <li>\ud83d\udcca \ub370\uc774\ud130\uc14b \uc900\ube44</li> <li>\ud83d\udd0d \ubca4\uce58\ub9c8\ud06c \uc720\ud615</li> <li>\ud83e\uddea Deepeval \uba54\ud2b8\ub9ad \ucd94\uac00</li> <li>\ud83e\uddea Evalchemy \ubca4\uce58\ub9c8\ud06c \ucd94\uac00</li> <li>\ud83e\uddea \ud14c\uc2a4\ud2b8 \ubc0f \uac80\uc99d</li> <li>\ud83d\udeab \ubb38\uc81c \ud574\uacb0</li> </ul>"},{"location":"how-to-add-benchmark/#_3","title":"\ud83c\udfaf \uac1c\uc694","text":"<p>VLLM \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc740 \ub450 \uac00\uc9c0 \ud3c9\uac00 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc9c0\uc6d0\ud569\ub2c8\ub2e4:</p> <ul> <li>Deepeval: \ucee4\uc2a4\ud140 \uba54\ud2b8\ub9ad \ubc0f RAG \ud3c9\uac00</li> <li>Evalchemy: \ud45c\uc900 \ubca4\uce58\ub9c8\ud06c \ubc0f \ub300\uaddc\ubaa8 \ud3c9\uac00</li> </ul> <p>\uc0c8\ub85c\uc6b4 \ubca4\uce58\ub9c8\ud06c\ub97c \ucd94\uac00\ud560 \ub54c\ub294 \ud3c9\uac00 \ubaa9\uc801\uacfc \ud2b9\uc131\uc5d0 \ub530\ub77c \uc801\uc808\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#_4","title":"\ud83d\udcca \ub370\uc774\ud130\uc14b \uc900\ube44","text":""},{"location":"how-to-add-benchmark/#1-jsonl","title":"1. JSONL \ud615\uc2dd\uc73c\ub85c \ub370\uc774\ud130\uc14b \uc0dd\uc131","text":"<p>\uba3c\uc800 \ucee4\uc2a4\ud140 \ub370\uc774\ud130\uc14b\uc744 JSONL \ud615\uc2dd\uc73c\ub85c \uc900\ube44\ud569\ub2c8\ub2e4:</p> <pre><code># \ub370\uc774\ud130\uc14b \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\nmkdir -p datasets/raw/custom_benchmark\n\n# \ud6c8\ub828 \ub370\uc774\ud130 \uc0dd\uc131 (train.jsonl)\ncat &gt; datasets/raw/custom_benchmark/train.jsonl &lt;&lt; 'EOF'\n{\"question\": \"\ud55c\uad6d\uc758 \uc218\ub3c4\ub294 \uc5b4\ub514\uc778\uac00\uc694?\", \"answer\": \"\uc11c\uc6b8\", \"context\": \"\ud55c\uad6d\uc740 \ub3d9\uc544\uc2dc\uc544\uc5d0 \uc704\uce58\ud55c \uad6d\uac00\uc785\ub2c8\ub2e4.\", \"category\": \"geography\", \"difficulty\": \"easy\"}\n{\"question\": \"\ud30c\uc774\uc36c\uc5d0\uc11c \ub9ac\uc2a4\ud2b8\ub97c \uc815\ub82c\ud558\ub294 \ubc29\ubc95\uc740?\", \"answer\": \"sort() \uba54\uc11c\ub4dc\ub098 sorted() \ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\", \"context\": \"\ud30c\uc774\uc36c\uc740 \ub2e4\uc591\ud55c \uc815\ub82c \ubc29\ubc95\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\", \"category\": \"programming\", \"difficulty\": \"medium\"}\n{\"question\": \"\uc9c0\uad6c\uc5d0\uc11c \uac00\uc7a5 \ud070 \ub300\ub959\uc740?\", \"answer\": \"\uc544\uc2dc\uc544\", \"context\": \"\uc9c0\uad6c\ub294 7\uac1c\uc758 \ub300\ub959\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\", \"category\": \"geography\", \"difficulty\": \"easy\"}\n{\"question\": \"HTTP\uc640 HTTPS\uc758 \ucc28\uc774\uc810\uc740?\", \"answer\": \"HTTPS\ub294 HTTP\uc5d0 SSL/TLS \uc554\ud638\ud654\uac00 \ucd94\uac00\ub41c \ud504\ub85c\ud1a0\ucf5c\uc785\ub2c8\ub2e4.\", \"context\": \"\uc6f9 \ud1b5\uc2e0\uc5d0\uc11c \ubcf4\uc548\uc740 \ub9e4\uc6b0 \uc911\uc694\ud569\ub2c8\ub2e4.\", \"category\": \"technology\", \"difficulty\": \"medium\"}\n{\"question\": \"\uc170\uc775\uc2a4\ud53c\uc5b4\uc758 \ub300\ud45c\uc791\uc740?\", \"answer\": \"\ud584\ub9bf, \ub85c\ubbf8\uc624\uc640 \uc904\ub9ac\uc5e3, \ub9e5\ubca0\uc2a4 \ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4.\", \"context\": \"\uc170\uc775\uc2a4\ud53c\uc5b4\ub294 \uc601\uad6d\uc758 \ub300\ud45c\uc801\uc778 \uadf9\uc791\uac00\uc785\ub2c8\ub2e4.\", \"category\": \"literature\", \"difficulty\": \"medium\"}\nEOF\n\n# \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc0dd\uc131 (test.jsonl)\ncat &gt; datasets/raw/custom_benchmark/test.jsonl &lt;&lt; 'EOF'\n{\"question\": \"\uc77c\ubcf8\uc758 \uc218\ub3c4\ub294 \uc5b4\ub514\uc778\uac00\uc694?\", \"answer\": \"\ub3c4\ucfc4\", \"context\": \"\uc77c\ubcf8\uc740 \ub3d9\uc544\uc2dc\uc544\uc758 \uc12c\ub098\ub77c\uc785\ub2c8\ub2e4.\", \"category\": \"geography\", \"difficulty\": \"easy\"}\n{\"question\": \"\uc790\ubc14\uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c \ubc30\uc5f4\uc744 \uc815\ub82c\ud558\ub294 \ubc29\ubc95\uc740?\", \"answer\": \"sort() \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\", \"context\": \"\uc790\ubc14\uc2a4\ud06c\ub9bd\ud2b8\ub294 \uc6f9 \uac1c\ubc1c\uc758 \ud575\uc2ec \uc5b8\uc5b4\uc785\ub2c8\ub2e4.\", \"category\": \"programming\", \"difficulty\": \"medium\"}\n{\"question\": \"\uc138\uacc4\uc5d0\uc11c \uac00\uc7a5 \uae34 \uac15\uc740?\", \"answer\": \"\ub098\uc77c\uac15\", \"context\": \"\uac15\uc740 \uc9c0\uad6c\uc758 \uc911\uc694\ud55c \uc218\uc790\uc6d0\uc785\ub2c8\ub2e4.\", \"category\": \"geography\", \"difficulty\": \"medium\"}\nEOF\n</code></pre> <p>JSONL \ud615\uc2dd \uc124\uba85: - JSONL = JSON Lines (\uac01 \uc904\ub9c8\ub2e4 \ud558\ub098\uc758 JSON \uac1d\uccb4) - \uba38\uc2e0\ub7ec\ub2dd \ub370\uc774\ud130\uc14b\uc758 \ud45c\uc900 \ud615\uc2dd - \ub300\uc6a9\ub7c9 \ub370\uc774\ud130\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc2a4\ud2b8\ub9ac\ubc0d \ucc98\ub9ac \uac00\ub2a5</p>"},{"location":"how-to-add-benchmark/#2","title":"2. \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \ud30c\uc77c \uc0dd\uc131","text":"<pre><code>mkdir -p datasets/manifests\ncat &gt; datasets/manifests/custom_benchmark_manifest.yaml &lt;&lt; 'EOF'\nname: \"custom_benchmark\"\nversion: \"1.0\"\ndescription: \"\ucee4\uc2a4\ud140 \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\"\nlicense: \"MIT\"\nlanguage: [\"ko\", \"en\"]\ndomain: \"general\"\ntask_type: \"text_generation\"\n\nsource:\n  type: \"local\"\n  path: \"datasets/raw/custom_benchmark\"\n\nsplits:\n  train:\n    file: \"raw/custom_benchmark/train.jsonl\"\n    size: 5\n    sha256: \"\uacc4\uc0b0\ub41c_\ud574\uc2dc\uac12\"\n  test:\n    file: \"raw/custom_benchmark/test.jsonl\"\n    size: 3\n    sha256: \"\uacc4\uc0b0\ub41c_\ud574\uc2dc\uac12\"\n\nschema:\n  input_field: \"question\"\n  output_field: \"answer\"\n  context_field: \"context\"\n  metadata_fields: [\"category\", \"difficulty\"]\n\nevaluation:\n  evalchemy:\n    enabled: true\n    tasks: [\"custom_task_1\", \"custom_task_2\"]\nEOF\n</code></pre>"},{"location":"how-to-add-benchmark/#_5","title":"\ud83d\udd0d \ubca4\uce58\ub9c8\ud06c \uc720\ud615","text":""},{"location":"how-to-add-benchmark/#deepeval","title":"Deepeval \uc801\ud569 \ubca4\uce58\ub9c8\ud06c","text":"<ul> <li>RAG \ud3c9\uac00: \uac80\uc0c9 \uc99d\uac15 \uc0dd\uc131 \ud488\uc9c8 \uce21\uc815</li> <li>\ub3c4\uba54\uc778 \ud2b9\ud654: \ud2b9\uc815 \uc5c5\ubb34/\ub3c4\uba54\uc778 \uc131\ub2a5 \ud3c9\uac00</li> <li>\ucee4\uc2a4\ud140 \uba54\ud2b8\ub9ad: \ube44\uc988\ub2c8\uc2a4 \uc694\uad6c\uc0ac\ud56d\uc5d0 \ub9de\ucd98 \ud3c9\uac00</li> <li>\uc18c\uaddc\ubaa8 \ub370\uc774\ud130\uc14b: &lt; 1,000 \uc0d8\ud50c</li> </ul>"},{"location":"how-to-add-benchmark/#evalchemy","title":"Evalchemy \uc801\ud569 \ubca4\uce58\ub9c8\ud06c","text":"<ul> <li>\ud45c\uc900 \ubca4\uce58\ub9c8\ud06c: MMLU, ARC, HellaSwag \ub4f1</li> <li>\ub300\uaddc\ubaa8 \ud3c9\uac00: &gt; 1,000 \uc0d8\ud50c</li> <li>\ub2e4\uad6d\uc5b4 \uc9c0\uc6d0: \ud55c\uad6d\uc5b4, \uc601\uc5b4 \ub4f1 \ub2e4\uc591\ud55c \uc5b8\uc5b4</li> <li>GPU \uc9d1\uc57d\uc801: \ub300\uc6a9\ub7c9 \ubaa8\ub378 \ud3c9\uac00</li> </ul>"},{"location":"how-to-add-benchmark/#deepeval_1","title":"\ud83e\uddea Deepeval \uba54\ud2b8\ub9ad \ucd94\uac00","text":""},{"location":"how-to-add-benchmark/#1","title":"1. \uba54\ud2b8\ub9ad \ud074\ub798\uc2a4 \uc0dd\uc131","text":"<pre><code>cat &gt;&gt; eval/deepeval_tests/metrics/custom_metric.py &lt;&lt;EOF\nfrom deepeval.metrics import BaseMetric\nfrom deepeval.test_case import LLMTestCase\nfrom typing import Optional, List, Dict, Any\nimport asyncio\n\nclass CustomMetric(BaseMetric):\n    \"\"\"\n    \ucee4\uc2a4\ud140 \ud3c9\uac00 \uba54\ud2b8\ub9ad\n\n    Args:\n        threshold (float): \ud1b5\uacfc \uae30\uc900 \uc810\uc218 (0.0 ~ 1.0)\n        model (str): \ud3c9\uac00\uc5d0 \uc0ac\uc6a9\ud560 \ubaa8\ub378\uba85\n        include_reason (bool): \ud3c9\uac00 \uc774\uc720 \ud3ec\ud568 \uc5ec\ubd80\n    \"\"\"\n\n    def __init__(\n        self,\n        threshold: float = 0.7,\n        model: Optional[str] = None,\n        include_reason: bool = True,\n        async_mode: bool = True\n    ):\n        self.threshold = threshold\n        self.model = model\n        self.include_reason = include_reason\n        self.async_mode = async_mode\n\n    def measure(self, test_case: LLMTestCase) -&gt; float:\n        \"\"\"\ub3d9\uae30 \ud3c9\uac00 \uc2e4\ud589\"\"\"\n        if self.async_mode:\n            return asyncio.run(self.a_measure(test_case))\n        return self._evaluate_sync(test_case)\n\n    async def a_measure(self, test_case: LLMTestCase, _show_indicator: bool = True) -&gt; float:\n        \"\"\"\ube44\ub3d9\uae30 \ud3c9\uac00 \uc2e4\ud589\"\"\"\n        return self._evaluate_sync(test_case)\n\n    def _evaluate_sync(self, test_case: LLMTestCase) -&gt; float:\n        \"\"\"\uc2e4\uc81c \ud3c9\uac00 \ub85c\uc9c1 \uad6c\ud604\"\"\"\n        # \uc5ec\uae30\uc5d0 \ud3c9\uac00 \ub85c\uc9c1\uc744 \uad6c\ud604\ud569\ub2c8\ub2e4\n        input_text = test_case.input\n        actual_output = test_case.actual_output\n        expected_output = test_case.expected_output\n\n        # \uc608\uc2dc: \uac04\ub2e8\ud55c \uc720\uc0ac\ub3c4 \uacc4\uc0b0\n        score = self._calculate_similarity(actual_output, expected_output)\n\n        # \uba54\ud2b8\ub9ad \uc18d\uc131 \uc124\uc815\n        self.score = score\n        self.success = score &gt;= self.threshold\n\n        if self.include_reason:\n            self.reason = self._generate_reason(score, self.threshold)\n\n        return score\n\n    def _calculate_similarity(self, actual: str, expected: str) -&gt; float:\n        \"\"\"\uc720\uc0ac\ub3c4 \uacc4\uc0b0 \ub85c\uc9c1\"\"\"\n        # \uc2e4\uc81c \uad6c\ud604\uc5d0\uc11c\ub294 \ub354 \uc815\uad50\ud55c \ubc29\ubc95\uc744 \uc0ac\uc6a9\n        from difflib import SequenceMatcher\n        return SequenceMatcher(None, actual, expected).ratio()\n\n    def _generate_reason(self, score: float, threshold: float) -&gt; str:\n        \"\"\"\ud3c9\uac00 \uc774\uc720 \uc0dd\uc131\"\"\"\n        if score &gt;= threshold:\n            return f\"\u2705 \uc810\uc218 {score:.3f}\ub85c \uae30\uc900 {threshold} \uc774\uc0c1 \ub2ec\uc131\"\n        else:\n            return f\"\u274c \uc810\uc218 {score:.3f}\ub85c \uae30\uc900 {threshold} \ubbf8\ub2ec\"\n\n    def is_successful(self) -&gt; bool:\n        \"\"\"\ud3c9\uac00 \uc131\uacf5 \uc5ec\ubd80 \ubc18\ud658\"\"\"\n        return self.success\n\n    @property\n    def __name__(self):\n        return \"Custom Metric\"\nEOF\n</code></pre>"},{"location":"how-to-add-benchmark/#2_1","title":"2. \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4 \uc791\uc131","text":"<pre><code>cat &gt;&gt; eval/deepeval_tests/test_custom_metric.py &lt;&lt;EOF\nimport pytest\nfrom deepeval import assert_test\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import EvaluationDataset\nfrom .metrics.custom_metric import CustomMetric\n\nclass TestCustomMetric:\n    \"\"\"\ucee4\uc2a4\ud140 \uba54\ud2b8\ub9ad \ud14c\uc2a4\ud2b8\"\"\"\n\n    @pytest.mark.parametrize(\"threshold\", [0.5, 0.7, 0.9])\n    def test_custom_metric_threshold(self, threshold):\n        \"\"\"\ub2e4\uc591\ud55c \uc784\uacc4\uac12\uc5d0\uc11c \uba54\ud2b8\ub9ad \ud14c\uc2a4\ud2b8\"\"\"\n        test_case = LLMTestCase(\n            input=\"\ud14c\uc2a4\ud2b8 \uc785\ub825\",\n            actual_output=\"\uc2e4\uc81c \ucd9c\ub825\",\n            expected_output=\"\uc608\uc0c1 \ucd9c\ub825\"\n        )\n\n        metric = CustomMetric(threshold=threshold)\n        assert_test(test_case, [metric])\n\n    def test_custom_metric_dataset(self):\n        \"\"\"\ub370\uc774\ud130\uc14b \uae30\ubc18 \ud3c9\uac00 \ud14c\uc2a4\ud2b8\"\"\"\n        dataset = EvaluationDataset(test_cases=[\n            LLMTestCase(\n                input=\"\uc9c8\ubb38 1\",\n                actual_output=\"\ub2f5\ubcc0 1\",\n                expected_output=\"\uc815\ub2f5 1\"\n            ),\n            LLMTestCase(\n                input=\"\uc9c8\ubb38 2\", \n                actual_output=\"\ub2f5\ubcc0 2\",\n                expected_output=\"\uc815\ub2f5 2\"\n            )\n        ])\n\n        metric = CustomMetric(threshold=0.7)\n        dataset.evaluate([metric])\n\n        # \uacb0\uacfc \uac80\uc99d\n        assert len(dataset.test_cases) == 2\n        for test_case in dataset.test_cases:\n            assert hasattr(test_case, 'metrics_metadata')\nEOF\n</code></pre>"},{"location":"how-to-add-benchmark/#3","title":"3. \uba54\ud2b8\ub9ad \ub4f1\ub85d","text":"<pre><code>cat &gt;&gt; eval/deepeval_tests/metrics/__init__.py &lt;&lt;EOF\nfrom .rag_precision import RAGPrecisionMetric\nfrom .custom_metric import CustomMetric\n\n# \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uba54\ud2b8\ub9ad \ubaa9\ub85d\nAVAILABLE_METRICS = {\n    'rag_precision': RAGPrecisionMetric,\n    'custom_metric': CustomMetric,\n}\n\ndef get_metric(metric_name: str, **kwargs):\n    \"\"\"\uba54\ud2b8\ub9ad \ud329\ud1a0\ub9ac \ud568\uc218\"\"\"\n    if metric_name not in AVAILABLE_METRICS:\n        raise ValueError(f\"Unknown metric: {metric_name}\")\n\n    return AVAILABLE_METRICS[metric_name](**kwargs)\nEOF\n</code></pre>"},{"location":"how-to-add-benchmark/#evalchemy_1","title":"\u26a1 Evalchemy \ubca4\uce58\ub9c8\ud06c \ucd94\uac00","text":"<p>Evalchemy\uc5d0 \uc0c8\ub85c\uc6b4 \ubca4\uce58\ub9c8\ud06c\ub97c \ucd94\uac00\ud558\ub294 \uacfc\uc815\uc740 3\ub2e8\uacc4\ub85c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4: 1.  YAML \ud0dc\uc2a4\ud06c \uc815\uc758: <code>lm-evaluation-harness</code>\uac00 \uc774\ud574\ud560 \uc218 \uc788\ub294 YAML \ud615\uc2dd\uc73c\ub85c \ud0dc\uc2a4\ud06c\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. 2.  <code>eval_config.json</code> \ub4f1\ub85d: \uc0dd\uc131\ud55c \ud0dc\uc2a4\ud06c\ub97c \uc911\uc559 \uc124\uc815 \ud30c\uc77c\uc5d0 \ucd94\uac00\ud558\uc5ec \uad00\ub9ac\ud569\ub2c8\ub2e4. 3.  \ud14c\uc2a4\ud2b8 \ubc0f \uc2e4\ud589: <code>run_evalchemy.sh</code> \uc2a4\ud06c\ub9bd\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubca4\uce58\ub9c8\ud06c\ub97c \uc2e4\ud589\ud558\uace0 \uac80\uc99d\ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#1_1","title":"1. \ud0dc\uc2a4\ud06c \ub514\ub809\ud1a0\ub9ac \ubc0f \ucd08\uae30\ud654 \ud30c\uc77c \uc0dd\uc131","text":"<p>\uba3c\uc800 \ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c\ub97c \uc704\ud55c \ub514\ub809\ud1a0\ub9ac\uc640 <code>lm-evaluation-harness</code>\uac00 \ud0dc\uc2a4\ud06c\ub97c \uc778\uc2dd\ud558\ub294 \ub370 \ud544\uc694\ud55c \ucd08\uae30\ud654 \ud30c\uc77c\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \uc791\uc5c5\uc740 \ucc98\uc74c\uc5d0 \ud55c \ubc88\ub9cc \uc218\ud589\ud558\uba74 \ub429\ub2c8\ub2e4.</p> <pre><code># \ud0dc\uc2a4\ud06c \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131 (eval/standard_evalchemy \ub0b4\ubd80\uc5d0 \uc704\uce58)\nmkdir -p eval/standard_evalchemy/tasks\n\n# __init__.py \ud30c\uc77c \uc0dd\uc131 (\ud0dc\uc2a4\ud06c \uc778\uc2dd\uc744 \uc704\ud574 \ud544\uc694)\ncat &gt; eval/standard_evalchemy/tasks/__init__.py &lt;&lt; 'EOF'\n\"\"\"\n\ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c \ubaa8\ub4c8\n\n\uc774 \ud30c\uc77c\uc740 lm_eval\uc774 \uc774 \ub514\ub809\ud1a0\ub9ac\ub97c \ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c\uac00 \ud3ec\ud568\ub41c\n\ud30c\uc774\uc36c \ud328\ud0a4\uc9c0\ub85c \uc778\uc2dd\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4. YAML \uae30\ubc18 \ud0dc\uc2a4\ud06c\ub97c \uc8fc\ub85c \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4\n\uc774 \ud30c\uc77c\uc740 \ud544\uc694\ud569\ub2c8\ub2e4.\n\"\"\"\n# \ud0dc\uc2a4\ud06c \ub514\ub809\ud1a0\ub9ac\uc784\uc744 \uba85\uc2dc\n__version__ = \"1.0.0\"\n__author__ = \"VLLM Eval Team\"\nEOF\n</code></pre>"},{"location":"how-to-add-benchmark/#2-yaml","title":"2. YAML \ud0dc\uc2a4\ud06c \uc815\uc758 \uc0dd\uc131","text":"<p><code>lm-evaluation-harness</code> v0.4+ \ud45c\uc900\uc5d0 \ub530\ub77c YAML \ud30c\uc77c\ub85c \uc0c8\ub85c\uc6b4 \ud0dc\uc2a4\ud06c\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.</p> <pre><code># custom_task_1.yaml \uc0dd\uc131\ncat &gt; eval/standard_evalchemy/tasks/custom_task_1.yaml &lt;&lt; 'EOF'\ntask: custom_task_1\ntest_split: test\nfewshot_split: train\ndoc_to_text: \"\uc9c8\ubb38: {{question}}\\n\ub2f5\ubcc0:\"\ndoc_to_target: \"{{answer}}\"\ndescription: \"\ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c 1 - \uac04\ub2e8\ud55c \uc9c8\ubb38\ub2f5\ubcc0\"\ndataset_path: json\ndataset_kwargs:\n  data_files:\n    train: \"../../../datasets/raw/custom_benchmark/train.jsonl\"\n    test: \"../../../datasets/raw/custom_benchmark/test.jsonl\"\noutput_type: generate_until\ngeneration_kwargs:\n  until: [\"\\n\", \"\uc9c8\ubb38:\"]\n  max_gen_toks: 256\nfilter_list:\n  - name: \"whitespace_cleanup\"\nmetric_list:\n  - metric: exact_match\n    aggregation: mean\n    higher_is_better: true\nmetadata:\n  version: 1.0\nEOF\n</code></pre> <p>\uc8fc\uc694 YAML \uc124\uc815: - <code>task</code>: \ud0dc\uc2a4\ud06c\uc758 \uace0\uc720 \uc774\ub984. <code>eval_config.json</code>\uc5d0\uc11c \uc774 \uc774\ub984\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. - <code>dataset_path</code>: <code>json</code>\uc73c\ub85c \uc124\uc815\ud558\uc5ec\u30ed\u30fc\u30ab\u30eb JSON/JSONL \ud30c\uc77c\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. - <code>dataset_kwargs</code>: \ub370\uc774\ud130 \ud30c\uc77c\uc758 \uc0c1\ub300 \uacbd\ub85c\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4. \uacbd\ub85c\ub294 <code>eval/standard_evalchemy/</code> \ub514\ub809\ud1a0\ub9ac \uae30\uc900\uc73c\ub85c \uc791\uc131\ud574\uc57c \ud569\ub2c8\ub2e4. - <code>doc_to_text</code>/<code>doc_to_target</code>: \ubaa8\ub378\uc5d0 \uc785\ub825\ub420 \ud504\ub86c\ud504\ud2b8 \ud615\uc2dd\uacfc \uc815\ub2f5 \ud544\ub4dc\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4. - <code>metric_list</code>: \ud3c9\uac00\uc5d0 \uc0ac\uc6a9\ud560 \uba54\ud2b8\ub9ad\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#3-configseval_configjson","title":"3. <code>configs/eval_config.json</code>\uc5d0 \ud0dc\uc2a4\ud06c \ub4f1\ub85d","text":"<p>YAML \ud30c\uc77c\uc744 \uc0dd\uc131\ud55c \ud6c4, <code>run_evalchemy.sh</code> \uc2a4\ud06c\ub9bd\ud2b8\uac00 \ud0dc\uc2a4\ud06c\ub97c \uc778\uc2dd\ud558\uace0 \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d <code>eval/standard_evalchemy/configs/eval_config.json</code> \ud30c\uc77c\uc5d0 \ub4f1\ub85d\ud569\ub2c8\ub2e4.</p> <pre><code># eval/standard_evalchemy/configs/eval_config.json \ud30c\uc77c\uc744 \uc5f4\uc5b4 \"tasks\" \uc139\uc158\uc5d0 \ub2e4\uc74c \ub0b4\uc6a9\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4.\n# \"custom_task_1\"\uc740 yaml \ud30c\uc77c\uc758 'task' \ud544\ub4dc\uc640 \uc77c\uce58\ud574\uc57c \ud569\ub2c8\ub2e4.\n</code></pre> <p><code>eval/standard_evalchemy/configs/eval_config.json</code> \uc218\uc815 \uc608\uc2dc: <pre><code>{\n  \"benchmarks\": {\n    // ...\n    \"custom_eval_group\": {\n      \"enabled\": true,\n      \"description\": \"\ub0b4\uac00 \ub9cc\ub4e0 \ucee4\uc2a4\ud140 \ubca4\uce58\ub9c8\ud06c \uadf8\ub8f9\",\n      \"tasks\": [\"custom_task_1\"]\n    }\n  },\n  \"tasks\": {\n    // ... \uae30\uc874 \ud0dc\uc2a4\ud06c\ub4e4 ...\n    \"custom_task_1\": {\n      \"enabled\": true,\n      \"tasks\": [\"custom_task_1\"],\n      \"num_fewshot\": 0,\n      \"batch_size\": 1,\n      \"limit\": null,\n      \"description\": \"\ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c 1 - \uac04\ub2e8\ud55c \uc9c8\ubb38\ub2f5\ubcc0\"\n    }\n  }\n}\n</code></pre> - <code>tasks</code> \uc139\uc158: <code>custom_task_1</code>\uc774\ub77c\ub294 \uc0c8 \ud56d\ubaa9\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c \ud0a4 \uac12(<code>\"custom_task_1\"</code>)\uc740 <code>run_evalchemy.sh</code> \uc2a4\ud06c\ub9bd\ud2b8 \ub0b4\uc5d0\uc11c \ucc38\uc870\ud558\ub294 \uc774\ub984\uc774 \ub429\ub2c8\ub2e4. \ub0b4\ubd80\uc758 <code>\"tasks\"</code> \ubc30\uc5f4\uc5d0 \uc788\ub294 <code>custom_task_1</code>\uc740 YAML \ud30c\uc77c\uc5d0 \uc815\uc758\ub41c <code>task</code> \uc774\ub984\uc785\ub2c8\ub2e4. - <code>benchmarks</code> \uc139\uc158 (\uc120\ud0dd \uc0ac\ud56d): \uc5ec\ub7ec \ud0dc\uc2a4\ud06c\ub97c <code>custom_eval_group</code>\uacfc \uac19\uc774 \ub17c\ub9ac\uc801\uc778 \uadf8\ub8f9\uc73c\ub85c \ubb36\uc5b4 \ud55c \ubc88\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <code>enabled: true</code>\ub85c \uc124\uc815\ud574\uc57c \uc2e4\ud589\ub429\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#_6","title":"\ud83e\uddea \ud14c\uc2a4\ud2b8 \ubc0f \uac80\uc99d","text":"<p>\uc0c8\ub86d\uac8c \ucd94\uac00\ud55c \ubca4\uce58\ub9c8\ud06c\uac00 \uc62c\ubc14\ub974\uac8c \ub3d9\uc791\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ud14c\uc2a4\ud2b8\ub294 <code>eval/standard_evalchemy</code> \ub514\ub809\ud1a0\ub9ac\uc5d0\uc11c \uc2e4\ud589\ud558\ub294 \uac83\uc744 \uae30\uc900\uc73c\ub85c \ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#1_2","title":"1. \uc124\uc815 \ud30c\uc77c \ubc0f \ud0dc\uc2a4\ud06c \uc720\ud6a8\uc131 \uac80\uc0ac","text":"<pre><code># \uc791\uc5c5 \ub514\ub809\ud1a0\ub9ac\ub85c \uc774\ub3d9\ncd eval/standard_evalchemy\n\n# \uc124\uc815 \ud30c\uc77c(eval_config.json)\uc774 \uc720\ud6a8\ud55c JSON\uc778\uc9c0 \ud655\uc778\njq empty configs/eval_config.json &amp;&amp; echo \"\u2705 eval_config.json is valid\"\n\n# \ucd94\uac00\ud55c \ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c\uac00 lm-evaluation-harness\uc5d0 \uc758\ud574 \uc778\uc2dd\ub418\ub294\uc9c0 \ud655\uc778\n# --tasks list \uc635\uc158\uc73c\ub85c \uc804\uccb4 \ud0dc\uc2a4\ud06c \ubaa9\ub85d\uc744 \ud655\uc778\ud558\uace0 custom_task_1\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294\uc9c0 \uac80\uc0ac\ud569\ub2c8\ub2e4.\n./run_evalchemy.sh --tasks list | grep custom_task_1\n</code></pre> <p>\ucc38\uace0: <code>run_evalchemy.sh</code>\ub294 \ub0b4\ubd80\uc801\uc73c\ub85c <code>lm_eval</code> \uc2e4\ud589 \uc2dc <code>--include_path tasks</code> \uc635\uc158\uc744 \uc790\ub3d9\uc73c\ub85c \ucd94\uac00\ud558\uc5ec <code>tasks/</code> \ub514\ub809\ud1a0\ub9ac\uc758 \ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c\ub97c \uc77d\uc5b4\uc635\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#2-dry-run","title":"2. Dry Run\uc73c\ub85c \uc2e4\ud589 \uc778\uc218 \ud655\uc778","text":"<p>\uc2e4\uc81c \ud3c9\uac00\ub97c \uc2e4\ud589\ud558\uae30 \uc804\uc5d0, <code>run_evalchemy.sh</code>\uac00 \uc0dd\uc131\ud558\ub294 <code>lm_eval</code> \uba85\ub839\uc5b4\uac00 \uc62c\ubc14\ub978\uc9c0 <code>--dry-run</code> \uc635\uc158\uc73c\ub85c \ud655\uc778\ud569\ub2c8\ub2e4.</p> <pre><code># --dry-run \uc635\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\uc81c \uc2e4\ud589 \uc5c6\uc774 \uc0dd\uc131\ub418\ub294 \uba85\ub839\uc5b4\ub9cc \ucd9c\ub825\n# --run-id\ub294 \uacb0\uacfc\uac00 \uc800\uc7a5\ub420 \ub514\ub809\ud1a0\ub9ac \uc774\ub984\uc774\ubbc0\ub85c \ud14c\uc2a4\ud2b8 \ubaa9\uc801\uc5d0 \ub9de\uac8c \uc9c0\uc815\ud569\ub2c8\ub2e4.\n./run_evalchemy.sh --endpoint http://localhost/vllm/v1/completions --run-id test_custom_task_dry_run --dry-run\n</code></pre>"},{"location":"how-to-add-benchmark/#3_1","title":"3. \uc18c\ub7c9 \uc0d8\ud50c\ub85c \ud14c\uc2a4\ud2b8 \uc2e4\ud589","text":"<p><code>limit</code> \uc635\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc801\uc740 \uc218\uc758 \uc0d8\ud50c\ub85c \ube60\ub974\uac8c \ud14c\uc2a4\ud2b8\ub97c \uc644\ub8cc\ud558\uace0 \uc804\uccb4 \ud30c\uc774\ud504\ub77c\uc778\uc774 \uc815\uc0c1\uc801\uc73c\ub85c \ub3d9\uc791\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4.</p> <p><code>configs/eval_config.json</code>\uc5d0\uc11c <code>limit</code> \uac12\uc744 <code>5</code> \uc815\ub3c4\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. <pre><code>// ...\n    \"custom_task_1\": {\n      \"enabled\": true,\n      \"tasks\": [\"custom_task_1\"],\n      \"limit\": 5, // 5\uac1c \uc0d8\ud50c\ub9cc \ud14c\uc2a4\ud2b8\n// ...\n</code></pre></p> <p>\ud14c\uc2a4\ud2b8 \uc2e4\ud589: <pre><code># \uc2e4\uc81c \ud3c9\uac00 \uc2e4\ud589 (5\uac1c \uc0d8\ud50c)\n# --batch-size 1\uc740 \uc548\uc815\uc801\uc778 \ud14c\uc2a4\ud2b8\ub97c \uc704\ud574 \uad8c\uc7a5\ub429\ub2c8\ub2e4.\n./run_evalchemy.sh --endpoint http://localhost/vllm/v1/completions --run-id test_custom_task_limit5 --batch-size 1\n\n# \uc2e4\ud589 \ud6c4 \uacb0\uacfc \ud655\uc778\ncat results/test_custom_task_limit5/evalchemy_summary_test_custom_task_limit5.json | jq\n</code></pre></p>"},{"location":"how-to-add-benchmark/#4","title":"4. \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \uc2e4\uc81c \ud3c9\uac00 \uc2e4\ud589","text":"<p>\ud14c\uc2a4\ud2b8\uac00 \uc131\uacf5\uc801\uc73c\ub85c \uc644\ub8cc\ub418\uba74, <code>limit</code> \uac12\uc744 <code>null</code>\ub85c \ubcc0\uacbd\ud558\uc5ec \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ud3c9\uac00\ub97c \uc9c4\ud589\ud569\ub2c8\ub2e4.</p> <p><code>configs/eval_config.json</code>\uc5d0\uc11c <code>limit</code> \uac12\uc744 <code>null</code>\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4. <pre><code>// ...\n    \"custom_task_1\": {\n      \"enabled\": true,\n      \"tasks\": [\"custom_task_1\"],\n      \"limit\": null, // \uc804\uccb4 \ub370\uc774\ud130\uc14b \uc0ac\uc6a9\n// ...\n</code></pre></p> <p>\uc2e4\ud589: <pre><code># \ud504\ub85c\ub355\uc158\uc6a9 \uc2e4\ud589\n./run_evalchemy.sh --endpoint http://your-vllm-server:8000/v1/completions --run-id custom_task_full_eval_$(date +%Y%m%d)\n</code></pre> - <code>run_evalchemy.sh</code> \uc2a4\ud06c\ub9bd\ud2b8\ub294 \uc774\ubbf8 <code>--include_path tasks</code> \uc635\uc158\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc5b4 \ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c\ub97c \uc790\ub3d9\uc73c\ub85c \uc778\uc2dd\ud569\ub2c8\ub2e4.</p>"},{"location":"how-to-add-benchmark/#troubleshooting","title":"\ud83d\udd27 \ubb38\uc81c \ud574\uacb0 (Troubleshooting)","text":""},{"location":"how-to-add-benchmark/#1-lm_eval","title":"1. lm_eval\uc774 \ucee4\uc2a4\ud140 \ud0dc\uc2a4\ud06c\ub97c \uc778\uc2dd\ud558\uc9c0 \ubabb\ud558\ub294 \uacbd\uc6b0","text":"<p>\uc99d\uc0c1: <code>Task 'custom_task_1' not found</code> \uc5d0\ub7ec</p> <p>\ud574\uacb0 \ubc29\ubc95: <pre><code># 1\ub2e8\uacc4: \uc791\uc5c5 \ub514\ub809\ud1a0\ub9ac \ud655\uc778\n# eval/standard_evalchemy \ub514\ub809\ud1a0\ub9ac\uc5d0\uc11c \uc2e4\ud589\ud574\uc57c \ud569\ub2c8\ub2e4.\npwd\n\n# 2\ub2e8\uacc4: \ud0dc\uc2a4\ud06c \ubaa9\ub85d \ud655\uc778\n./run_evalchemy.sh --tasks list | grep custom_task_1\n\n# 3\ub2e8\uacc4: YAML \ud30c\uc77c \uad6c\ubb38 \uac80\uc0ac\npython3 -c \"import yaml; print(yaml.safe_load(open('tasks/custom_task_1.yaml')))\"\n\n# 4\ub2e8\uacc4: \ud0dc\uc2a4\ud06c \ub514\ub809\ud1a0\ub9ac \uad6c\uc870 \ud655\uc778\nls -la tasks/\n# \ub2e4\uc74c \ud30c\uc77c\ub4e4\uc774 \uc788\uc5b4\uc57c \ud568:\n# - __init__.py\n# - custom_task_1.yaml\n</code></pre></p>"},{"location":"how-to-add-benchmark/#2_2","title":"2. \ub370\uc774\ud130\uc14b \ub85c\ub529 \uc5d0\ub7ec","text":"<p>\uc99d\uc0c1: <code>Dataset 'custom_dataset' doesn't exist on the Hub</code> \ub610\ub294 <code>FileNotFoundError</code></p> <p>\uc6d0\uc778: YAML \ud30c\uc77c\uc5d0\uc11c \uc798\ubabb\ub41c \ub370\uc774\ud130\uc14b \uacbd\ub85c \ucc38\uc870</p> <p>\ud574\uacb0 \ubc29\ubc95: <pre><code># 1\ub2e8\uacc4: YAML \ud30c\uc77c\uc758 dataset_kwargs \uacbd\ub85c \ud655\uc778\n# \uacbd\ub85c\ub294 eval/standard_evalchemy \ub514\ub809\ud1a0\ub9ac\ub97c \uae30\uc900\uc73c\ub85c \ud55c \uc0c1\ub300 \uacbd\ub85c\uc5ec\uc57c \ud569\ub2c8\ub2e4.\ncat tasks/custom_task_1.yaml | grep -A 2 data_files\n\n# 2\ub2e8\uacc4: \uc2e4\uc81c \ud30c\uc77c \uc874\uc7ac \ud655\uc778\nls -la ../../../datasets/raw/custom_benchmark/\n# train.jsonl\uacfc test.jsonl\uc774 \uc788\uc5b4\uc57c \ud568\n</code></pre></p>"},{"location":"how-to-add-benchmark/#3-macos-cuda","title":"3. macOS\uc5d0\uc11c CUDA \uc5d0\ub7ec","text":"<p>\uc99d\uc0c1: <code>AssertionError: Torch not compiled with CUDA enabled</code></p> <p>\ud574\uacb0 \ubc29\ubc95: - <code>run_evalchemy.sh</code> \uc2a4\ud06c\ub9bd\ud2b8\ub294 macOS \ud658\uacbd\uc744 \uc790\ub3d9\uc73c\ub85c \uac10\uc9c0\ud558\uace0 <code>--device cpu</code> \uc635\uc158\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4. \ub9cc\uc57d \uc2a4\ud06c\ub9bd\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \uc9c1\uc811 <code>lm_eval</code>\uc744 \uc2e4\ud589\ud55c\ub2e4\uba74 <code>--device cpu</code> \uc635\uc158\uc744 \uba85\uc2dc\uc801\uc73c\ub85c \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <pre><code># \uc2a4\ud06c\ub9bd\ud2b8 \uc0ac\uc6a9 \uc2dc \uc790\ub3d9\uc73c\ub85c \ucc98\ub9ac\ub428\n./run_evalchemy.sh --endpoint http://localhost/vllm/v1/completions\n\n# \uc9c1\uc811 \uc2e4\ud589 \uc2dc\npython3 -m lm_eval --device cpu ...\n</code></pre>"},{"location":"how-to-add-benchmark/#4-yaml-split","title":"4. YAML \ud30c\uc77c split \uc5d0\ub7ec","text":"<p>\uc99d\uc0c1: <code>KeyError: 'test'</code> \ub610\ub294 <code>KeyError: 'validation'</code></p> <p>\uc6d0\uc778: \ub370\uc774\ud130\uc14b\uc5d0 \ud574\ub2f9 split\uc774 \uc5c6\uac70\ub098, JSONL \ud30c\uc77c\uc758 \ud0a4\uac00 \uc798\ubabb \uc9c0\uc815\ub428</p> <p>\ud574\uacb0 \ubc29\ubc95: <pre><code># 1\ub2e8\uacc4: JSONL \ud30c\uc77c \ub0b4\uc6a9 \ud655\uc778 (\ud0a4\uac00 'train', 'test'\ub85c \ub418\uc5b4 \uc788\ub294\uc9c0)\n# 2\ub2e8\uacc4: YAML \ud30c\uc77c\uc5d0\uc11c \uc62c\ubc14\ub978 split \uc0ac\uc6a9 (test_split: test)\n# 3\ub2e8\uacc4: \uc0ac\uc6a9 \uac00\ub2a5\ud55c split \ud655\uc778\npython3 -c \"\nimport datasets\nds = datasets.load_dataset('json', data_files={'train': '../../../datasets/raw/custom_benchmark/train.jsonl', 'test': '../../../datasets/raw/custom_benchmark/test.jsonl'})\nprint('Available splits:', list(ds.keys()))\n\"\n# \uc704 \ucf54\ub4dc\ub294 eval/standard_evalchemy \ub514\ub809\ud1a0\ub9ac\uc5d0\uc11c \uc2e4\ud589\ud574\uc57c \ud569\ub2c8\ub2e4.\n</code></pre></p>"},{"location":"how-to-add-benchmark/#5","title":"5. \uacb0\uacfc \ud30c\uc77c\uc774 \uc0dd\uc131\ub418\uc9c0 \uc54a\uc744 \ub54c","text":"<p>\uc99d\uc0c1: <code>results/{run_id}</code> \ub514\ub809\ud1a0\ub9ac\ub294 \uc0dd\uc131\ub418\uc5c8\uc9c0\ub9cc \ub0b4\uc6a9\uc774 \ube44\uc5b4\uc788\uc74c</p> <p>\uc6d0\uc778: VLLM \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc5f0\uacb0 \uc2e4\ud328, \ubaa8\ub378 \ucd94\ub860 \uc2e4\ud328 \ub4f1</p> <p>\ud574\uacb0 \ubc29\ubc95: <pre><code># 1\ub2e8\uacc4: \uc5d0\ub7ec \ub85c\uadf8 \ud655\uc778\ncat results/{run_id}/evalchemy_errors_{run_id}.log\n\n# 2\ub2e8\uacc4: VLLM \uc11c\ubc84 \uc0c1\ud0dc \ubc0f \uc5d4\ub4dc\ud3ec\uc778\ud2b8 URL \ud655\uc778\ncurl http://your-vllm-server:8000/v1/models\n\n# 3\ub2e8\uacc4: --log-level DEBUG \uc635\uc158\uc73c\ub85c \uc0c1\uc138 \ub85c\uadf8 \ud655\uc778\n./run_evalchemy.sh --endpoint ... --log-level DEBUG\n</code></pre></p>"},{"location":"how-to-add-benchmark/#_7","title":"\ud83d\udcda \ucc38\uace0 \uc790\ub8cc","text":"<ul> <li>lm-evaluation-harness v0.4+ Documentation</li> <li>Deepeval Documentation</li> <li>VLLM Documentation</li> <li>Argo Workflows</li> </ul>"},{"location":"how-to-add-benchmark/#_8","title":"\ud83e\udd1d \uae30\uc5ec\ud558\uae30","text":"<p>\uc0c8\ub85c\uc6b4 \ubca4\uce58\ub9c8\ud06c\ub97c \ucd94\uac00\ud55c \ud6c4\uc5d0\ub294:</p> <ol> <li>\ubb38\uc11c \uc5c5\ub370\uc774\ud2b8: \uc774 \uac00\uc774\ub4dc\uc640 README \uc5c5\ub370\uc774\ud2b8</li> <li>\ud14c\uc2a4\ud2b8 \ucd94\uac00: \ucda9\ubd84\ud55c \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \ubc0f \ud1b5\ud569 \ud14c\uc2a4\ud2b8 \uc791\uc131</li> <li>PR \uc0dd\uc131: \uc0c1\uc138\ud55c \uc124\uba85\uacfc \ud568\uaed8 Pull Request \uc0dd\uc131</li> <li>\ub9ac\ubdf0 \uc694\uccad: \ud300 \uba64\ubc84\ub4e4\uc758 \ucf54\ub4dc \ub9ac\ubdf0 \uc694\uccad</li> </ol> <p>\uc9c8\ubb38\uc774\ub098 \ub3c4\uc6c0\uc774 \ud544\uc694\ud558\uba74 \uc774\uc288\ub97c \uc0dd\uc131\ud558\uac70\ub098 \ud300 \ucc44\ub110\uc5d0 \ubb38\uc758\ud574 \uc8fc\uc138\uc694! \ud83d\ude80</p> <p>\uc8fc\uc758\uc0ac\ud56d: - \ubc29\ubc95 1: \uc804\uccb4 \ud0dc\uc2a4\ud06c \ubaa9\ub85d\uc5d0\uc11c <code>custom_task_1</code>, <code>custom_task_2</code>\uac00 \ubcf4\uc774\uba74 \uc131\uacf5 - \ubc29\ubc95 2: macOS\uc5d0\uc11c\ub294 \ubc18\ub4dc\uc2dc <code>--device cpu</code> \uc635\uc158 \ud544\uc694 (CUDA \ubbf8\uc9c0\uc6d0) - <code>--limit 2</code>\ub294 \ud14c\uc2a4\ud2b8\uc6a9\uc774\ubbc0\ub85c \uc2e4\uc81c \ud3c9\uac00\uc5d0\uc11c\ub294 \uc81c\uac70\ud574\uc57c \ud568</p>"},{"location":"local-testing-guide/","title":"\ub85c\uceec \ud14c\uc2a4\ud2b8 \uac00\uc774\ub4dc (macOS)","text":"<p>\uc774 \ubb38\uc11c\ub294 GitHub\uc5d0 \ud478\uc2dc\ud558\uae30 \uc804\uc5d0 macOS \ud658\uacbd\uc5d0\uc11c VLLM \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc744 \ub2e8\uacc4\ubcc4\ub85c \ud14c\uc2a4\ud2b8\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.</p>"},{"location":"local-testing-guide/#_1","title":"\ud83d\udccb \ubaa9\ucc28","text":"<ol> <li>\ud83d\udee0 \uc0ac\uc804 \uc900\ube44</li> <li>\u2699\ufe0f \ud658\uacbd \uc124\uc815</li> <li>\ud83e\uddea \ub2e8\uacc4\ubcc4 \ud14c\uc2a4\ud2b8</li> <li>\ud83d\udd17 \ud1b5\ud569 \ud14c\uc2a4\ud2b8</li> <li>\ud83d\udeab \ubb38\uc81c \ud574\uacb0</li> <li>\ud83d\ude80 \uc131\ub2a5 \ucd5c\uc801\ud654</li> </ol>"},{"location":"local-testing-guide/#_2","title":"\ud83d\udee0 \uc0ac\uc804 \uc900\ube44","text":""},{"location":"local-testing-guide/#_3","title":"\ud544\uc218 \ub3c4\uad6c \uc124\uce58","text":""},{"location":"local-testing-guide/#1-orbstack","title":"\uc635\uc158 1: OrbStack \uc0ac\uc6a9 (\uad8c\uc7a5)","text":"<pre><code># Homebrew \uc124\uce58 (\uc5c6\ub294 \uacbd\uc6b0)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# OrbStack \uc124\uce58 (Docker Desktop \ub300\uc2e0 \uad8c\uc7a5)\nbrew install --cask orbstack\n\n# \ud544\uc218 \ub3c4\uad6c\ub4e4 \uc124\uce58\nbrew install python@3.11 kubectl helm jq yq\n\n# PostgreSQL \uad00\ub828 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58 (pytest \uc758\uc874\uc131)\nbrew install postgresql@14 libpq\n\n# Python \ud328\ud0a4\uc9c0 \uad00\ub9ac\uc790 \uc5c5\uadf8\ub808\uc774\ub4dc\npip3 install --upgrade pip setuptools wheel\n</code></pre>"},{"location":"local-testing-guide/#2-docker-desktop","title":"\uc635\uc158 2: Docker Desktop \uc0ac\uc6a9","text":"<pre><code># Homebrew \uc124\uce58 (\uc5c6\ub294 \uacbd\uc6b0)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# \ud544\uc218 \ub3c4\uad6c\ub4e4 \uc124\uce58\nbrew install python@3.11 docker docker-compose kind kubectl helm jq yq\nbrew install --cask docker\n\n# PostgreSQL \uad00\ub828 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58 (pytest \uc758\uc874\uc131)\nbrew install postgresql@14 libpq\n\n# Python \ud328\ud0a4\uc9c0 \uad00\ub9ac\uc790 \uc5c5\uadf8\ub808\uc774\ub4dc\npip3 install --upgrade pip setuptools wheel\n</code></pre>"},{"location":"local-testing-guide/#orbstack","title":"OrbStack \uc124\uc815 (\uad8c\uc7a5)","text":"<pre><code># OrbStack \uc2dc\uc791\nopen -a OrbStack\n\n# OrbStack\uc774 \uc2e4\ud589\ub420 \ub54c\uae4c\uc9c0 \ub300\uae30\nwhile ! docker info &gt; /dev/null 2&gt;&amp;1; do\n    echo \"OrbStack \uc2dc\uc791 \ub300\uae30 \uc911...\"\n    sleep 3\ndone\n\necho \"OrbStack\uc774 \uc131\uacf5\uc801\uc73c\ub85c \uc2dc\uc791\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\"\n\n# Kubernetes \ud074\ub7ec\uc2a4\ud130 \ud65c\uc131\ud654 (OrbStack \ub0b4\uc7a5)\norb start k8s\n\n# kubectl \ucee8\ud14d\uc2a4\ud2b8 \ud655\uc778\nkubectl config current-context\nkubectl get nodes\n</code></pre>"},{"location":"local-testing-guide/#docker-desktop","title":"Docker Desktop \uc124\uc815 (\ub300\uc548)","text":"<pre><code># Docker Desktop \uc2dc\uc791\nopen -a Docker\n\n# Docker\uac00 \uc2e4\ud589\ub420 \ub54c\uae4c\uc9c0 \ub300\uae30\nwhile ! docker info &gt; /dev/null 2&gt;&amp;1; do\n    echo \"Docker \uc2dc\uc791 \ub300\uae30 \uc911...\"\n    sleep 5\ndone\n\necho \"Docker\uac00 \uc131\uacf5\uc801\uc73c\ub85c \uc2dc\uc791\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\"\n</code></pre>"},{"location":"local-testing-guide/#_4","title":"\ud504\ub85c\uc81d\ud2b8 \ud074\ub860 \ubc0f \uc774\ub3d9","text":"<pre><code># \ud504\ub85c\uc81d\ud2b8 \ub514\ub809\ud1a0\ub9ac\ub85c \uc774\ub3d9\ncd /path/to/your/vllm-eval\n\n# \ud604\uc7ac \uc704\uce58 \ud655\uc778\npwd\nls -la\n</code></pre>"},{"location":"local-testing-guide/#_5","title":"\u2699\ufe0f \ud658\uacbd \uc124\uc815","text":""},{"location":"local-testing-guide/#1-python","title":"1. Python \uac00\uc0c1\ud658\uacbd \uc124\uc815","text":"<pre><code># Python 3.11 \uac00\uc0c1\ud658\uacbd \uc0dd\uc131\npython3.11 -m venv venv\n\n# \uac00\uc0c1\ud658\uacbd \ud65c\uc131\ud654\nsource venv/bin/activate\n\n# \uac00\uc0c1\ud658\uacbd \ud655\uc778\nwhich python\npython --version  # Python 3.11.x \ud655\uc778\n</code></pre>"},{"location":"local-testing-guide/#2","title":"2. \uc758\uc874\uc131 \uc124\uce58","text":"<pre><code># \uac1c\ubc1c \uc758\uc874\uc131 \uc124\uce58\npip install -r requirements-dev.txt\npip install -r requirements-test.txt\n\n# PostgreSQL \uad00\ub828 Python \ud328\ud0a4\uc9c0 \uc124\uce58 (\ud14c\uc2a4\ud2b8 \uc758\uc874\uc131)\nexport LDFLAGS=\"-L/opt/homebrew/opt/libpq/lib\"\nexport CPPFLAGS=\"-I/opt/homebrew/opt/libpq/include\"\nexport PKG_CONFIG_PATH=\"/opt/homebrew/opt/libpq/lib/pkgconfig\"\npip install \"psycopg[binary]\"\n\n# \ucd94\uac00 \ud328\ud0a4\uc9c0 \uc124\uce58 (\ud544\uc694\ud55c \uacbd\uc6b0)\npip install deepeval lm-eval torch transformers datasets\n</code></pre>"},{"location":"local-testing-guide/#3","title":"3. \ud658\uacbd \ubcc0\uc218 \uc124\uc815","text":"<pre><code># .env \ud30c\uc77c \uc0dd\uc131\ncat &gt; .env &lt;&lt; 'EOF'\n# \ud14c\uc2a4\ud2b8 \ud658\uacbd \ubcc0\uc218\nPYTHONPATH=.\nLOG_LEVEL=DEBUG\nEVAL_CONFIG_PATH=configs/evalchemy.json\nOUTPUT_DIR=./test_results\nRUN_ID=local_test_$(date +%Y%m%d_%H%M%S)\n\n# \ud14c\uc2a4\ud2b8\uc6a9 \ubaa8\ub378 \uc5d4\ub4dc\ud3ec\uc778\ud2b8 (\uc2e4\uc81c \uc11c\ube44\uc2a4 \ub300\uc2e0 Mock \uc0ac\uc6a9)\nVLLM_MODEL_ENDPOINT=http://localhost:8000/v1\n\n# MinIO \ud14c\uc2a4\ud2b8 \uc124\uc815\nMINIO_ENDPOINT=localhost:9000\nMINIO_ACCESS_KEY=minioadmin\nMINIO_SECRET_KEY=minioadmin\nMINIO_BUCKET=llm-eval-ds-test\n\n# ClickHouse \ud14c\uc2a4\ud2b8 \uc124\uc815\nCLICKHOUSE_HOST=localhost\nCLICKHOUSE_PORT=8123\nCLICKHOUSE_USER=default\nCLICKHOUSE_PASSWORD=\nCLICKHOUSE_DATABASE=vllm_eval_test\nEOF\n\n# \ud658\uacbd \ubcc0\uc218 \ub85c\ub4dc\nsource .env\n</code></pre>"},{"location":"local-testing-guide/#_6","title":"\ud83e\uddea \ub2e8\uacc4\ubcc4 \ud14c\uc2a4\ud2b8","text":""},{"location":"local-testing-guide/#1","title":"1\ub2e8\uacc4: \ucf54\ub4dc \ud488\uc9c8 \uac80\uc0ac","text":"<pre><code>pip install ruff black isort mypy\n\necho \"=== 1\ub2e8\uacc4: \ucf54\ub4dc \ud488\uc9c8 \uac80\uc0ac ===\"\n\n# Ruff \ub9b0\ud305\necho \"Ruff \ub9b0\ud305 \uc2e4\ud589 \uc911...\"\nruff check . --fix\necho \"\u2705 Ruff \ub9b0\ud305 \uc644\ub8cc\"\n\n# Black \ud3ec\ub9f7\ud305\necho \"Black \ud3ec\ub9f7\ud305 \uc2e4\ud589 \uc911...\"\nblack . --check --diff\necho \"\u2705 Black \ud3ec\ub9f7\ud305 \ud655\uc778 \uc644\ub8cc\"\n\n# isort import \uc815\ub82c\necho \"isort import \uc815\ub82c \ud655\uc778 \uc911...\"\nisort . --check-only --diff\necho \"\u2705 isort \ud655\uc778 \uc644\ub8cc\"\n\n# MyPy \ud0c0\uc785 \uac80\uc0ac\necho \"MyPy \ud0c0\uc785 \uac80\uc0ac \uc2e4\ud589 \uc911...\"\nmypy scripts/ eval/ --ignore-missing-imports\necho \"\u2705 MyPy \ud0c0\uc785 \uac80\uc0ac \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#2_1","title":"2\ub2e8\uacc4: \uc2a4\ud0a4\ub9c8 \ubc0f \uc124\uc815 \uac80\uc99d","text":"<pre><code>echo \"=== 2\ub2e8\uacc4: \uc2a4\ud0a4\ub9c8 \ubc0f \uc124\uc815 \uac80\uc99d ===\"\n\n# \uc2a4\ud0a4\ub9c8 \uac80\uc99d\necho \"\uc2a4\ud0a4\ub9c8 \uac80\uc99d \uc2e4\ud589 \uc911...\"\npython scripts/validate_schemas.py\necho \"\u2705 \uc2a4\ud0a4\ub9c8 \uac80\uc99d \uc644\ub8cc\"\n\n# \uc124\uc815 \ud30c\uc77c \uac80\uc99d\necho \"Evalchemy \uc124\uc815 \uac80\uc99d \uc911...\"\npython -c \"\nimport json\nwith open('configs/evalchemy.json', 'r') as f:\n    config = json.load(f)\n    print(f'\u2705 \uc124\uc815 \ud30c\uc77c \uc720\ud6a8: {len(config[\\\"benchmarks\\\"])}\uac1c \ubca4\uce58\ub9c8\ud06c \ubc1c\uacac')\n\"\n\n# \ub370\uc774\ud130\uc14b \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \uac80\uc99d\necho \"\ub370\uc774\ud130\uc14b \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \uac80\uc99d \uc911...\"\nif [ -f \"datasets/dataset_manifest.json\" ]; then\n    python -c \"\nimport json\nwith open('datasets/dataset_manifest.json', 'r') as f:\n    manifest = json.load(f)\n    print(f'\u2705 \ub370\uc774\ud130\uc14b \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \uc720\ud6a8: {len(manifest.get(\\\"datasets\\\", []))}\uac1c \ub370\uc774\ud130\uc14b')\n\"\nelse\n    echo \"\u26a0\ufe0f  \ub370\uc774\ud130\uc14b \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \ud30c\uc77c\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \ud14c\uc2a4\ud2b8\uc6a9\uc73c\ub85c \uc0dd\uc131\ud569\ub2c8\ub2e4.\"\n    mkdir -p datasets\n    cat &gt; datasets/dataset_manifest.json &lt;&lt; 'EOF'\n{\n  \"version\": \"1.0.0\",\n  \"datasets\": [\n    {\n      \"name\": \"test_rag_dataset\",\n      \"version\": \"sha256:test123\",\n      \"path\": \"test_rag_dataset.jsonl\",\n      \"size\": 1000,\n      \"created_at\": \"2024-01-01T00:00:00Z\"\n    }\n  ]\n}\nEOF\n    echo \"\u2705 \ud14c\uc2a4\ud2b8\uc6a9 \ub370\uc774\ud130\uc14b \ub9e4\ub2c8\ud398\uc2a4\ud2b8 \uc0dd\uc131 \uc644\ub8cc\"\nfi\n</code></pre>"},{"location":"local-testing-guide/#3_1","title":"3\ub2e8\uacc4: \ub2e8\uc704 \ud14c\uc2a4\ud2b8","text":"<pre><code>echo \"=== 3\ub2e8\uacc4: \ub2e8\uc704 \ud14c\uc2a4\ud2b8 ===\"\n\n# pytest \uc2e4\ud589 (\ube60\ub978 \ud14c\uc2a4\ud2b8\ub9cc)\necho \"\ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc2e4\ud589 \uc911...\"\npython -m pytest eval/deepeval_tests/test_llm_rag.py -v --tb=short -x\n\n# \ud14c\uc2a4\ud2b8 \ucee4\ubc84\ub9ac\uc9c0 \ud655\uc778\necho \"\ud14c\uc2a4\ud2b8 \ucee4\ubc84\ub9ac\uc9c0 \ud655\uc778 \uc911...\"\npython -m pytest eval/deepeval_tests/test_llm_rag.py --cov=eval --cov-report=term-missing --cov-report=html\n\necho \"\u2705 \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc644\ub8cc\"\necho \"\ud83d\udcca \ucee4\ubc84\ub9ac\uc9c0 \ub9ac\ud3ec\ud2b8: htmlcov/index.html\"\n</code></pre>"},{"location":"local-testing-guide/#4","title":"4\ub2e8\uacc4: \uc2a4\ud06c\ub9bd\ud2b8 \uae30\ub2a5 \ud14c\uc2a4\ud2b8","text":"<pre><code>echo \"=== 4\ub2e8\uacc4: \uc2a4\ud06c\ub9bd\ud2b8 \uae30\ub2a5 \ud14c\uc2a4\ud2b8 ===\"\n\n# \uc911\ubcf5 \uc81c\uac70 \uc2a4\ud06c\ub9bd\ud2b8 \ud14c\uc2a4\ud2b8\necho \"\uc911\ubcf5 \uc81c\uac70 \uc2a4\ud06c\ub9bd\ud2b8 \ud14c\uc2a4\ud2b8 \uc911...\"\nmkdir -p test_datasets\ncat &gt; test_datasets/test_data.jsonl &lt;&lt; 'EOF'\n{\"input\": \"What is AI?\", \"output\": \"AI is artificial intelligence.\"}\n{\"input\": \"What is AI?\", \"output\": \"AI is artificial intelligence.\"}\n{\"input\": \"What is ML?\", \"output\": \"ML is machine learning.\"}\nEOF\n\npython scripts/dedup_datasets.py \\\n    --input-dir test_datasets \\\n    --output-dir test_datasets/deduped \\\n    --threshold 0.2 \\\n    --dry-run\n\necho \"\u2705 \uc911\ubcf5 \uc81c\uac70 \uc2a4\ud06c\ub9bd\ud2b8 \ud14c\uc2a4\ud2b8 \uc644\ub8cc\"\n\n# Evalchemy \uc2a4\ud06c\ub9bd\ud2b8 \uac80\uc99d \ud14c\uc2a4\ud2b8\necho \"Evalchemy \uc2a4\ud06c\ub9bd\ud2b8 \uac80\uc99d \ud14c\uc2a4\ud2b8 \uc911...\"\nchmod +x eval/evalchemy/run_evalchemy.sh\n./eval/evalchemy/run_evalchemy.sh --help\n./eval/evalchemy/run_evalchemy.sh --validate-config\n./eval/evalchemy/run_evalchemy.sh --list-benchmarks\n\necho \"\u2705 Evalchemy \uc2a4\ud06c\ub9bd\ud2b8 \ud14c\uc2a4\ud2b8 \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#5-docker","title":"5\ub2e8\uacc4: Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ud14c\uc2a4\ud2b8","text":"<pre><code>echo \"=== 5\ub2e8\uacc4: Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ud14c\uc2a4\ud2b8 ===\"\n\n# Deepeval \uc774\ubbf8\uc9c0 \ube4c\ub4dc\necho \"Deepeval \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc911...\"\ndocker build -f docker/deepeval.Dockerfile -t vllm-eval/deepeval:test .\necho \"\u2705 Deepeval \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc644\ub8cc\"\n\n# Evalchemy \uc774\ubbf8\uc9c0 \ube4c\ub4dc (GPU \uc5c6\uc774 CPU \ubc84\uc804)\necho \"Evalchemy \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc911...\"\ndocker build -f docker/evalchemy-cpu.Dockerfile -t vllm-eval/evalchemy:test .\necho \"\u2705 Evalchemy \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc644\ub8cc\"\n\n# Workflow Tools \uc774\ubbf8\uc9c0 \ube4c\ub4dc\necho \"Workflow Tools \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc911...\"\ndocker build -f docker/workflow-tools.Dockerfile -t vllm-eval/workflow-tools:test .\necho \"\u2705 Workflow Tools \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc644\ub8cc\"\n\n# \uc774\ubbf8\uc9c0 \ud06c\uae30 \ud655\uc778\necho \"\ube4c\ub4dc\ub41c \uc774\ubbf8\uc9c0 \ud06c\uae30:\"\ndocker images | grep vllm-eval\n</code></pre>"},{"location":"local-testing-guide/#6","title":"6\ub2e8\uacc4: \ucee8\ud14c\uc774\ub108 \uc2e4\ud589 \ud14c\uc2a4\ud2b8","text":"<pre><code>echo \"=== 6\ub2e8\uacc4: \ucee8\ud14c\uc774\ub108 \uc2e4\ud589 \ud14c\uc2a4\ud2b8 ===\"\n\n# Deepeval \ucee8\ud14c\uc774\ub108 \ud14c\uc2a4\ud2b8\necho \"Deepeval \ucee8\ud14c\uc774\ub108 \ud14c\uc2a4\ud2b8 \uc911...\"\ndocker run --rm vllm-eval/deepeval:test python -c \"\nimport deepeval\nfrom deepeval.test_case import LLMTestCase\nprint('\u2705 Deepeval \ucee8\ud14c\uc774\ub108 \uc815\uc0c1 \uc791\ub3d9')\n\"\n\n# Workflow Tools \ucee8\ud14c\uc774\ub108 \ud14c\uc2a4\ud2b8\necho \"Workflow Tools \ucee8\ud14c\uc774\ub108 \ud14c\uc2a4\ud2b8 \uc911...\"\ndocker run --rm vllm-eval/workflow-tools:test sh -c \"\nyq --version &amp;&amp; \\\njq --version &amp;&amp; \\\nkubectl version --client &amp;&amp; \\\nhelm version --client\necho '\u2705 Workflow Tools \ucee8\ud14c\uc774\ub108 \uc815\uc0c1 \uc791\ub3d9'\n\"\n\necho \"\u2705 \ucee8\ud14c\uc774\ub108 \uc2e4\ud589 \ud14c\uc2a4\ud2b8 \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#_7","title":"\ud83d\udd17 \ud1b5\ud569 \ud14c\uc2a4\ud2b8","text":""},{"location":"local-testing-guide/#orbstack-kubernetes","title":"OrbStack Kubernetes \ud074\ub7ec\uc2a4\ud130 \ud14c\uc2a4\ud2b8 (\uad8c\uc7a5)","text":"<pre><code>echo \"=== \ud1b5\ud569 \ud14c\uc2a4\ud2b8: OrbStack Kubernetes ===\"\n\n# OrbStack Kubernetes \ud074\ub7ec\uc2a4\ud130 \uc0dd\uc131/\ud65c\uc131\ud654\necho \"OrbStack Kubernetes \ud074\ub7ec\uc2a4\ud130 \uc124\uc815 \uc911...\"\norb start k8s\n\n# \ud074\ub7ec\uc2a4\ud130 \uc0c1\ud0dc \ud655\uc778\nkubectl config use-context orbstack\nkubectl cluster-info\nkubectl get nodes\n\n# \ub124\uc784\uc2a4\ud398\uc774\uc2a4 \uc0dd\uc131\nkubectl create namespace vllm-eval-test --dry-run=client -o yaml | kubectl apply -f -\n\necho \"\u2705 OrbStack Kubernetes \ud074\ub7ec\uc2a4\ud130 \uc900\ube44 \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#kind-kubernetes","title":"Kind \ud074\ub7ec\uc2a4\ud130\ub97c \uc774\uc6a9\ud55c \ub85c\uceec Kubernetes \ud14c\uc2a4\ud2b8 (\ub300\uc548)","text":"<pre><code>echo \"=== \ud1b5\ud569 \ud14c\uc2a4\ud2b8: Kind \ud074\ub7ec\uc2a4\ud130 ===\"\n\n# Kind \ud074\ub7ec\uc2a4\ud130 \uc0dd\uc131\necho \"Kind \ud074\ub7ec\uc2a4\ud130 \uc0dd\uc131 \uc911...\"\ncat &gt; kind-config.yaml &lt;&lt; 'EOF'\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 8080\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 8443\n    protocol: TCP\nEOF\n\nkind create cluster --name vllm-eval-test --config kind-config.yaml\n\n# \ud074\ub7ec\uc2a4\ud130 \uc0c1\ud0dc \ud655\uc778\nkubectl cluster-info --context kind-vllm-eval-test\nkubectl get nodes\n\necho \"\u2705 Kind \ud074\ub7ec\uc2a4\ud130 \uc0dd\uc131 \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#helm","title":"Helm \ucc28\ud2b8 \uac80\uc99d","text":"<pre><code>echo \"Helm \ucc28\ud2b8 \uac80\uc99d \uc911...\"\n\n# Argo Workflows \ucc28\ud2b8 \uac80\uc99d\nhelm lint charts/argo-workflows/\nhelm template test-argo charts/argo-workflows/ --debug --dry-run\n\n# ClickHouse \ucc28\ud2b8 \uac80\uc99d\nhelm lint charts/clickhouse/\nhelm template test-clickhouse charts/clickhouse/ --debug --dry-run\n\n# Grafana \ucc28\ud2b8 \uac80\uc99d\nhelm lint charts/grafana/\nhelm template test-grafana charts/grafana/ --debug --dry-run\n\necho \"\u2705 Helm \ucc28\ud2b8 \uac80\uc99d \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#minio-clickhouse","title":"\ub85c\uceec MinIO \ubc0f ClickHouse \ud14c\uc2a4\ud2b8","text":""},{"location":"local-testing-guide/#orbstack_1","title":"OrbStack \uc0ac\uc6a9 (\uad8c\uc7a5)","text":"<pre><code>echo \"\ub85c\uceec \uc11c\ube44\uc2a4 \ud14c\uc2a4\ud2b8 \ud658\uacbd \uad6c\uc131 \uc911 (OrbStack)...\"\n\n# OrbStack\uc5d0\uc11c \uc9c1\uc811 \ucee8\ud14c\uc774\ub108 \uc2e4\ud589\necho \"MinIO \ucee8\ud14c\uc774\ub108 \uc2dc\uc791 \uc911...\"\ndocker run -d --name minio-test \\\n  -p 9000:9000 -p 9001:9001 \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio:latest server /data --console-address \":9001\"\n\necho \"ClickHouse \ucee8\ud14c\uc774\ub108 \uc2dc\uc791 \uc911...\"\ndocker run -d --name clickhouse-test \\\n  -p 8123:8123 -p 9009:9009 \\\n  -e CLICKHOUSE_DB=vllm_eval_test \\\n  clickhouse/clickhouse-server:latest\n\n# \uc11c\ube44\uc2a4 \uc900\ube44 \ub300\uae30\necho \"\uc11c\ube44\uc2a4 \uc2dc\uc791 \ub300\uae30 \uc911...\"\nsleep 20\n\n# MinIO \uc5f0\uacb0 \ud14c\uc2a4\ud2b8\necho \"MinIO \uc5f0\uacb0 \ud14c\uc2a4\ud2b8 \uc911...\"\ncurl -f http://localhost:9000/minio/health/live || echo \"MinIO \uc5f0\uacb0 \uc2e4\ud328\"\n\n# ClickHouse \uc5f0\uacb0 \ud14c\uc2a4\ud2b8\necho \"ClickHouse \uc5f0\uacb0 \ud14c\uc2a4\ud2b8 \uc911...\"\ncurl -f http://localhost:8123/ping || echo \"ClickHouse \uc5f0\uacb0 \uc2e4\ud328\"\n\necho \"\u2705 \ub85c\uceec \uc11c\ube44\uc2a4 \ud14c\uc2a4\ud2b8 \uc644\ub8cc\"\n\n# \uc815\ub9ac\necho \"\ud14c\uc2a4\ud2b8 \ucee8\ud14c\uc774\ub108 \uc815\ub9ac \uc911...\"\ndocker stop minio-test clickhouse-test 2&gt;/dev/null || true\ndocker rm minio-test clickhouse-test 2&gt;/dev/null || true\n</code></pre>"},{"location":"local-testing-guide/#docker-compose","title":"Docker Compose \uc0ac\uc6a9 (\ub300\uc548)","text":"<pre><code>echo \"\ub85c\uceec \uc11c\ube44\uc2a4 \ud14c\uc2a4\ud2b8 \ud658\uacbd \uad6c\uc131 \uc911 (Docker Compose)...\"\n\n# Docker Compose\ub85c \ud14c\uc2a4\ud2b8 \uc11c\ube44\uc2a4 \uc2dc\uc791\ncat &gt; docker-compose.test.yml &lt;&lt; 'EOF'\nversion: '3.8'\nservices:\n  minio:\n    image: minio/minio:latest\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio_data:/data\n\n  clickhouse:\n    image: clickhouse/clickhouse-server:latest\n    ports:\n      - \"8123:8123\"\n      - \"9009:9009\"\n    environment:\n      CLICKHOUSE_DB: vllm_eval_test\n    volumes:\n      - clickhouse_data:/var/lib/clickhouse\n\nvolumes:\n  minio_data:\n  clickhouse_data:\nEOF\n\ndocker-compose -f docker-compose.test.yml up -d\n\n# \uc11c\ube44\uc2a4 \uc900\ube44 \ub300\uae30\necho \"\uc11c\ube44\uc2a4 \uc2dc\uc791 \ub300\uae30 \uc911...\"\nsleep 30\n\n# MinIO \uc5f0\uacb0 \ud14c\uc2a4\ud2b8\necho \"MinIO \uc5f0\uacb0 \ud14c\uc2a4\ud2b8 \uc911...\"\ncurl -f http://localhost:9000/minio/health/live || echo \"MinIO \uc5f0\uacb0 \uc2e4\ud328\"\n\n# ClickHouse \uc5f0\uacb0 \ud14c\uc2a4\ud2b8\necho \"ClickHouse \uc5f0\uacb0 \ud14c\uc2a4\ud2b8 \uc911...\"\ncurl -f http://localhost:8123/ping || echo \"ClickHouse \uc5f0\uacb0 \uc2e4\ud328\"\n\necho \"\u2705 \ub85c\uceec \uc11c\ube44\uc2a4 \ud14c\uc2a4\ud2b8 \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#_8","title":"\ud83d\udd27 \ubb38\uc81c \ud574\uacb0","text":""},{"location":"local-testing-guide/#_9","title":"\uc77c\ubc18\uc801\uc778 \ubb38\uc81c\ub4e4","text":""},{"location":"local-testing-guide/#1-python_1","title":"1. Python \uc758\uc874\uc131 \ucda9\ub3cc","text":"<pre><code># \uac00\uc0c1\ud658\uacbd \uc7ac\uc0dd\uc131\ndeactivate\nrm -rf venv\npython3.11 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"local-testing-guide/#2-docker","title":"2. Docker \ube4c\ub4dc \uc2e4\ud328","text":"<pre><code># Docker \uce90\uc2dc \uc815\ub9ac\ndocker system prune -f\ndocker builder prune -f\n\n# \ube4c\ub4dc \ub85c\uadf8 \uc0c1\uc138 \ud655\uc778\ndocker build -f docker/deepeval.Dockerfile -t vllm-eval/deepeval:test . --no-cache --progress=plain\n</code></pre>"},{"location":"local-testing-guide/#3-orbstack","title":"3. OrbStack \ud074\ub7ec\uc2a4\ud130 \ubb38\uc81c","text":"<pre><code># OrbStack Kubernetes \ud074\ub7ec\uc2a4\ud130 \uc7ac\uc2dc\uc791\norb delete k8s\norb start k8s\n\n# \ucee8\ud14d\uc2a4\ud2b8 \uc7ac\uc124\uc815\nkubectl config use-context orbstack\n</code></pre>"},{"location":"local-testing-guide/#4-kind","title":"4. Kind \ud074\ub7ec\uc2a4\ud130 \ubb38\uc81c (\ub300\uc548)","text":"<pre><code># \ud074\ub7ec\uc2a4\ud130 \uc0ad\uc81c \ud6c4 \uc7ac\uc0dd\uc131\nkind delete cluster --name vllm-eval-test\nkind create cluster --name vllm-eval-test --config kind-config.yaml\n</code></pre>"},{"location":"local-testing-guide/#5","title":"5. \ud3ec\ud2b8 \ucda9\ub3cc","text":"<pre><code># \uc0ac\uc6a9 \uc911\uc778 \ud3ec\ud2b8 \ud655\uc778\nlsof -i :8000\nlsof -i :9000\nlsof -i :8123\n\n# \ud504\ub85c\uc138\uc2a4 \uc885\ub8cc\nkill -9 &lt;PID&gt;\n\n# OrbStack \ucee8\ud14c\uc774\ub108 \ud655\uc778 \ubc0f \uc815\ub9ac\ndocker ps | grep -E \"(minio|clickhouse)\"\ndocker stop $(docker ps -q --filter \"name=minio-test\")\ndocker stop $(docker ps -q --filter \"name=clickhouse-test\")\n</code></pre>"},{"location":"local-testing-guide/#6-postgresql-pytest","title":"6. PostgreSQL \uc758\uc874\uc131 \ubb38\uc81c (pytest \uc2e4\ud589 \uc2dc)","text":"<pre><code># \uc99d\uc0c1: pytest \uc2e4\ud589 \uc2dc \ub2e4\uc74c\uacfc \uac19\uc740 \uc624\ub958 \ubc1c\uc0dd\n# ImportError: no pq wrapper available.\n# - couldn't import psycopg 'c' implementation: No module named 'psycopg_c'\n# - couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n# - couldn't import psycopg 'python' implementation: libpq library not found\n\n# \ud574\uacb0 \ubc29\ubc95 1: PostgreSQL \ubc0f libpq \uc124\uce58\nbrew install postgresql@14 libpq\n\n# \ud574\uacb0 \ubc29\ubc95 2: \ud658\uacbd\ubcc0\uc218 \uc124\uc815 \ubc0f psycopg \uc7ac\uc124\uce58\nexport LDFLAGS=\"-L/opt/homebrew/opt/libpq/lib\"\nexport CPPFLAGS=\"-I/opt/homebrew/opt/libpq/include\"\nexport PKG_CONFIG_PATH=\"/opt/homebrew/opt/libpq/lib/pkgconfig\"\n\n# psycopg[binary] \uc7ac\uc124\uce58\npip install \"psycopg[binary]\" --force-reinstall\n\n# \uac80\uc99d: pytest \uc2e4\ud589 \ud14c\uc2a4\ud2b8\npython -m pytest eval/deepeval_tests/test_llm_rag.py -v --tb=short -x\n\necho \"\u2705 PostgreSQL \uc758\uc874\uc131 \ubb38\uc81c \ud574\uacb0 \uc644\ub8cc\"\n</code></pre>"},{"location":"local-testing-guide/#7-orbstack","title":"7. OrbStack \uad00\ub828 \ubb38\uc81c","text":"<pre><code># OrbStack \uc7ac\uc2dc\uc791\norb restart\n\n# OrbStack \uc0c1\ud0dc \ud655\uc778\norb status\n\n# Docker \ucee8\ud14d\uc2a4\ud2b8 \ud655\uc778\ndocker context ls\ndocker context use orbstack\n\n# Kubernetes \ucee8\ud14d\uc2a4\ud2b8 \ud655\uc778\nkubectl config get-contexts\nkubectl config use-context orbstack\n</code></pre>"},{"location":"local-testing-guide/#_10","title":"\ub85c\uadf8 \ud655\uc778 \ubc29\ubc95","text":"<pre><code># \ud14c\uc2a4\ud2b8 \ub85c\uadf8 \ud655\uc778\ntail -f test_results/*/evalchemy_*.log\n\n# Docker \ucee8\ud14c\uc774\ub108 \ub85c\uadf8\ndocker logs &lt;container_id&gt;\n\n# Kubernetes \ub85c\uadf8\nkubectl logs -f &lt;pod_name&gt;\n</code></pre>"},{"location":"local-testing-guide/#_11","title":"\u26a1 \uc131\ub2a5 \ucd5c\uc801\ud654","text":""},{"location":"local-testing-guide/#_12","title":"\ube4c\ub4dc \uc2dc\uac04 \ub2e8\ucd95","text":"<pre><code># OrbStack\uc5d0\uc11c Docker \ube4c\ub4dc \uce90\uc2dc \ud65c\uc6a9 (\uc790\ub3d9\uc73c\ub85c \ucd5c\uc801\ud654\ub428)\nexport DOCKER_BUILDKIT=1\n\n# OrbStack\uc758 \ube60\ub978 \ube4c\ub4dc \ud65c\uc6a9\ndocker build -f docker/deepeval.Dockerfile -t vllm-eval/deepeval:test .\n\n# \ubcd1\ub82c \ube4c\ub4dc (OrbStack\uc5d0\uc11c \uc790\ub3d9 \ucd5c\uc801\ud654)\ndocker build --parallel -f docker/deepeval.Dockerfile -t vllm-eval/deepeval:test .\n\n# OrbStack \ube4c\ub4dc \uce90\uc2dc \ud655\uc778\ndocker system df\n</code></pre>"},{"location":"local-testing-guide/#_13","title":"\ud14c\uc2a4\ud2b8 \uc2e4\ud589 \uc2dc\uac04 \ub2e8\ucd95","text":"<pre><code># \ube60\ub978 \ud14c\uc2a4\ud2b8\ub9cc \uc2e4\ud589\npython -m pytest -m \"not slow\" -x\n\n# \ubcd1\ub82c \ud14c\uc2a4\ud2b8 \uc2e4\ud589\npython -m pytest -n auto\n</code></pre>"},{"location":"local-testing-guide/#_14","title":"\ub9ac\uc18c\uc2a4 \uc0ac\uc6a9\ub7c9 \ubaa8\ub2c8\ud130\ub9c1","text":"<pre><code># \uc2dc\uc2a4\ud15c \ub9ac\uc18c\uc2a4 \ubaa8\ub2c8\ud130\ub9c1\ntop -pid $(pgrep -f python)\n\n# OrbStack \ub9ac\uc18c\uc2a4 \uc0ac\uc6a9\ub7c9 \ud655\uc778\norb status\ndocker stats\n\n# OrbStack \ub514\uc2a4\ud06c \uc0ac\uc6a9\ub7c9\norb info\ndocker system df\n\n# \ud504\ub85c\uc81d\ud2b8 \ub514\uc2a4\ud06c \uc0ac\uc6a9\ub7c9 \ud655\uc778\ndu -sh test_results/\ndu -sh venv/\n\n# OrbStack VM \ub9ac\uc18c\uc2a4 \ud655\uc778\norb config get resources\n</code></pre>"},{"location":"local-testing-guide/#_15","title":"\ud83d\udcdd \ud14c\uc2a4\ud2b8 \uccb4\ud06c\ub9ac\uc2a4\ud2b8","text":""},{"location":"local-testing-guide/#_16","title":"\ud478\uc2dc \uc804 \ud544\uc218 \ud655\uc778\uc0ac\ud56d","text":"<ul> <li>[ ] PostgreSQL/libpq \uc124\uce58 \ubc0f \ud658\uacbd\ubcc0\uc218 \uc124\uc815 \uc644\ub8cc</li> <li>[ ] \ubaa8\ub4e0 \ub9b0\ud2b8 \uac80\uc0ac \ud1b5\uacfc</li> <li>[ ] \ub2e8\uc704 \ud14c\uc2a4\ud2b8 100% \ud1b5\uacfc</li> <li>[ ] \uc2a4\ud0a4\ub9c8 \uac80\uc99d \ud1b5\uacfc</li> <li>[ ] Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc131\uacf5</li> <li>[ ] \ucee8\ud14c\uc774\ub108 \uc2e4\ud589 \ud14c\uc2a4\ud2b8 \ud1b5\uacfc</li> <li>[ ] Helm \ucc28\ud2b8 \uac80\uc99d \ud1b5\uacfc</li> <li>[ ] \ud1b5\ud569 \ud14c\uc2a4\ud2b8 \ud1b5\uacfc</li> <li>[ ] \ubb38\uc11c \uc5c5\ub370\uc774\ud2b8 \uc644\ub8cc</li> </ul>"},{"location":"local-testing-guide/#_17","title":"\uc131\ub2a5 \uae30\uc900","text":"<ul> <li>[ ] \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \uc2e4\ud589 \uc2dc\uac04 &lt; 2\ubd84</li> <li>[ ] Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc \uc2dc\uac04 &lt; 5\ubd84</li> <li>[ ] \uc804\uccb4 \ud14c\uc2a4\ud2b8 \uc2e4\ud589 \uc2dc\uac04 &lt; 10\ubd84</li> <li>[ ] \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 &lt; 4GB</li> <li>[ ] \ub514\uc2a4\ud06c \uc0ac\uc6a9\ub7c9 &lt; 2GB</li> </ul>"},{"location":"local-testing-guide/#_18","title":"\ud83d\ude80 \ucd5c\uc885 \uc2e4\ud589 \uc2a4\ud06c\ub9bd\ud2b8","text":"<p>\ubaa8\ub4e0 \ud14c\uc2a4\ud2b8\ub97c \ud55c \ubc88\uc5d0 \uc2e4\ud589\ud558\ub294 \uc2a4\ud06c\ub9bd\ud2b8:</p> <pre><code>#!/bin/bash\n# \uc804\uccb4 \ud14c\uc2a4\ud2b8 \uc2e4\ud589 \uc2a4\ud06c\ub9bd\ud2b8\n\nset -e\n\necho \"\ud83d\ude80 VLLM \ud3c9\uac00 \uc2dc\uc2a4\ud15c \ub85c\uceec \ud14c\uc2a4\ud2b8 \uc2dc\uc791\"\n\n# \ud658\uacbd \uc124\uc815\nsource venv/bin/activate\nsource .env\n\n# PostgreSQL \ud658\uacbd\ubcc0\uc218 \uc124\uc815 (macOS)\nexport LDFLAGS=\"-L/opt/homebrew/opt/libpq/lib\"\nexport CPPFLAGS=\"-I/opt/homebrew/opt/libpq/include\"\nexport PKG_CONFIG_PATH=\"/opt/homebrew/opt/libpq/lib/pkgconfig\"\n\n# 1\ub2e8\uacc4: \ucf54\ub4dc \ud488\uc9c8 \uac80\uc0ac\necho \"1\ufe0f\u20e3 \ucf54\ub4dc \ud488\uc9c8 \uac80\uc0ac...\"\nruff check . --fix\nblack . --check\nisort . --check-only\nmypy scripts/ eval/ --ignore-missing-imports\n\n# 2\ub2e8\uacc4: \uc2a4\ud0a4\ub9c8 \uac80\uc99d\necho \"2\ufe0f\u20e3 \uc2a4\ud0a4\ub9c8 \uac80\uc99d...\"\npython scripts/validate_schemas.py\n\n# 3\ub2e8\uacc4: \ub2e8\uc704 \ud14c\uc2a4\ud2b8\necho \"3\ufe0f\u20e3 \ub2e8\uc704 \ud14c\uc2a4\ud2b8...\"\npython -m pytest eval/deepeval_tests/test_llm_rag.py -v\n\n# 4\ub2e8\uacc4: \uc2a4\ud06c\ub9bd\ud2b8 \ud14c\uc2a4\ud2b8\necho \"4\ufe0f\u20e3 \uc2a4\ud06c\ub9bd\ud2b8 \ud14c\uc2a4\ud2b8...\"\n./eval/evalchemy/run_evalchemy.sh --validate-config\n\n# 5\ub2e8\uacc4: Docker \ube4c\ub4dc\necho \"5\ufe0f\u20e3 Docker \uc774\ubbf8\uc9c0 \ube4c\ub4dc...\"\ndocker build -f docker/deepeval.Dockerfile -t vllm-eval/deepeval:test .\ndocker build -f docker/workflow-tools.Dockerfile -t vllm-eval/workflow-tools:test .\n\n# 6\ub2e8\uacc4: Helm \uac80\uc99d\necho \"6\ufe0f\u20e3 Helm \ucc28\ud2b8 \uac80\uc99d...\"\nhelm lint charts/argo-workflows/\nhelm lint charts/clickhouse/\nhelm lint charts/grafana/\n\necho \"\u2705 \ubaa8\ub4e0 \ud14c\uc2a4\ud2b8 \uc644\ub8cc! GitHub\uc5d0 \ud478\uc2dc\ud560 \uc900\ube44\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\"\n</code></pre> <p>\uc774 \uc2a4\ud06c\ub9bd\ud2b8\ub97c <code>test-all.sh</code>\ub85c \uc800\uc7a5\ud558\uace0 \uc2e4\ud589\ud558\uba74 \ubaa8\ub4e0 \ud14c\uc2a4\ud2b8\ub97c \uc790\ub3d9\uc73c\ub85c \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>chmod +x test-all.sh\n./test-all.sh\n</code></pre>"},{"location":"local-testing-guide/#_19","title":"\ud83d\udcde \uc9c0\uc6d0","text":"<p>\ubb38\uc81c\uac00 \ubc1c\uc0dd\ud558\uba74 \ub2e4\uc74c\uc744 \ud655\uc778\ud558\uc138\uc694:</p> <ol> <li>\ubb38\uc81c \ud574\uacb0 \uc139\uc158</li> <li>\ub85c\uadf8 \ud30c\uc77c (<code>test_results/</code> \ub514\ub809\ud1a0\ub9ac)</li> <li>GitHub Issues \ud15c\ud50c\ub9bf \uc0ac\uc6a9\ud558\uc5ec \ubc84\uadf8 \ub9ac\ud3ec\ud2b8 \uc791\uc131</li> </ol>"},{"location":"local-testing-guide/#orbstack_2","title":"\ud83c\udf1f OrbStack \ud2b9\ubcc4 \uae30\ub2a5 \ud65c\uc6a9","text":""},{"location":"local-testing-guide/#orbstack_3","title":"OrbStack\uc758 \uc7a5\uc810","text":"<ol> <li>\ube60\ub978 \uc2dc\uc791 \uc2dc\uac04: Docker Desktop \ub300\ube44 2-3\ubc30 \ube60\ub978 \uc2dc\uc791</li> <li>\ub0ae\uc740 \ub9ac\uc18c\uc2a4 \uc0ac\uc6a9\ub7c9: \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 50% \uc808\uc57d</li> <li>\ub0b4\uc7a5 Kubernetes: \ubcc4\ub3c4 \uc124\uce58 \uc5c6\uc774 \uc989\uc2dc \uc0ac\uc6a9 \uac00\ub2a5</li> <li>\ub124\uc774\ud2f0\ube0c \uc131\ub2a5: Apple Silicon \ucd5c\uc801\ud654</li> <li>\uc790\ub3d9 \ud3ec\ud2b8 \ud3ec\uc6cc\ub529: \ubcf5\uc7a1\ud55c \ub124\ud2b8\uc6cc\ud06c \uc124\uc815 \ubd88\ud544\uc694</li> </ol>"},{"location":"local-testing-guide/#orbstack_4","title":"OrbStack \uace0\uae09 \uc124\uc815","text":"<pre><code># OrbStack \ub9ac\uc18c\uc2a4 \uc124\uc815 \ucd5c\uc801\ud654\norb config set resources.cpu 4\norb config set resources.memory 8GB\norb config set resources.disk 100GB\n\n# Kubernetes \ubc84\uc804 \uad00\ub9ac\norb list k8s-versions\norb start k8s vllm-eval-prod --version=1.29\n\n# \ub3c4\uba54\uc778 \uc124\uc815 (\uc790\ub3d9 DNS \ud574\uacb0)\norb config set domains.enabled true\n\n# \ud30c\uc77c \uacf5\uc720 \ucd5c\uc801\ud654\norb config set mount.type virtiofs\n</code></pre>"},{"location":"local-testing-guide/#orbstack_5","title":"OrbStack \ub124\ud2b8\uc6cc\ud0b9 \ud65c\uc6a9","text":"<pre><code># \uc790\ub3d9 \ub3c4\uba54\uc778 \uc811\uadfc (OrbStack \uace0\uc720 \uae30\ub2a5)\n# http://minio-test.orb.local:9000 \uc73c\ub85c \uc811\uadfc \uac00\ub2a5\n# http://clickhouse-test.orb.local:8123 \uc73c\ub85c \uc811\uadfc \uac00\ub2a5\n\n# \ucee8\ud14c\uc774\ub108 \uac04 \ud1b5\uc2e0 \ud14c\uc2a4\ud2b8\ndocker run --rm --name test-client alpine:latest \\\n  sh -c \"ping -c 3 minio-test &amp;&amp; ping -c 3 clickhouse-test\"\n\n# Kubernetes \uc11c\ube44\uc2a4 \uc811\uadfc\nkubectl port-forward svc/grafana 3000:3000 &amp;\n# http://localhost:3000 \ub610\ub294 http://grafana.orb.local:3000\n</code></pre>"},{"location":"local-testing-guide/#orbstack_6","title":"OrbStack \uc131\ub2a5 \ucd5c\uc801\ud654 \ud301","text":"<pre><code># \ube4c\ub4dc \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud55c \uc124\uc815\nexport DOCKER_BUILDKIT=1\nexport BUILDKIT_PROGRESS=plain\n\n# OrbStack \uce90\uc2dc \ucd5c\uc801\ud654\ndocker builder prune --filter until=24h\norb prune --all\n\n# \uac1c\ubc1c \ud658\uacbd \uc2a4\ub0c5\uc0f7 \uc0dd\uc131\norb snapshot create vllm-eval-baseline\norb snapshot restore vllm-eval-baseline  # \ud544\uc694\uc2dc \ubcf5\uc6d0\n</code></pre> <p>\ucc38\uace0:  - \uc774 \uac00\uc774\ub4dc\ub294 macOS \ud658\uacbd\uc744 \uae30\uc900\uc73c\ub85c \uc791\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4.  - OrbStack\uc740 macOS \uc804\uc6a9\uc774\ubbc0\ub85c, Linux\ub098 Windows\uc5d0\uc11c\ub294 Docker Desktop\uc774\ub098 \ub2e4\ub978 \ub300\uc548\uc744 \uc0ac\uc6a9\ud558\uc138\uc694. - OrbStack \uc0ac\uc6a9 \uc2dc \ub354 \ube60\ub974\uace0 \ud6a8\uc728\uc801\uc778 \uac1c\ubc1c \ud658\uacbd\uc744 \uacbd\ud5d8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p>"},{"location":"local-vllm-evaluation-guide/","title":"\ud83d\ude80 macOS OrbStack \ud658\uacbd\uc5d0\uc11c VLLM \ub85c\uceec \ud3c9\uac00 \uac00\uc774\ub4dc","text":"<p>\uc774 \ubb38\uc11c\ub294 macOS\uc758 OrbStack \ud658\uacbd\uc5d0\uc11c VLLM \ubaa8\ub378\uc744 \uc2e4\ud589\ud558\uace0 LLM \ud3c9\uac00\ub97c \uc218\ud589\ud558\ub294 \ub2e8\uacc4\ubcc4 \uac00\uc774\ub4dc\uc785\ub2c8\ub2e4.</p>"},{"location":"local-vllm-evaluation-guide/#_1","title":"\ud83d\udccb \ubaa9\ucc28","text":"<ol> <li>\ud83d\udee0 \uc0ac\uc804 \uc694\uad6c\uc0ac\ud56d</li> <li>\ud83d\udd27 OrbStack \uc124\uce58 \ubc0f \uc124\uc815</li> <li>\ud83e\udd16 VLLM \uc11c\ubc84 \uc2e4\ud589</li> <li>\ud83d\udd2c \ud3c9\uac00 \ud658\uacbd \uad6c\ucd95</li> <li>\ud83e\uddea Deepeval \ud3c9\uac00 \uc2e4\ud589</li> <li>\ud83d\udcda Evalchemy \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589</li> <li>\ud83d\udcc8 \uacb0\uacfc \ubd84\uc11d \ubc0f \uc2dc\uac01\ud654</li> <li>\ud83d\udeab \ubb38\uc81c \ud574\uacb0</li> </ol>"},{"location":"local-vllm-evaluation-guide/#_2","title":"\ud83d\udee0 \uc0ac\uc804 \uc694\uad6c\uc0ac\ud56d","text":""},{"location":"local-vllm-evaluation-guide/#_3","title":"\uc2dc\uc2a4\ud15c \uc694\uad6c\uc0ac\ud56d","text":"<ul> <li>macOS: 13.0 \uc774\uc0c1 (Apple Silicon \uad8c\uc7a5)</li> <li>RAM: \ucd5c\uc18c 16GB, \uad8c\uc7a5 32GB</li> <li>\ub514\uc2a4\ud06c: \ucd5c\uc18c 20GB \uc5ec\uc720 \uacf5\uac04</li> <li>GPU: Apple Silicon GPU \ub610\ub294 NVIDIA GPU (\uc120\ud0dd\uc0ac\ud56d)</li> </ul>"},{"location":"local-vllm-evaluation-guide/#_4","title":"\ud544\uc218 \ub3c4\uad6c \uc124\uce58","text":"<pre><code># Homebrew \uc124\uce58 (\uc5c6\ub294 \uacbd\uc6b0)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# \ud544\uc218 \ub3c4\uad6c\ub4e4 \uc124\uce58\nbrew install python@3.11 git curl jq\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#orbstack","title":"\ud83d\udd27 OrbStack \uc124\uce58 \ubc0f \uc124\uc815","text":""},{"location":"local-vllm-evaluation-guide/#1-orbstack","title":"1. OrbStack \uc124\uce58","text":"<pre><code># OrbStack \uc124\uce58 (Docker Desktop \ub300\uc2e0 \uad8c\uc7a5)\nbrew install --cask orbstack\n\n# OrbStack \uc2dc\uc791\nopen -a OrbStack\n\n# OrbStack \uc2dc\uc791 \ud655\uc778\nwhile ! docker info &gt; /dev/null 2&gt;&amp;1; do\n    echo \"OrbStack \uc2dc\uc791 \ub300\uae30 \uc911...\"\n    sleep 3\ndone\necho \"\u2705 OrbStack\uc774 \uc131\uacf5\uc801\uc73c\ub85c \uc2dc\uc791\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\"\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2","title":"2. \ud504\ub85c\uc81d\ud2b8 \ud074\ub860 \ubc0f \uc124\uc815","text":"<pre><code># \ud504\ub85c\uc81d\ud2b8 \ud074\ub860\ngit clone https://github.com/your-org/vllm-eval.git\ncd vllm-eval\n\n# Python \uac00\uc0c1\ud658\uacbd \uc0dd\uc131\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# \ud544\uc218 \ud328\ud0a4\uc9c0 \uc124\uce58\npip install --upgrade pip\npip install -r requirements-dev.txt\npip install -r requirements-deepeval.txt\npip install -r requirements-evalchemy.txt\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#vllm","title":"\ud83e\udd16 VLLM \uc11c\ubc84 \uc2e4\ud589","text":""},{"location":"local-vllm-evaluation-guide/#1","title":"1. \ubaa8\ub378 \ub2e4\uc6b4\ub85c\ub4dc \ubc0f \uc2e4\ud589","text":"<pre><code># VLLM \uc11c\ubc84 \uc2e4\ud589 (\uc608: Qwen2-7B \ubaa8\ub378)\ndocker run -d \\\n  --name vllm-server \\\n  --gpus all \\\n  -p 8000:8000 \\\n  vllm/vllm-openai:latest \\\n  --model \"Qwen/Qwen2-7B-Instruct\" \\\n  --served-model-name \"qwen3-8b\" \\\n  --host 0.0.0.0 \\\n  --port 8000\n\n# \uc11c\ubc84 \uc0c1\ud0dc \ud655\uc778\ndocker logs vllm-server\n\n# API \ud14c\uc2a4\ud2b8\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"qwen3-8b\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"\ud55c\uad6d\uc758 \uc218\ub3c4\ub294 \uc5b4\ub514\uc778\uac00\uc694?\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 256,\n    \"top_p\": 0.95,\n    \"stream\": false\n  }'\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2_1","title":"2. \ubaa8\ub378 \uc11c\ubc84 \ud5ec\uc2a4\uccb4\ud06c","text":"<pre><code># \ubaa8\ub378 \ubaa9\ub85d \ud655\uc778\ncurl http://localhost:8000/v1/models | jq\n\n# \uc11c\ubc84 \uc0c1\ud0dc \ud655\uc778\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_5","title":"\ud83d\udd2c \ud3c9\uac00 \ud658\uacbd \uad6c\ucd95","text":""},{"location":"local-vllm-evaluation-guide/#1_1","title":"1. \ud658\uacbd \ubcc0\uc218 \uc124\uc815","text":"<pre><code># .env \ud30c\uc77c \uc0dd\uc131\ncat &gt; .env &lt;&lt; 'EOF'\n# VLLM \ubaa8\ub378 \uc5d4\ub4dc\ud3ec\uc778\ud2b8\nVLLM_MODEL_ENDPOINT=http://localhost:8000/v1\nMODEL_NAME=qwen3-8b\n\n# \ud3c9\uac00 \uc124\uc815\nEVAL_CONFIG_PATH=configs/evalchemy.json\nOUTPUT_DIR=./test_results\nRUN_ID=local_eval_$(date +%Y%m%d_%H%M%S)\n\n# \ub85c\uadf8 \uc124\uc815\nLOG_LEVEL=INFO\nPYTHONPATH=.\nEOF\n\n# \ud658\uacbd \ubcc0\uc218 \ub85c\ub4dc\nsource .env\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2_2","title":"2. \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b \uc900\ube44","text":"<pre><code># \uacb0\uacfc \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\nmkdir -p test_results\n\n# \ud14c\uc2a4\ud2b8\uc6a9 \ub370\uc774\ud130\uc14b \uc0dd\uc131\nmkdir -p datasets/raw/local_test_dataset\ncat &gt; datasets/raw/local_test_dataset/test.jsonl &lt;&lt; 'EOF'\n{\"input\": \"\ud55c\uad6d\uc758 \uc218\ub3c4\ub294 \uc5b4\ub514\uc778\uac00\uc694?\", \"expected_output\": \"\ud55c\uad6d\uc758 \uc218\ub3c4\ub294 \uc11c\uc6b8\uc785\ub2c8\ub2e4.\", \"context\": \"\ud55c\uad6d \uc9c0\ub9ac\uc5d0 \uad00\ud55c \uc9c8\ubb38\uc785\ub2c8\ub2e4.\"}\n{\"input\": \"\ud30c\uc774\uc36c\uc5d0\uc11c \ub9ac\uc2a4\ud2b8\ub97c \uc815\ub82c\ud558\ub294 \ubc29\ubc95\uc740?\", \"expected_output\": \"\ud30c\uc774\uc36c\uc5d0\uc11c \ub9ac\uc2a4\ud2b8\ub97c \uc815\ub82c\ud558\ub824\uba74 sort() \uba54\uc11c\ub4dc\ub098 sorted() \ud568\uc218\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\", \"context\": \"\ud504\ub85c\uadf8\ub798\ubc0d \uad00\ub828 \uc9c8\ubb38\uc785\ub2c8\ub2e4.\"}\n{\"input\": \"\uc9c0\uad6c\uc758 \ub458\ub808\ub294 \uc5bc\ub9c8\ub098 \ub429\ub2c8\uae4c?\", \"expected_output\": \"\uc9c0\uad6c\uc758 \ub458\ub808\ub294 \uc57d 40,075km\uc785\ub2c8\ub2e4.\", \"context\": \"\uc9c0\uad6c\uacfc\ud559\uc5d0 \uad00\ud55c \uc9c8\ubb38\uc785\ub2c8\ub2e4.\"}\nEOF\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#deepeval","title":"\ud83e\uddea Deepeval \ud3c9\uac00 \uc2e4\ud589","text":""},{"location":"local-vllm-evaluation-guide/#1_2","title":"1. \ucee4\uc2a4\ud140 \ud3c9\uac00 \uc2a4\ud06c\ub9bd\ud2b8 \uc0dd\uc131","text":"<pre><code># \ub85c\uceec \ud3c9\uac00 \uc2a4\ud06c\ub9bd\ud2b8 \uc0dd\uc131\ncat &gt; scripts/run_local_deepeval.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"\n\ub85c\uceec VLLM \uc11c\ubc84\ub97c \uc774\uc6a9\ud55c Deepeval \ud3c9\uac00 \uc2a4\ud06c\ub9bd\ud2b8\n\"\"\"\n\nimport os\nimport json\nimport asyncio\nfrom typing import List, Dict, Any\nfrom deepeval import evaluate\nfrom deepeval.models import DeepEvalBaseLLM\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.metrics import (\n    ContextualPrecisionMetric,\n    ContextualRecallMetric,\n    ContextualRelevancyMetric,\n    AnswerRelevancyMetric\n)\nimport openai\nimport logging\n\n# \ub85c\uae45 \uc124\uc815\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass VLLMModel(DeepEvalBaseLLM):\n    \"\"\"VLLM OpenAI \ud638\ud658 API\ub97c \uc704\ud55c \ubaa8\ub378 \ud074\ub798\uc2a4\"\"\"\n\n    def __init__(self, model_name: str = \"qwen3-8b\", base_url: str = \"http://localhost:8000/v1\"):\n        self.model_name = model_name\n        self.client = openai.OpenAI(\n            base_url=base_url,\n            api_key=\"dummy\"  # VLLM\uc5d0\uc11c\ub294 API \ud0a4\uac00 \ud544\uc694\uc5c6\uc74c\n        )\n\n    def load_model(self):\n        return self.model_name\n\n    def generate(self, prompt: str, schema: Dict = None) -&gt; str:\n        \"\"\"\ud14d\uc2a4\ud2b8 \uc0dd\uc131\"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1,\n                max_tokens=512\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            logger.error(f\"Generation failed: {e}\")\n            return \"\"\n\n    async def a_generate(self, prompt: str, schema: Dict = None) -&gt; str:\n        \"\"\"\ube44\ub3d9\uae30 \ud14d\uc2a4\ud2b8 \uc0dd\uc131\"\"\"\n        return self.generate(prompt, schema)\n\n    def get_model_name(self) -&gt; str:\n        return self.model_name\n\ndef load_test_dataset(file_path: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"JSONL \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b \ub85c\ub4dc\"\"\"\n    test_cases = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            test_cases.append(json.loads(line.strip()))\n    return test_cases\n\ndef create_test_cases(dataset: List[Dict], model: VLLMModel) -&gt; List[LLMTestCase]:\n    \"\"\"\ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4 \uc0dd\uc131\"\"\"\n    test_cases = []\n\n    for item in dataset:\n        # \ubaa8\ub378\ub85c\ubd80\ud130 \uc2e4\uc81c \uc751\ub2f5 \uc0dd\uc131\n        actual_output = model.generate(item[\"input\"])\n\n        test_case = LLMTestCase(\n            input=item[\"input\"],\n            actual_output=actual_output,\n            expected_output=item[\"expected_output\"],\n            context=[item.get(\"context\", \"\")]\n        )\n        test_cases.append(test_case)\n        logger.info(f\"Created test case: {item['input'][:50]}...\")\n\n    return test_cases\n\ndef main():\n    \"\"\"\uba54\uc778 \ud3c9\uac00 \uc2e4\ud589\"\"\"\n    # \ubaa8\ub378 \ucd08\uae30\ud654\n    model = VLLMModel()\n\n    # \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b \ub85c\ub4dc\n    dataset_path = \"eval/deepeval_tests/datasets/test_local_dataset.jsonl\"\n    dataset = load_test_dataset(dataset_path)\n\n    # \ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4 \uc0dd\uc131\n    test_cases = create_test_cases(dataset, model)\n\n    # \ud3c9\uac00 \uba54\ud2b8\ub9ad \uc815\uc758\n    metrics = [\n        AnswerRelevancyMetric(\n            threshold=0.7,\n            model=model,\n            include_reason=True\n        ),\n        ContextualRelevancyMetric(\n            threshold=0.7,\n            model=model,\n            include_reason=True\n        )\n    ]\n\n    # \ud3c9\uac00 \uc2e4\ud589\n    logger.info(\"Starting evaluation...\")\n    results = evaluate(\n        test_cases=test_cases,\n        metrics=metrics,\n        print_results=True\n    )\n\n    # \uacb0\uacfc \uc800\uc7a5\n    output_dir = os.getenv(\"OUTPUT_DIR\", \"./test_results\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    results_file = f\"{output_dir}/deepeval_results_{os.getenv('RUN_ID', 'local')}.json\"\n    with open(results_file, 'w', encoding='utf-8') as f:\n        json.dump({\n            \"test_results\": [\n                {\n                    \"input\": tc.input,\n                    \"actual_output\": tc.actual_output,\n                    \"expected_output\": tc.expected_output,\n                    \"metrics\": {\n                        metric.__class__.__name__: {\n                            \"score\": getattr(metric, 'score', None),\n                            \"threshold\": getattr(metric, 'threshold', None),\n                            \"success\": getattr(metric, 'success', None),\n                            \"reason\": getattr(metric, 'reason', None)\n                        }\n                        for metric in metrics\n                    }\n                }\n                for tc in test_cases\n            ]\n        }, f, ensure_ascii=False, indent=2)\n\n    logger.info(f\"Results saved to: {results_file}\")\n    return results\n\nif __name__ == \"__main__\":\n    main()\nEOF\n\n# \uc2e4\ud589 \uad8c\ud55c \ubd80\uc5ec\nchmod +x scripts/run_local_deepeval.py\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2-deepeval","title":"2. Deepeval \uc2e4\ud589","text":"<pre><code># Deepeval \ud3c9\uac00 \uc2e4\ud589\npython scripts/run_local_deepeval.py\n\n# \uacb0\uacfc \ud655\uc778\nls -la test_results/\ncat test_results/deepeval_results_*.json | jq\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#evalchemy","title":"\u26a1 Evalchemy \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589","text":""},{"location":"local-vllm-evaluation-guide/#1-evalchemy","title":"1. \ub85c\uceec Evalchemy \uc124\uc815","text":"<pre><code># \ub85c\uceec\uc6a9 Evalchemy \uc124\uc815 \ud30c\uc77c \uc0dd\uc131\ncat &gt; eval/evalchemy/configs/local_eval_config.json &lt;&lt; 'EOF'\n{\n  \"benchmarks\": {\n    \"arc_easy\": {\n      \"enabled\": true,\n      \"tasks\": [\"arc_easy\"],\n      \"num_fewshot\": 5,\n      \"batch_size\": 4,\n      \"limit\": 10,\n      \"description\": \"ARC Easy \ubca4\uce58\ub9c8\ud06c (\ub85c\uceec \ud14c\uc2a4\ud2b8\uc6a9)\",\n      \"metrics\": [\"acc\", \"acc_norm\"]\n    },\n    \"hellaswag\": {\n      \"enabled\": true,\n      \"tasks\": [\"hellaswag\"],\n      \"num_fewshot\": 10,\n      \"batch_size\": 4,\n      \"limit\": 10,\n      \"description\": \"HellaSwag \ubca4\uce58\ub9c8\ud06c (\ub85c\uceec \ud14c\uc2a4\ud2b8\uc6a9)\",\n      \"metrics\": [\"acc\", \"acc_norm\"]\n    }\n  }\n}\nEOF\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2-evalchemy","title":"2. \ub85c\uceec Evalchemy \uc2e4\ud589 \uc2a4\ud06c\ub9bd\ud2b8","text":"<pre><code># \ub85c\uceec Evalchemy \uc2e4\ud589 \uc2a4\ud06c\ub9bd\ud2b8 \uc0dd\uc131\ncat &gt; scripts/run_local_evalchemy.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"\n\ub85c\uceec VLLM \uc11c\ubc84\ub97c \uc774\uc6a9\ud55c Evalchemy \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nimport logging\nfrom typing import Dict, Any\n\n# \ub85c\uae45 \uc124\uc815\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef run_evalchemy_benchmark(config_path: str, output_dir: str) -&gt; Dict[str, Any]:\n    \"\"\"Evalchemy \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589\"\"\"\n\n    # \ud658\uacbd \ubcc0\uc218 \uc124\uc815\n    env = os.environ.copy()\n    env.update({\n        \"VLLM_MODEL_ENDPOINT\": \"http://localhost:8000/v1\",\n        \"MODEL_NAME\": \"qwen3-8b\",\n        \"OUTPUT_DIR\": output_dir,\n        \"EVAL_CONFIG_PATH\": config_path\n    })\n\n    # lm_eval \uba85\ub839\uc5b4 \uad6c\uc131\n    cmd = [\n        \"lm_eval\",\n        \"--model\", \"openai-chat-completions\",\n        \"--model_args\", f\"base_url=http://localhost:8000/v1,model={env['MODEL_NAME']},tokenizer={env['MODEL_NAME']}\",\n        \"--tasks\", \"arc_easy,hellaswag\",\n        \"--num_fewshot\", \"5\",\n        \"--batch_size\", \"4\",\n        \"--limit\", \"10\",\n        \"--output_path\", f\"{output_dir}/evalchemy_results.json\",\n        \"--log_samples\"\n    ]\n\n    logger.info(f\"Running command: {' '.join(cmd)}\")\n\n    try:\n        # \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589\n        result = subprocess.run(\n            cmd,\n            env=env,\n            capture_output=True,\n            text=True,\n            timeout=3600  # 1\uc2dc\uac04 \ud0c0\uc784\uc544\uc6c3\n        )\n\n        if result.returncode == 0:\n            logger.info(\"Evalchemy benchmark completed successfully\")\n\n            # \uacb0\uacfc \ud30c\uc77c \uc77d\uae30\n            results_file = f\"{output_dir}/evalchemy_results.json\"\n            if os.path.exists(results_file):\n                with open(results_file, 'r') as f:\n                    results = json.load(f)\n                return results\n            else:\n                logger.warning(\"Results file not found\")\n                return {}\n        else:\n            logger.error(f\"Benchmark failed with return code: {result.returncode}\")\n            logger.error(f\"Error output: {result.stderr}\")\n            return {}\n\n    except subprocess.TimeoutExpired:\n        logger.error(\"Benchmark timed out\")\n        return {}\n    except Exception as e:\n        logger.error(f\"Benchmark failed with exception: {e}\")\n        return {}\n\ndef main():\n    \"\"\"\uba54\uc778 \uc2e4\ud589 \ud568\uc218\"\"\"\n    config_path = \"eval/evalchemy/configs/local_eval_config.json\"\n    output_dir = os.getenv(\"OUTPUT_DIR\", \"./test_results\")\n\n    # \ucd9c\ub825 \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\n    os.makedirs(output_dir, exist_ok=True)\n\n    # \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589\n    results = run_evalchemy_benchmark(config_path, output_dir)\n\n    if results:\n        logger.info(\"Benchmark results:\")\n        for task, metrics in results.get(\"results\", {}).items():\n            logger.info(f\"  {task}: {metrics}\")\n    else:\n        logger.error(\"No results obtained\")\n\nif __name__ == \"__main__\":\n    main()\nEOF\n\n# \uc2e4\ud589 \uad8c\ud55c \ubd80\uc5ec\nchmod +x scripts/run_local_evalchemy.py\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#3-evalchemy","title":"3. Evalchemy \uc2e4\ud589","text":"<pre><code># lm-evaluation-harness \uc124\uce58 (\ud544\uc694\ud55c \uacbd\uc6b0)\npip install lm-eval[openai]\n\n# Evalchemy \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589\npython scripts/run_local_evalchemy.py\n\n# \uacb0\uacfc \ud655\uc778\nls -la test_results/\ncat test_results/evalchemy_results.json | jq\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_6","title":"\ud83d\udcca \uacb0\uacfc \ubd84\uc11d \ubc0f \uc2dc\uac01\ud654","text":""},{"location":"local-vllm-evaluation-guide/#1_3","title":"1. \uacb0\uacfc \uc9d1\uacc4 \uc2a4\ud06c\ub9bd\ud2b8","text":"<pre><code># \uacb0\uacfc \uc9d1\uacc4 \uc2a4\ud06c\ub9bd\ud2b8 \uc0dd\uc131\ncat &gt; scripts/aggregate_local_results.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"\n\ub85c\uceec \ud3c9\uac00 \uacb0\uacfc \uc9d1\uacc4 \ubc0f \uc2dc\uac01\ud654\n\"\"\"\n\nimport os\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport logging\n\n# \ub85c\uae45 \uc124\uc815\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef load_deepeval_results(results_dir: str) -&gt; dict:\n    \"\"\"Deepeval \uacb0\uacfc \ub85c\ub4dc\"\"\"\n    results = {}\n    for file in os.listdir(results_dir):\n        if file.startswith(\"deepeval_results_\") and file.endswith(\".json\"):\n            with open(os.path.join(results_dir, file), 'r') as f:\n                results[file] = json.load(f)\n    return results\n\ndef load_evalchemy_results(results_dir: str) -&gt; dict:\n    \"\"\"Evalchemy \uacb0\uacfc \ub85c\ub4dc\"\"\"\n    results = {}\n    for file in os.listdir(results_dir):\n        if file.startswith(\"evalchemy_results\") and file.endswith(\".json\"):\n            with open(os.path.join(results_dir, file), 'r') as f:\n                results[file] = json.load(f)\n    return results\n\ndef create_summary_report(deepeval_results: dict, evalchemy_results: dict, output_dir: str):\n    \"\"\"\ud1b5\ud569 \ubcf4\uace0\uc11c \uc0dd\uc131\"\"\"\n    report = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"model_name\": os.getenv(\"MODEL_NAME\", \"unknown\"),\n        \"summary\": {\n            \"deepeval\": {},\n            \"evalchemy\": {}\n        }\n    }\n\n    # Deepeval \uacb0\uacfc \uc694\uc57d\n    if deepeval_results:\n        for filename, data in deepeval_results.items():\n            test_results = data.get(\"test_results\", [])\n            if test_results:\n                # \uba54\ud2b8\ub9ad\ubcc4 \ud3c9\uade0 \uacc4\uc0b0\n                metrics_summary = {}\n                for result in test_results:\n                    for metric_name, metric_data in result.get(\"metrics\", {}).items():\n                        if metric_name not in metrics_summary:\n                            metrics_summary[metric_name] = []\n                        if metric_data.get(\"score\") is not None:\n                            metrics_summary[metric_name].append(metric_data[\"score\"])\n\n                # \ud3c9\uade0 \uacc4\uc0b0\n                avg_metrics = {}\n                for metric_name, scores in metrics_summary.items():\n                    if scores:\n                        avg_metrics[metric_name] = {\n                            \"average_score\": sum(scores) / len(scores),\n                            \"count\": len(scores)\n                        }\n\n                report[\"summary\"][\"deepeval\"][filename] = avg_metrics\n\n    # Evalchemy \uacb0\uacfc \uc694\uc57d\n    if evalchemy_results:\n        for filename, data in evalchemy_results.items():\n            results = data.get(\"results\", {})\n            summary = {}\n            for task, metrics in results.items():\n                summary[task] = {\n                    \"accuracy\": metrics.get(\"acc\", 0),\n                    \"normalized_accuracy\": metrics.get(\"acc_norm\", 0)\n                }\n            report[\"summary\"][\"evalchemy\"][filename] = summary\n\n    # \ubcf4\uace0\uc11c \uc800\uc7a5\n    report_file = f\"{output_dir}/evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    with open(report_file, 'w', encoding='utf-8') as f:\n        json.dump(report, f, ensure_ascii=False, indent=2)\n\n    logger.info(f\"Summary report saved to: {report_file}\")\n    return report\n\ndef create_visualizations(report: dict, output_dir: str):\n    \"\"\"\uacb0\uacfc \uc2dc\uac01\ud654\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        # \uc2a4\ud0c0\uc77c \uc124\uc815\n        plt.style.use('seaborn-v0_8')\n        sns.set_palette(\"husl\")\n\n        # Deepeval \uacb0\uacfc \uc2dc\uac01\ud654\n        deepeval_data = report[\"summary\"][\"deepeval\"]\n        if deepeval_data:\n            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n            fig.suptitle(f'Local VLLM Evaluation Results - {report[\"model_name\"]}', fontsize=16)\n\n            # \uba54\ud2b8\ub9ad\ubcc4 \uc810\uc218 \uc2dc\uac01\ud654\n            all_scores = []\n            all_metrics = []\n\n            for filename, metrics in deepeval_data.items():\n                for metric_name, metric_data in metrics.items():\n                    all_scores.append(metric_data[\"average_score\"])\n                    all_metrics.append(metric_name.replace(\"Metric\", \"\"))\n\n            if all_scores:\n                axes[0, 0].bar(all_metrics, all_scores)\n                axes[0, 0].set_title('Deepeval Metrics Scores')\n                axes[0, 0].set_ylabel('Score')\n                axes[0, 0].tick_params(axis='x', rotation=45)\n\n        # Evalchemy \uacb0\uacfc \uc2dc\uac01\ud654\n        evalchemy_data = report[\"summary\"][\"evalchemy\"]\n        if evalchemy_data:\n            tasks = []\n            accuracies = []\n\n            for filename, results in evalchemy_data.items():\n                for task, metrics in results.items():\n                    tasks.append(task)\n                    accuracies.append(metrics[\"accuracy\"])\n\n            if tasks:\n                axes[0, 1].bar(tasks, accuracies)\n                axes[0, 1].set_title('Evalchemy Benchmark Accuracies')\n                axes[0, 1].set_ylabel('Accuracy')\n                axes[0, 1].tick_params(axis='x', rotation=45)\n\n        plt.tight_layout()\n        chart_file = f\"{output_dir}/evaluation_charts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n        plt.savefig(chart_file, dpi=300, bbox_inches='tight')\n        logger.info(f\"Charts saved to: {chart_file}\")\n\n    except ImportError:\n        logger.warning(\"matplotlib/seaborn not installed, skipping visualization\")\n    except Exception as e:\n        logger.error(f\"Visualization failed: {e}\")\n\ndef main():\n    \"\"\"\uba54\uc778 \uc2e4\ud589 \ud568\uc218\"\"\"\n    output_dir = os.getenv(\"OUTPUT_DIR\", \"./test_results\")\n\n    # \uacb0\uacfc \ub85c\ub4dc\n    deepeval_results = load_deepeval_results(output_dir)\n    evalchemy_results = load_evalchemy_results(output_dir)\n\n    # \ud1b5\ud569 \ubcf4\uace0\uc11c \uc0dd\uc131\n    report = create_summary_report(deepeval_results, evalchemy_results, output_dir)\n\n    # \uc2dc\uac01\ud654\n    create_visualizations(report, output_dir)\n\n    # \ucf58\uc194 \ucd9c\ub825\n    print(\"\\n=== Local VLLM Evaluation Summary ===\")\n    print(f\"Model: {report['model_name']}\")\n    print(f\"Timestamp: {report['timestamp']}\")\n\n    if report[\"summary\"][\"deepeval\"]:\n        print(\"\\n--- Deepeval Results ---\")\n        for filename, metrics in report[\"summary\"][\"deepeval\"].items():\n            print(f\"File: {filename}\")\n            for metric_name, data in metrics.items():\n                print(f\"  {metric_name}: {data['average_score']:.3f} (n={data['count']})\")\n\n    if report[\"summary\"][\"evalchemy\"]:\n        print(\"\\n--- Evalchemy Results ---\")\n        for filename, results in report[\"summary\"][\"evalchemy\"].items():\n            print(f\"File: {filename}\")\n            for task, metrics in results.items():\n                print(f\"  {task}: {metrics['accuracy']:.3f}\")\n\nif __name__ == \"__main__\":\n    main()\nEOF\n\n# \uc2e4\ud589 \uad8c\ud55c \ubd80\uc5ec\nchmod +x scripts/aggregate_local_results.py\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2_3","title":"2. \uacb0\uacfc \uc9d1\uacc4 \uc2e4\ud589","text":"<pre><code># \uc2dc\uac01\ud654 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58\npip install matplotlib seaborn pandas\n\n# \uacb0\uacfc \uc9d1\uacc4 \ubc0f \uc2dc\uac01\ud654\npython scripts/aggregate_local_results.py\n\n# \uc0dd\uc131\ub41c \ud30c\uc77c \ud655\uc778\nls -la test_results/evaluation_*\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_7","title":"\ud83d\udd27 \ud1b5\ud569 \uc2e4\ud589 \uc2a4\ud06c\ub9bd\ud2b8","text":"<pre><code># \uc804\uccb4 \ub85c\uceec \ud3c9\uac00 \uc2e4\ud589 \uc2a4\ud06c\ub9bd\ud2b8 \uc0dd\uc131\ncat &gt; scripts/run_full_local_evaluation.sh &lt;&lt; 'EOF'\n#!/bin/bash\nset -e\n\necho \"\ud83d\ude80 \ub85c\uceec VLLM \ud3c9\uac00 \uc2dc\uc791\"\necho \"====================\"\n\n# \ud658\uacbd \ubcc0\uc218 \ub85c\ub4dc\nsource .env\n\n# 1. VLLM \uc11c\ubc84 \uc0c1\ud0dc \ud655\uc778\necho \"\ud83d\udce1 VLLM \uc11c\ubc84 \uc0c1\ud0dc \ud655\uc778 \uc911...\"\nif ! curl -f http://localhost:8000/health &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u274c VLLM \uc11c\ubc84\uac00 \uc2e4\ud589\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4. \uc11c\ubc84\ub97c \uba3c\uc800 \uc2dc\uc791\ud574\uc8fc\uc138\uc694.\"\n    exit 1\nfi\necho \"\u2705 VLLM \uc11c\ubc84 \uc815\uc0c1 \uc791\ub3d9\"\n\n# 2. \uacb0\uacfc \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\nmkdir -p $OUTPUT_DIR\n\n# 3. Deepeval \uc2e4\ud589\necho \"\ud83e\uddea Deepeval \ud3c9\uac00 \uc2e4\ud589 \uc911...\"\npython scripts/run_local_deepeval.py\necho \"\u2705 Deepeval \uc644\ub8cc\"\n\n# 4. Evalchemy \uc2e4\ud589\necho \"\u26a1 Evalchemy \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589 \uc911...\"\npython scripts/run_local_evalchemy.py\necho \"\u2705 Evalchemy \uc644\ub8cc\"\n\n# 5. \uacb0\uacfc \uc9d1\uacc4\necho \"\ud83d\udcca \uacb0\uacfc \uc9d1\uacc4 \ubc0f \uc2dc\uac01\ud654 \uc911...\"\npython scripts/aggregate_local_results.py\necho \"\u2705 \uacb0\uacfc \uc9d1\uacc4 \uc644\ub8cc\"\n\n# 6. \uacb0\uacfc \ucd9c\ub825\necho \"\"\necho \"\ud83c\udf89 \ub85c\uceec VLLM \ud3c9\uac00 \uc644\ub8cc!\"\necho \"====================\"\necho \"\uacb0\uacfc \ud30c\uc77c \uc704\uce58: $OUTPUT_DIR\"\necho \"\uc8fc\uc694 \ud30c\uc77c:\"\necho \"  - Deepeval \uacb0\uacfc: $OUTPUT_DIR/deepeval_results_*.json\"\necho \"  - Evalchemy \uacb0\uacfc: $OUTPUT_DIR/evalchemy_results.json\"\necho \"  - \ud1b5\ud569 \ubcf4\uace0\uc11c: $OUTPUT_DIR/evaluation_summary_*.json\"\necho \"  - \uc2dc\uac01\ud654 \ucc28\ud2b8: $OUTPUT_DIR/evaluation_charts_*.png\"\nEOF\n\n# \uc2e4\ud589 \uad8c\ud55c \ubd80\uc5ec\nchmod +x scripts/run_full_local_evaluation.sh\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_8","title":"\ud83d\udea8 \ubb38\uc81c \ud574\uacb0","text":""},{"location":"local-vllm-evaluation-guide/#1-vllm","title":"1. VLLM \uc11c\ubc84 \uad00\ub828 \ubb38\uc81c","text":"<pre><code># \uc11c\ubc84 \ub85c\uadf8 \ud655\uc778\ndocker logs vllm-server\n\n# \uc11c\ubc84 \uc7ac\uc2dc\uc791\ndocker restart vllm-server\n\n# \ud3ec\ud2b8 \uc0ac\uc6a9 \ud655\uc778\nlsof -i :8000\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2-python","title":"2. Python \uc758\uc874\uc131 \ubb38\uc81c","text":"<pre><code># \uac00\uc0c1\ud658\uacbd \uc7ac\uc0dd\uc131\ndeactivate\nrm -rf venv\npython3.11 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#3","title":"3. \uba54\ubaa8\ub9ac \ubd80\uc871 \ubb38\uc81c","text":"<pre><code># Docker \uba54\ubaa8\ub9ac \uc81c\ud55c \uc124\uc815\ndocker run -d \\\n  --name vllm-server \\\n  --memory=\"8g\" \\\n  --gpus all \\\n  -p 8000:8000 \\\n  vllm/vllm-openai:latest \\\n  --model \"Qwen/Qwen2-7B-Instruct\" \\\n  --gpu-memory-utilization 0.8\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#4","title":"4. \ud3c9\uac00 \uacb0\uacfc\uac00 \ub098\uc624\uc9c0 \uc54a\ub294 \uacbd\uc6b0","text":"<pre><code># \ub85c\uadf8 \ub808\ubca8 \ubcc0\uacbd\nexport LOG_LEVEL=DEBUG\n\n# \ub2e8\uacc4\ubcc4 \ub514\ubc84\uae45\npython -c \"\nimport openai\nclient = openai.OpenAI(base_url='http://localhost:8000/v1', api_key='dummy')\nresponse = client.chat.completions.create(\n    model='qwen3-8b',\n    messages=[{'role': 'user', 'content': 'Hello!'}]\n)\nprint(response.choices[0].message.content)\n\"\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_9","title":"\ud83c\udfaf \uc2e4\ud589 \uc694\uc57d","text":""},{"location":"local-vllm-evaluation-guide/#1_4","title":"\ubc29\ubc95 1: \ud1b5\ud569 \uc2a4\ud06c\ub9bd\ud2b8 \uc0ac\uc6a9 (\uad8c\uc7a5)","text":"<pre><code># \ud55c \ubc88\uc5d0 \ubaa8\ub4e0 \uac83\uc744 \uc2e4\ud589 (VLLM \uc11c\ubc84 \uc790\ub3d9 \uac10\uc9c0)\n./scripts/run_complete_local_evaluation.sh\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#2_4","title":"\ubc29\ubc95 2: \uc218\ub3d9 \ub2e8\uacc4\ubcc4 \uc2e4\ud589","text":"<pre><code># 1. VLLM \uc11c\ubc84 \uc2dc\uc791 (\uc120\ud0dd\uc0ac\ud56d)\ndocker run -d \\\n  --name vllm-server \\\n  --gpus all \\\n  -p 8000:8000 \\\n  vllm/vllm-openai:latest \\\n  --model \"Qwen/Qwen2-7B-Instruct\" \\\n  --served-model-name \"qwen3-8b\"\n\n# 2. \uac1c\ubcc4 \ud14c\uc2a4\ud2b8 \uc2e4\ud589\npython scripts/run_simple_deepeval_test.py      # Mock \ud14c\uc2a4\ud2b8\npython scripts/run_vllm_deepeval_test.py        # \uc2e4\uc81c VLLM \ud14c\uc2a4\ud2b8\n\n# \uc804\uccb4 \ud3c9\uac00 \uc2e4\ud589\npython scripts/run_complete_local_evalchemy.py\n\n# \uac1c\ubcc4 \ud14c\uc2a4\ud2b8 \uc2e4\ud589\npython scripts/run_simple_deepeval_test.py\npython scripts/run_simple_evalchemy_test.py\n\n# 3. \uacb0\uacfc \ud655\uc778\ncat test_results/*.json | jq\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_10","title":"\uc2e4\ud589 \uacb0\uacfc \uc608\uc2dc","text":"<pre><code>\ud83d\ude80 macOS OrbStack VLLM \ub85c\uceec \ud3c9\uac00 \ud1b5\ud569 \uc2e4\ud589\n=============================================\n\ud83d\udccb 1. \ud658\uacbd \ud655\uc778\n\ud83d\udccb 2. \ud544\uc218 \ud328\ud0a4\uc9c0 \ud655\uc778\n\u2705 \ud544\uc218 \ud328\ud0a4\uc9c0 \ud655\uc778 \uc644\ub8cc\n\ud83d\udccb 3. \uacb0\uacfc \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\n\u2705 \uacb0\uacfc \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131: ./test_results\n\ud83d\udccb 4. VLLM \uc11c\ubc84 \uc0c1\ud0dc \ud655\uc778\n\u2705 VLLM \uc11c\ubc84 \ubc1c\uacac: http://localhost:1234\n\ud83d\udccb 5. \uc2e4\uc81c VLLM \uc11c\ubc84\ub85c \ud3c9\uac00 \uc2e4\ud589\n\n\ud83d\udcca \ucd5c\uc885 \uacb0\uacfc:\n  \ucd1d \ud14c\uc2a4\ud2b8: 5\n  \ud3c9\uade0 \uc810\uc218: 0.50\n  \uc131\uacf5\ub960: 50.0%\n\n\u2705 \ub85c\uceec VLLM \ud3c9\uac00\uac00 \uc644\ub8cc\ub418\uc5c8\uc2b5\ub2c8\ub2e4!\n</code></pre>"},{"location":"local-vllm-evaluation-guide/#_11","title":"\ud83c\udf89 \uc815\ub9ac","text":"<p>\uc774 \uac00\uc774\ub4dc\ub97c \ud1b5\ud574 macOS OrbStack \ud658\uacbd\uc5d0\uc11c VLLM \ubaa8\ub378\uc758 \ub85c\uceec \ud3c9\uac00\ub97c \uc644\uc804\ud788 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p>"},{"location":"local-vllm-evaluation-guide/#_12","title":"\ud83d\udd27 \uc8fc\uc694 \uae30\ub2a5","text":"<ul> <li>\uc790\ub3d9 \ud658\uacbd \uac10\uc9c0: VLLM \uc11c\ubc84 \uc5ec\ub7ec \ud3ec\ud2b8 \uc790\ub3d9 \ud0d0\uc9c0</li> <li>Mock \ubaa8\ub4dc \uc9c0\uc6d0: \uc11c\ubc84 \uc5c6\uc774\ub3c4 \ud14c\uc2a4\ud2b8 \uac00\ub2a5</li> <li>\ud1b5\ud569 \uc2e4\ud589: \ud55c \ubc88\uc758 \uba85\ub839\uc73c\ub85c \uc804\uccb4 \ud3c9\uac00 \uc218\ud589</li> <li>\uc0c1\uc138\ud55c \uacb0\uacfc \ubd84\uc11d: JSON \ud615\ud0dc\uc758 \uad6c\uc870\ud654\ub41c \uacb0\uacfc</li> </ul>"},{"location":"local-vllm-evaluation-guide/#_13","title":"\ud83d\udcca \uc0dd\uc131\ub418\ub294 \uacb0\uacfc \ud30c\uc77c","text":"<ul> <li><code>simple_deepeval_results.json</code>: Mock \ud14c\uc2a4\ud2b8 \uacb0\uacfc</li> <li><code>vllm_deepeval_results.json</code>: VLLM \uc11c\ubc84 \ud14c\uc2a4\ud2b8 \uacb0\uacfc</li> </ul>"},{"location":"local-vllm-evaluation-guide/#_14","title":"\ud83d\ude80 \ub2e4\uc74c \ub2e8\uacc4","text":"<p>\uc774 \ub85c\uceec \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc744 \uae30\ubc18\uc73c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \ud655\uc7a5\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4: - \ucee4\uc2a4\ud140 \ud3c9\uac00 \uba54\ud2b8\ub9ad \ucd94\uac00 - \ub2e4\uc591\ud55c \ubaa8\ub378 \ube44\uad50 \ud3c9\uac00 - \uc131\ub2a5 \ubca4\uce58\ub9c8\ud06c \ud655\uc7a5 - CI/CD \ud30c\uc774\ud504\ub77c\uc778 \ud1b5\ud569 </p>"},{"location":"standard-evalchemy-guide/","title":"Standard Evalchemy Evaluation Guide","text":"<p>This guide explains how to run standard evalchemy evaluation and troubleshoot common issues.</p>"},{"location":"standard-evalchemy-guide/#quick-start","title":"Quick Start","text":""},{"location":"standard-evalchemy-guide/#1-build-the-docker-image","title":"1. Build the Docker image","text":"<p>docker build -f docker/standard_evalchemy.Dockerfile \\  -t standard-evalchemy:latest .</p>"},{"location":"standard-evalchemy-guide/#3-run-evaluation","title":"3. Run evaluation","text":"<p>docker run --rm \\  --network host \\  -v $(pwd)/results:/app/evalchemy-src/results \\  -e VLLM_MODEL_ENDPOINT=\"http://localhost:8080/v1/completions\" \\  -e MODEL_NAME=\"Qwen/Qwen2-0.5B\" \\  -e SERVED_MODEL_NAME=\"qwen2-0.5b\" \\  -e TOKENIZER=\"Qwen/Qwen2-0.5B\" \\  -e TOKENIZER_BACKEND=\"huggingface\" \\  -e MODEL_CONFIG='{\"model_type\": \"curator\", \"api_config\": {\"max_retries\": 3, \"retry_delay\": 5.0}}' \\  -e EVALUATION_CONFIG='{\"limit\": 100, \"output_format\": \"json\", \"log_samples\": true, \"max_tokens\": 2000}' \\  standard-evalchemy:latest</p>"},{"location":"standard-evalchemy-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"standard-evalchemy-guide/#1-model-endpoint-is-not-valid-or-model-id-missing","title":"1. \"Model endpoint is not valid or model ID missing\"","text":"<p>This error occurs when:</p> <ul> <li>The container can't connect to VLLM server</li> <li>Required configuration is missing</li> <li>Model information is not properly set</li> </ul> <p>Solution:</p> <ol> <li>Add <code>--network host</code> to Docker run command</li> <li>Provide all required environment variables:</li> <li><code>MODEL_NAME</code></li> <li><code>SERVED_MODEL_NAME</code></li> <li><code>TOKENIZER</code></li> <li><code>MODEL_CONFIG</code></li> <li><code>EVALUATION_CONFIG</code></li> </ol>"},{"location":"standard-evalchemy-guide/#2-missing-configuration-section-model_config","title":"2. \"Missing configuration section: .model_config\"","text":"<p>This error occurs when model configuration is not provided.</p> <p>Solution: Add MODEL_CONFIG environment variable:</p> <pre><code>-e MODEL_CONFIG='{\"model_type\": \"curator\", \"api_config\": {\"max_retries\": 3, \"retry_delay\": 5.0}}'\n</code></pre>"},{"location":"standard-evalchemy-guide/#3-missing-configuration-section-evaluation_config","title":"3. \"Missing configuration section: .evaluation_config\"","text":"<p>This error occurs when evaluation configuration is not provided.</p> <p>Solution: Add EVALUATION_CONFIG environment variable:</p> <pre><code>-e EVALUATION_CONFIG='{\"limit\": 100, \"output_format\": \"json\", \"log_samples\": true, \"max_tokens\": 2000}'\n</code></pre>"},{"location":"standard-evalchemy-guide/#configuration-options","title":"Configuration Options","text":""},{"location":"standard-evalchemy-guide/#model-configuration","title":"Model Configuration","text":"<pre><code>{\n  \"model_type\": \"curator\",\n  \"api_config\": {\n    \"max_retries\": 3,\n    \"retry_delay\": 5.0\n  }\n}\n</code></pre>"},{"location":"standard-evalchemy-guide/#evaluation-configuration","title":"Evaluation Configuration","text":"<pre><code>{\n  \"limit\": 100,\n  \"output_format\": \"json\",\n  \"log_samples\": true,\n  \"max_tokens\": 2000\n}\n</code></pre>"},{"location":"standard-evalchemy-guide/#available-benchmarks","title":"Available Benchmarks","text":"<p>The standard evalchemy includes several Arabic language benchmarks:</p> <ol> <li>hellaswag_ar: Tests commonsense reasoning</li> <li>piqa_ar: Physical interaction reasoning</li> <li>arabicmmlu: Multi-task language understanding</li> <li>aexams: Arabic exams benchmark</li> <li>copa_ar: Choice of plausible alternatives</li> </ol>"},{"location":"standard-evalchemy-guide/#results-structure","title":"Results Structure","text":"<p>After running the evaluation, results will be available in:</p> <pre><code>results/\n\u2514\u2500\u2500 {run_id}/\n    \u251c\u2500\u2500 evalchemy_{run_id}.log              # Execution log\n    \u251c\u2500\u2500 {benchmark}_results.json            # Results for each benchmark\n    \u2514\u2500\u2500 {benchmark}/                        # Detailed benchmark data\n</code></pre>"},{"location":"standard-evalchemy-guide/#interpreting-results","title":"Interpreting Results","text":"<p>Results are provided in JSON format with the following metrics:</p> <ul> <li><code>acc</code>: Accuracy score</li> <li><code>acc_stderr</code>: Standard error of accuracy</li> <li><code>acc_norm</code>: Normalized accuracy (if applicable)</li> </ul> <p>Example result structure:</p> <pre><code>{\n  \"hellaswag_ar\": {\n    \"acc,none\": 0.29,\n    \"acc_stderr,none\": 0.045604802157206845,\n    \"acc_norm,none\": 0.29,\n    \"acc_norm_stderr,none\": 0.045604802157206845\n  }\n}\n</code></pre>"},{"location":"standard-evalchemy-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always use <code>--network host</code> when running locally</li> <li>Provide all required environment variables</li> <li>Create results directory before running</li> <li>Check VLLM server accessibility before running evaluation</li> <li>Monitor the evaluation logs for progress</li> </ol>"},{"location":"standard-evalchemy-guide/#troubleshooting-steps","title":"Troubleshooting Steps","text":"<ol> <li>Verify VLLM server is running:</li> </ol> <pre><code>curl http://localhost:8080/v1/models\n</code></pre> <ol> <li>Check model endpoint:</li> </ol> <pre><code>curl -X POST http://localhost:8080/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\":\"/data/local_models/finetune-qwen2-0-5b\", \"prompt\": \"Hello\", \"max_tokens\": 10}'\n</code></pre> <ol> <li>Verify directory permissions:</li> </ol> <pre><code>ls -l results/\n</code></pre> <ol> <li>Check container logs:</li> </ol> <pre><code>docker logs &lt;container_id&gt;\n</code></pre>"},{"location":"standard-evalchemy-guide/#notes","title":"Notes","text":"<ul> <li>The backend service warnings (<code>model-benchmark-backend-svc</code>) can be ignored if you're not using the backend service</li> <li>Results are saved both in raw format and standardized format</li> <li>Each benchmark run gets a unique run ID based on timestamp</li> </ul>"},{"location":"vllm-eval-prd/","title":"Production Guide","text":""},{"location":"vllm-eval-prd/#vllm-prd-v2-product-requirements-document","title":"VLLM \ubaa8\ub378 \uc131\ub2a5 \uc790\ub3d9 \ud3c9\uac00 PRD\u00a0v2 (Product Requirements Document)","text":""},{"location":"vllm-eval-prd/#1-purpose","title":"1. \ubaa9\uc801 (Purpose)","text":"<p>\ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130\uc5d0 \ubc30\ud3ec\ub41c VLLM \uc11c\ube59 \ubaa8\ub378\uc744 Deepeval \ubc0f Evalchemy\ub85c \uc9c0\uc18d\uc801\u00b7\uc790\ub3d9\uc73c\ub85c \ud3c9\uac00\ud558\uc5ec</p> <ul> <li>\ubaa8\ub378 \ub9b4\ub9ac\uc2a4\ub9c8\ub2e4 \uac1d\uad00\uc801 \ud488\uc9c8 \uc9c0\ud45c\ub97c \ud655\ubcf4\ud558\uace0</li> <li>\ud488\uc9c8 \ud1f4\ud654(regression)\ub97c \uc989\uc2dc \ud0d0\uc9c0\ud558\uba70</li> <li>\uc6b4\uc601 \ud300\uc758 Microsoft\u202fTeams \ucc44\ub110\uc5d0 \uc2e4\uc2dc\uac04 \ub9ac\ud3ec\ud305/\uc54c\ub9bc\uc744 \uc81c\uacf5\ud55c\ub2e4.</li> </ul>"},{"location":"vllm-eval-prd/#2-background","title":"2. \ubc30\uacbd (Background)","text":"<ul> <li>Deepeval: PyTest\u2011\uc2a4\ud0c0\uc77c \ub2e8\uc704\u2011\ud14c\uc2a4\ud2b8\ud615 LLM \ud3c9\uac00 \ud504\ub808\uc784\uc6cc\ud06c\ub85c 30+ Metrics, RAG &amp; E2E \ud3c9\uac00 \uc9c0\uc6d0.</li> <li>Evalchemy: EleutherAI lm\u2011evaluation\u2011harness \uae30\ubc18 Unified Benchmark Runner, \ud45c\uc900 \ubca4\uce58\ub9c8\ud06c\uc640 API\u2011\uae30\ubc18 \ubaa8\ub378 \uc9c0\uc6d0.</li> <li>VLLM: GPU \uba54\ubaa8\ub9ac \ud6a8\uc728\uc744 \uadf9\ub300\ud654\ud55c PagedAttention Inference \uc5d4\uc9c4. \ubc84\uc804 \uc5c5\ub9c8\ub2e4 \ud488\uc9c8 \ubcc0\ub3d9 \uac00\ub2a5\uc131\uc774 \ub192\uc544 \uc790\ub3d9\ud654\ub41c Regression Test\uac00 \ud544\uc218.</li> </ul>"},{"location":"vllm-eval-prd/#3-scope","title":"3. \ubc94\uc704 (Scope)","text":"<ul> <li> <p>\ud3ec\ud568:</p> </li> <li> <p>Deepeval\u00a0\u00b7\u00a0Evalchemy \ub0b4\uc7a5 \ub370\uc774\ud130\uc14b \ud65c\uc6a9 \ubc0f \uc911\ubcf5 \uc81c\uac70\ub41c \ud14c\uc2a4\ud2b8 \uc2a4\uc704\ud2b8 \uad6c\uc131</p> </li> <li>\ucee4\uc2a4\ud140 Metric(\uc608: RAG \uc815\ub2f5\ub960, Hallucination)</li> <li>\ud45c\uc900 \ubca4\uce58\ub9c8\ud06c (ARC, HellaSwag, MMLU \ub4f1)</li> <li>\ud30c\uc774\ud504\ub77c\uc778 \ud2b8\ub9ac\uac70: GHCR \uc774\ubbf8\uc9c0 \ud0dc\uadf8 <code>release-*</code> Push \uace0\uc815</li> <li>\uacb0\uacfc \uc800\uc7a5(ClickHouse) + Grafana Dashboard + Microsoft\u202fTeams \uc54c\ub9bc</li> <li>\uc81c\uc678: \ubaa8\ub378 \ud559\uc2b5/\ud30c\uc778\ud29c\ub2dd, \ud074\ub7ec\uc2a4\ud130 \ud504\ub85c\ube44\uc800\ub2dd</li> </ul>"},{"location":"vllm-eval-prd/#4-stakeholders","title":"4. \uc774\ud574\uad00\uacc4\uc790 (Stakeholders)","text":"<ul> <li>\uc81c\ud488 \uc624\ub108, ML\u00a0Ops, \ub9ac\uc11c\uce58, \ud50c\ub7ab\ud3fc\u00a0Ops</li> </ul>"},{"location":"vllm-eval-prd/#5-success-metrics","title":"5. \uc131\uacf5 \uc9c0\ud45c (Success Metrics)","text":"\ud56d\ubaa9 \ubaa9\ud45c\uac12 \uce21\uc815 \ubc29\ubc95 \ub9b4\ub9ac\uc2a4\u2011to\u2011\ub9ac\ud3ec\ud2b8 \uc9c0\uc5f0 \u2264\u00a02\u202fh \ud30c\uc774\ud504\ub77c\uc778 \uc644\ub8cc \ud0c0\uc784\uc2a4\ud0ec\ud504 \ud488\uc9c8 \ud1f4\ud654 \uac10\uc9c0\uc728 \u2265\u00a095\u202f% Known\u2011Bad \ub9ac\uadf8\ub808\uc158 \uc138\ud2b8 \ud30c\uc774\ud504\ub77c\uc778 \uc548\uc815\uc131 \uc2e4\ud328\u00a0&lt;\u00a01\u202f%/\uc6d4 CronJob \uc131\uacf5\ub960"},{"location":"vllm-eval-prd/#6-highlevel-architecture","title":"6. \uc2dc\uc2a4\ud15c \uc544\ud0a4\ud14d\ucc98 (High\u2011Level Architecture)","text":"<ol> <li> <p>Trigger Layer</p> </li> <li> <p>Event Source: GHCR Repository Package Push Webhook</p> </li> <li>Mechanism: Argo\u00a0Events\u00a0\u2192 Argo\u00a0Workflows</li> <li> <p>Execution Layer \u2013 Argo Workflow Steps</p> </li> <li> <p><code>prepare-dataset</code></p> </li> <li><code>deepeval-runner</code> (GPU=0)</li> <li><code>evalchemy-runner</code> (GPU=N)</li> <li><code>aggregate-metrics</code></li> <li> <p>Storage &amp; Results</p> </li> <li> <p>Dataset &amp; Snapshot: MinIO\u00a0(bucket\u00a0<code>llm-eval-ds</code>)</p> </li> <li>Raw Logs: PVC (7\u00a0\uc77c \ubcf4\uc874)</li> <li>Aggregated Metrics: ClickHouse\u00a0<code>vllm_eval.results</code></li> <li> <p>Observability</p> </li> <li> <p>Prometheus Exporter (\ud30c\uc774\ud504\ub77c\uc778 Job \uc0c1\ud0dc)</p> </li> <li>Grafana \ub300\uc2dc\ubcf4\ub4dc <code>LLM Quality Overview</code></li> <li>Microsoft\u202fTeams Incoming Webhook <code>VLLM\u2011CI</code> \ucc44\ub110 \uc54c\ub9bc (Adaptive\u00a0Card)</li> </ol>"},{"location":"vllm-eval-prd/#7-functional-requirements","title":"7. \uae30\ub2a5 \uc694\uad6c\uc0ac\ud56d (Functional Requirements)","text":"<ul> <li>F\u201101\u00a0\ub9b4\ub9ac\uc2a4 \ud6c5: <code>ghcr.io/{repo}/*</code> \uc774\ubbf8\uc9c0\u00a0Push &amp;\u00a0<code>tag =~ /release-.+/</code> \uc2dc \ud30c\uc774\ud504\ub77c\uc778 \uc790\ub3d9 \uc2e4\ud589</li> <li>F\u201102\u00a0\ub370\uc774\ud130\uc14b \ubc84\uc804: <code>datasets/raw/</code> \ub514\ub809\ud1a0\ub9ac \ub0b4 \ub370\uc774\ud130\uc14b\ubcc4 \uad00\ub9ac</li> <li>F\u201103\u00a0Deepeval Custom Metric Registry: <code>eval/deepeval_tests/metrics/*.py</code></li> <li>F\u201104\u00a0Evalchemy Benchmark Selection: <code>configs/evalchemy.json</code> \ub0b4 <code>tasks</code> \ubaa9\ub85d</li> <li>F\u201105\u00a0\uacb0\uacfc \uc2a4\ud0a4\ub9c8: <code>run_id</code>,<code>model_tag</code>,<code>metric</code>,<code>value</code>,<code>ts</code></li> <li>F\u201106\u00a0Regression Alert: \ucd5c\uadfc N\ud68c Rolling\u00a0Mean \ub300\ube44 10\u202f%\u2193 \uc2dc Teams Mentions\u00a0(@LLM\u2011Ops)</li> </ul>"},{"location":"vllm-eval-prd/#8-nonfunctional","title":"8. \ube44\uae30\ub2a5 \uc694\uad6c\uc0ac\ud56d (Non\u2011Functional)","text":"<ul> <li>\ubcf4\uc548: ServiceAccount \ucd5c\uc18c\u00a0RBAC, Secrets External\u00a0Secret \uc5f0\ub3d9</li> <li>\uc131\ub2a5: Benchmark Job Timeout\u00a0\u2264\u00a060\u202fmin; GPU\u00a0Pod PriorityClass\u00a0=\u00a0high</li> <li>\ud655\uc7a5\uc131: Benchmark Matrix \ubcd1\ub82c\u00a0Fan\u2011out</li> <li>\uac00\uc6a9\uc131: Argo Workflow Controller\u00a0HA\u00a0(2\u00a0Replicas)</li> <li>\uac10\uc0ac \ub85c\uadf8: \ubaa8\ub4e0 RUN_ID \ubcc4 JSONL \ub85c\uadf8 S3\u00a0\u2265\u00a090\u202f\uc77c \ubcf4\uc874</li> </ul>"},{"location":"vllm-eval-prd/#9-assets","title":"9. \ub370\uc774\ud130\uc14b\u00b7\ubca4\uce58\ub9c8\ud06c (Assets)","text":"\uce74\ud14c\uace0\ub9ac \uc608\uc2dc \uad00\ub9ac \ubc29\uc2dd \ud45c\uc900 \ubca4\uce58\ub9c8\ud06c ARC, HellaSwag, MMLU Evalchemy Preset \ud55c\uad6d\uc5b4 \ubca4\uce58\ub9c8\ud06c Ko\u2011MMLU, Ko\u2011ARC \ubcc4\ub3c4 bucket <code>ko-benchmark</code> \ub9ac\uadf8\ub808\uc158 \uc138\ud2b8 \uc11c\ube44\uc2a4 \ucffc\ub9ac \uc2a4\ub0c5\uc0f7 1\u202fk \uc77c\u00a01\u202f\ud68c \uc775\uba85\ud654 &amp;\u00a0SHA\u2011256 Dedup <p>Deduplication\u00a0\uc804\ub7b5: SHA\u20111/256\u00a0Hash\u00a0\u2192 Exact\u00a0Match \uc81c\uac70 \u2192 Near\u2011Dup\u00a0(LSH +\u00a0Levenshtein\u00a0&lt;\u00a00.2) \ud544\ud130\ub85c Deepeval\u00b7Evalchemy\u00a0\uc591\uce21 \ub370\uc774\ud130\uc14b \uc911\ubcf5 \uc81c\uac70.</p>"},{"location":"vllm-eval-prd/#10-timeline","title":"10. \uad6c\ud604 \ub85c\ub4dc\ub9f5 (Timeline)","text":"<ul> <li>W1\u20112\u00a0\ub370\uc774\ud130\uc14b \uc815\uc758 &amp;\u00a0\uc801\uc7ac, Dedup \ud30c\uc774\ud504 \uad6c\ucd95</li> <li>W3\u20114\u00a0Deepeval Metric \ud504\ub85c\ud1a0\ud0c0\uc785 + \uc218\ub3d9 \ud3c9\uac00 \ubca0\uc774\uc2a4\ub77c\uc778</li> <li>W5\u20116\u00a0Argo Workflow CI/CD \ud1b5\ud569, Teams \uc54c\ub9bc \uc801\uc6a9</li> <li>W7\u00a0Grafana\u00a0\ub300\uc2dc\ubcf4\ub4dc, ClickHouse \ub9ac\ud150\uc158 \uc815\ucc45</li> <li>W8\u00a0Pilot\u00a0\ub9b4\ub9ac\uc2a4(\ub0b4\ubd80)\u00a0\u2192 \ud53c\ub4dc\ubc31 \ubc18\uc601 \ud6c4 Prod\u00a0Go\u2011Live</li> </ul>"},{"location":"vllm-eval-prd/#11-risks-mitigations","title":"11. \uc704\ud5d8 \ubc0f \uc644\ud654 (Risks\u00a0&amp;\u00a0Mitigations)","text":"\uc704\ud5d8 \uc601\ud5a5 \uc644\ud654\ucc45 \ubca4\uce58\ub9c8\ud06c \uc2dc\uac04 \uacfc\ub2e4 \ub9b4\ub9ac\uc2a4 \uc9c0\uc5f0 \ub370\uc774\ud130\uc14b \uc0d8\ud50c\ub9c1, GPU\u00a0\uc218\ud3c9 \uc2a4\ucf00\uc77c \ud3c9\uac00 \uc9c0\ud45c \ud574\uc11d \uc624\ub958 \uc798\ubabb\ub41c \ud488\uc9c8 \ud310\ub2e8 \ub9ac\uc11c\uce58 \ucf54\ub4dc \ub9ac\ubdf0 GPU \ube44\uc6a9 \uc99d\uac00 \uc6b4\uc601 \ube44\uc6a9 \uc0c1\uc2b9 Spot\u00a0GPU + Budget Guardrail"},{"location":"vllm-eval-prd/#12-glossary","title":"12. \uc6a9\uc5b4 (Glossary)","text":"<ul> <li>GHCR: GitHub Container Registry</li> <li>Incoming Webhook: Teams \ucc44\ub110\uc5d0 JSON\u00a0Payload\ub85c \uba54\uc2dc\uc9c0 \uc804\uc1a1</li> <li>Deepeval: LLM \ub2e8\uc704 \ud14c\uc2a4\ud2b8 \ud504\ub808\uc784\uc6cc\ud06c</li> <li>Evalchemy: \ud45c\uc900 LLM \ubca4\uce58\ub9c8\ud06c \uc2e4\ud589\uae30</li> <li>VLLM: \uace0\uc131\ub2a5 Inference \uc5d4\uc9c4</li> <li>Regression Alert: \ubc30\ud3ec \ubc84\uc804 \ub300\ube44 \uc131\ub2a5 \ud558\ub77d \uacbd\uace0</li> </ul>"},{"location":"api/deepeval-api/","title":"Deepeval API","text":"<p>Deepeval is a comprehensive evaluation framework specifically designed for Large Language Models (LLMs), providing pytest-style unit testing capabilities for AI applications. It offers 30+ built-in metrics and supports both RAG (Retrieval-Augmented Generation) and end-to-end evaluation scenarios.</p> <p>Framework Overview</p> <p>Deepeval provides:</p> <ul> <li>30+ Built-in Metrics: Comprehensive evaluation across multiple dimensions</li> <li>RAG Evaluation: Specialized metrics for retrieval-augmented generation</li> <li>Custom Metrics: Extensible framework for domain-specific evaluations</li> <li>Pytest Integration: Familiar testing patterns for AI applications</li> </ul>"},{"location":"api/deepeval-api/#prerequisites","title":"Prerequisites","text":"<p>Setup Requirements</p> <p>Before using Deepeval, ensure you have:</p> <ul> <li>Python 3.8+ environment</li> <li>Required dependencies installed via <code>requirements-dev.txt</code></li> <li>A running VLLM server endpoint</li> <li>Access to evaluation datasets</li> </ul>"},{"location":"api/deepeval-api/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"api/deepeval-api/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"api/deepeval-api/#configuration","title":"Configuration","text":"<p>Deepeval uses configuration files located in <code>configs/</code>:</p> <ul> <li><code>deepeval.yaml</code>: Main configuration for evaluation parameters</li> <li><code>pytest.ini</code>: Pytest-specific settings</li> </ul> <p>Configuration Files</p> <p>Review and customize the configuration files in the <code>configs/</code> directory to match your evaluation requirements and model specifications.</p>"},{"location":"api/deepeval-api/#core-evaluation-metrics","title":"Core Evaluation Metrics","text":"<p>Deepeval provides several categories of evaluation metrics:</p>"},{"location":"api/deepeval-api/#rag-specific-metrics","title":"RAG-Specific Metrics","text":"<p>RAG Evaluation</p> <p>For Retrieval-Augmented Generation systems, Deepeval offers specialized metrics:</p> <ul> <li>Context Relevance: Measures how relevant retrieved context is to the query</li> <li>Hallucination Detection: Identifies generated content not supported by context  </li> <li>RAG Precision: Evaluates accuracy of responses given the retrieved context</li> <li>Context Utilization: Assesses how effectively the model uses provided context</li> </ul>"},{"location":"api/deepeval-api/#general-llm-metrics","title":"General LLM Metrics","text":"<ul> <li>Semantic Similarity: Compares generated text with reference answers</li> <li>Toxicity Detection: Identifies harmful or inappropriate content</li> <li>Bias Evaluation: Measures potential biases in model outputs</li> <li>Factual Consistency: Verifies factual accuracy of generated content</li> </ul>"},{"location":"api/deepeval-api/#running-deepeval-tests","title":"Running Deepeval Tests","text":""},{"location":"api/deepeval-api/#basic-test-execution","title":"Basic Test Execution","text":"<p>Navigate to the deepeval tests directory and run evaluations:</p> <pre><code>cd eval/deepeval_tests\npytest test_llm_rag.py -v\n</code></pre>"},{"location":"api/deepeval-api/#custom-metric-testing","title":"Custom Metric Testing","text":"<p>Run tests with custom metrics:</p> <pre><code>pytest test_custom_metric.py -v --model-endpoint http://localhost:8000/v1\n</code></pre> <p>Test Structure</p> <p>Deepeval tests are located in <code>eval/deepeval_tests/</code> and follow pytest conventions:</p> <pre><code>eval/deepeval_tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 metrics/              # Custom metric definitions\n\u251c\u2500\u2500 test_custom_metric.py # Custom metric tests\n\u2514\u2500\u2500 test_llm_rag.py      # RAG evaluation tests\n</code></pre>"},{"location":"api/deepeval-api/#running-specific-metrics","title":"Running Specific Metrics","text":"<p>Execute targeted evaluations:</p> <pre><code># Run only RAG precision tests\npytest test_llm_rag.py::test_rag_precision -v\n\n# Run hallucination detection\npytest test_llm_rag.py::test_hallucination_detection -v\n</code></pre>"},{"location":"api/deepeval-api/#custom-metrics-development","title":"Custom Metrics Development","text":""},{"location":"api/deepeval-api/#creating-custom-metrics","title":"Creating Custom Metrics","text":"<p>Deepeval supports custom metric development. Create new metrics in the <code>metrics/</code> directory:</p> <pre><code>from deepeval.metrics import BaseMetric\nfrom deepeval.test_case import LLMTestCase\n\nclass CustomRelevanceMetric(BaseMetric):\n    def __init__(self, threshold: float = 0.7):\n        self.threshold = threshold\n\n    def measure(self, test_case: LLMTestCase) -&gt; float:\n        # Implement your custom evaluation logic\n        pass\n\n    def is_successful(self) -&gt; bool:\n        return self.score &gt;= self.threshold\n</code></pre> <p>Metric Development</p> <p>When developing custom metrics:</p> <ul> <li>Inherit from <code>BaseMetric</code> base class</li> <li>Implement <code>measure()</code> method for scoring logic</li> <li>Define success criteria via <code>is_successful()</code></li> <li>Add comprehensive error handling</li> </ul>"},{"location":"api/deepeval-api/#registering-custom-metrics","title":"Registering Custom Metrics","text":"<p>Register your custom metrics in the test files:</p> <pre><code>from metrics.custom_relevance import CustomRelevanceMetric\n\ndef test_custom_relevance():\n    metric = CustomRelevanceMetric(threshold=0.8)\n    test_case = LLMTestCase(\n        input=\"Your test input\",\n        actual_output=\"Model's actual output\",\n        expected_output=\"Expected output\"\n    )\n    metric.measure(test_case)\n    assert metric.is_successful()\n</code></pre>"},{"location":"api/deepeval-api/#docker-integration","title":"Docker Integration","text":""},{"location":"api/deepeval-api/#building-deepeval-container","title":"Building Deepeval Container","text":"<pre><code>docker build -f docker/deepeval.Dockerfile -t deepeval-runner:latest .\n</code></pre>"},{"location":"api/deepeval-api/#running-containerized-evaluations","title":"Running Containerized Evaluations","text":"<pre><code>docker run --rm \\\n    --network host \\\n    -e VLLM_ENDPOINT=\"http://localhost:8000/v1\" \\\n    -e EVALUATION_CONFIG='{\"metrics\": [\"rag_precision\", \"hallucination\"], \"threshold\": 0.7}' \\\n    -v $(pwd)/eval/deepeval_tests/results:/workspace/results \\\n    deepeval-runner:latest\n</code></pre> <p>Docker Networking</p> <p>Use <code>--network host</code> to ensure the container can access your local VLLM server endpoint.</p>"},{"location":"api/deepeval-api/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"api/deepeval-api/#evaluation-parameters","title":"Evaluation Parameters","text":"<p>Configure evaluation behavior in <code>deepeval.yaml</code>:</p> <pre><code>evaluation:\n  model_endpoint: \"http://localhost:8000/v1\"\n  batch_size: 10\n  timeout: 300\n  metrics:\n    rag_precision:\n      threshold: 0.8\n      strict_mode: true\n    hallucination:\n      threshold: 0.3\n      detection_model: \"gpt-4\"\n</code></pre>"},{"location":"api/deepeval-api/#parallel-execution","title":"Parallel Execution","text":"<p>Run evaluations in parallel for improved performance:</p> <pre><code>pytest test_llm_rag.py -n 4 --dist=loadfile\n</code></pre> <p>Performance Optimization</p> <p>For large-scale evaluations:</p> <ul> <li>Use parallel execution with <code>-n</code> flag</li> <li>Implement batching for API calls</li> <li>Configure appropriate timeouts</li> <li>Monitor resource usage during evaluation</li> </ul>"},{"location":"api/deepeval-api/#output-results","title":"Output &amp; Results","text":""},{"location":"api/deepeval-api/#test-results-structure","title":"Test Results Structure","text":"<p>Deepeval generates comprehensive evaluation reports:</p> <pre><code>eval/deepeval_tests/results/\n\u251c\u2500\u2500 test_results.json         # Aggregated test results\n\u251c\u2500\u2500 detailed_metrics.json     # Per-metric breakdown\n\u251c\u2500\u2500 failed_cases.json         # Failed test cases for analysis\n\u2514\u2500\u2500 performance_stats.json    # Execution performance metrics\n</code></pre>"},{"location":"api/deepeval-api/#interpreting-results","title":"Interpreting Results","text":"<p>Result Analysis</p> <p>Deepeval results include:</p> <ul> <li>Overall Score: Aggregated performance across all metrics</li> <li>Per-Metric Scores: Individual metric performance</li> <li>Pass/Fail Status: Binary success indicators</li> <li>Confidence Intervals: Statistical confidence measures</li> <li>Error Analysis: Detailed failure case information</li> </ul>"},{"location":"api/deepeval-api/#integration-with-vllm-eval-pipeline","title":"Integration with VLLM Eval Pipeline","text":"<p>Deepeval integrates seamlessly with the broader VLLM evaluation ecosystem:</p> <pre><code># Integration with aggregation pipeline\npython scripts/aggregate_metrics.py --include-deepeval --results-dir eval/deepeval_tests/results\n</code></pre> <p>Pipeline Integration</p> <p>Deepeval results are automatically compatible with the VLLM evaluation aggregation system and can be included in comprehensive model performance reports.</p>"},{"location":"api/evalchemy-api/","title":"Evalchemy API","text":"<p>Evalchemy is a unified benchmark runner built on EleutherAI's lm-evaluation-harness, providing comprehensive evaluation capabilities for large language models. It supports both standard academic benchmarks and custom evaluation tasks with flexible API-based model integration.</p> <p>Evalchemy Overview</p> <p>Evalchemy provides:</p> <ul> <li>Unified Benchmarking: Single interface for multiple evaluation frameworks</li> <li>Standard Benchmarks: ARC, HellaSwag, MMLU, Ko-MMLU, Ko-ARC support</li> <li>API Integration: Compatible with VLLM and other serving frameworks</li> <li>Flexible Deployment: Docker and script-based execution options</li> </ul>"},{"location":"api/evalchemy-api/#prerequisites","title":"Prerequisites","text":"<p>System Requirements</p> <p>Before using Evalchemy, ensure you have:</p> <ul> <li>Python 3.8+ environment</li> <li>A running VLLM server endpoint</li> <li>Docker (for containerized execution)</li> <li>Sufficient computational resources for benchmark evaluation</li> <li>Network connectivity between client and model server</li> </ul>"},{"location":"api/evalchemy-api/#execution-methods","title":"Execution Methods","text":"<p>Evalchemy supports two primary execution approaches, each optimized for different use cases:</p>"},{"location":"api/evalchemy-api/#1-docker-execution-recommended","title":"1. Docker Execution (Recommended)","text":"<p>Advantages:</p> <ul> <li>Consistent execution environment</li> <li>Simplified dependency management  </li> <li>Easy integration with Kubernetes workflows</li> <li>Reproducible results across systems</li> </ul>"},{"location":"api/evalchemy-api/#2-script-execution","title":"2. Script Execution","text":"<p>Advantages:</p> <ul> <li>Direct system access for debugging</li> <li>Faster iteration during development</li> <li>Custom environment configuration</li> <li>Local filesystem integration</li> </ul>"},{"location":"api/evalchemy-api/#docker-based-evaluation","title":"Docker-Based Evaluation","text":""},{"location":"api/evalchemy-api/#building-the-container","title":"Building the Container","text":"<p>Create the Evalchemy Docker image:</p> <pre><code>docker build -f docker/standard-evalchemy.Dockerfile \\\n    -t standard-evalchemy:latest .\n</code></pre> <p>Image Optimization</p> <p>The Docker build process includes optimized dependency installation and caching for faster subsequent builds.</p>"},{"location":"api/evalchemy-api/#container-execution","title":"Container Execution","text":"<p>Run comprehensive benchmarks in a containerized environment:</p> <pre><code>docker run --rm \\\n    --network host \\\n    -v $(pwd)/results:/app/results \\\n    -v $(pwd)/parsed:/app/parsed \\\n    -e MODEL_ENDPOINT=\"http://localhost:8080/v1/completions\" \\\n    -e MODEL_NAME=\"Qwen/Qwen2-0.5B\" \\\n    -e TOKENIZER=\"Qwen/Qwen2-0.5B\" \\\n    standard-evalchemy:latest\n</code></pre> <p>Network Configuration</p> <p><code>--network host</code> resolves MODEL ID and MODEL NAME resolution issues by providing direct access to the host network stack.</p>"},{"location":"api/evalchemy-api/#environment-variables-explained","title":"Environment Variables Explained","text":"<p>Model Configuration:</p> <ul> <li><code>MODEL_ENDPOINT</code>: Complete API endpoint URL including path</li> <li><code>MODEL_NAME</code>: HuggingFace model identifier</li> <li><code>SERVED_MODEL_NAME</code>: Model name as configured in VLLM server</li> <li><code>TOKENIZER</code>: Tokenizer specification for accurate token counting</li> <li><code>TOKENIZER_BACKEND</code>: Backend implementation (<code>huggingface</code>, <code>tiktoken</code>)</li> </ul> <p>Evaluation Configuration:</p> <ul> <li><code>MODEL_CONFIG</code>: JSON configuration for API behavior and retries</li> <li><code>EVALUATION_CONFIG</code>: Benchmark parameters including limits and output format</li> </ul>"},{"location":"api/evalchemy-api/#output-structure","title":"Output Structure","text":"<p>After execution, results are organized in the following structure:</p> <pre><code>\u251c\u2500\u2500 eval/\n\u2502   \u2514\u2500\u2500 standard_evalchemy/\n\u2502       \u251c\u2500\u2500 parsed/          # Processed benchmark data\n\u2502       \u2514\u2500\u2500 results/         # Evaluation outcomes and metrics\n</code></pre> <p>Output Details</p> <ul> <li>parsed/: Contains preprocessed benchmark questions and expected answers</li> <li>results/: Includes model responses, accuracy metrics, and detailed analysis</li> </ul>"},{"location":"api/evalchemy-api/#script-based-evaluation","title":"Script-Based Evaluation","text":""},{"location":"api/evalchemy-api/#environment-setup","title":"Environment Setup","text":""},{"location":"api/evalchemy-api/#install-core-dependencies","title":"Install Core Dependencies","text":"<pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"api/evalchemy-api/#install-evalchemy-library","title":"Install Evalchemy Library","text":"<pre><code>git clone https://github.com/ThakiCloud/evalchemy.git\ncd evalchemy\npip install -e .\ncd ..\n</code></pre> <p>Editable Installation</p> <p>The <code>-e</code> flag installs Evalchemy in editable mode, allowing for local modifications and development.</p>"},{"location":"api/evalchemy-api/#running-evaluations","title":"Running Evaluations","text":"<p>Execute benchmarks using the provided shell script:</p> <pre><code>./run_evalchemy.sh \\\n  --endpoint http://localhost:8000/v1/completions \\\n  --model-name \"facebook/opt-125m\" \\\n  --tokenizer \"facebook/opt-125m\" \\\n  --tokenizer-backend \"huggingface\" \\\n  --batch-size 1 \\\n  --run-id test_01\n</code></pre> <p>Script Parameters:</p> <ul> <li><code>--endpoint</code>: API endpoint for model inference</li> <li><code>--model-name</code>: Model identifier for evaluation context</li> <li><code>--tokenizer</code>: Tokenizer specification for proper preprocessing</li> <li><code>--tokenizer-backend</code>: Tokenization implementation choice</li> <li><code>--batch-size</code>: Number of parallel requests (adjust based on server capacity)</li> <li><code>--run-id</code>: Unique identifier for tracking evaluation runs</li> </ul>"},{"location":"api/evalchemy-api/#script-output-structure","title":"Script Output Structure","text":"<p>Results are generated in the same directory structure:</p> <pre><code>\u251c\u2500\u2500 eval/\n\u2502   \u2514\u2500\u2500 standard_evalchemy/\n\u2502       \u251c\u2500\u2500 parsed/          # Benchmark preprocessing results\n\u2502       \u2514\u2500\u2500 results/         # Evaluation metrics and analysis\n</code></pre>"},{"location":"api/evalchemy-api/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"api/evalchemy-api/#custom-benchmark-selection","title":"Custom Benchmark Selection","text":"<p>Modify evaluation parameters by editing configuration files:</p> <pre><code># Edit benchmark selection\nvim configs/evaluation_config.yaml\n\n# Available benchmarks\nbenchmarks:\n  - arc_easy\n  - arc_challenge  \n  - hellaswag\n  - mmlu\n  - ko_mmlu\n  - ko_arc\n</code></pre>"},{"location":"api/evalchemy-api/#performance-tuning","title":"Performance Tuning","text":"<p>Optimization Tips</p> <p>For High-Throughput Evaluation:</p> <ul> <li>Increase <code>--batch-size</code> based on server capacity</li> <li>Configure appropriate retry policies in <code>MODEL_CONFIG</code></li> <li>Monitor server resource utilization during evaluation</li> </ul> <p>For Memory-Constrained Environments:</p> <ul> <li>Reduce batch size to minimize memory usage</li> <li>Use streaming evaluation for large datasets</li> <li>Configure appropriate timeouts to prevent hanging requests</li> </ul>"},{"location":"api/evalchemy-api/#multi-gpu-evaluation","title":"Multi-GPU Evaluation","text":"<p>For accelerated evaluation on multi-GPU systems:</p> <pre><code>docker run --rm \\\n    --gpus all \\\n    --network host \\\n    -e CUDA_VISIBLE_DEVICES=\"0,1,2,3\" \\\n    -e MODEL_ENDPOINT=\"http://localhost:8080/v1/completions\" \\\n    # ... other environment variables\n    standard-evalchemy:latest\n</code></pre>"},{"location":"api/evalchemy-api/#benchmark-coverage","title":"Benchmark Coverage","text":""},{"location":"api/evalchemy-api/#standard-academic-benchmarks","title":"Standard Academic Benchmarks","text":"<p>Supported Benchmarks</p> <p>English Benchmarks:</p> <ul> <li>ARC (Easy/Challenge): Science question answering</li> <li>HellaSwag: Commonsense reasoning completion</li> <li>MMLU: Massive multitask language understanding</li> </ul> <p>Korean Benchmarks:</p> <ul> <li>Ko-MMLU: Korean multitask language understanding  </li> <li>Ko-ARC: Korean science question answering</li> </ul>"},{"location":"api/evalchemy-api/#custom-benchmark-integration","title":"Custom Benchmark Integration","text":"<p>Evalchemy supports custom benchmark integration:</p> <pre><code># Custom benchmark configuration\n{\n    \"task\": \"custom_benchmark\",\n    \"dataset_path\": \"/path/to/custom/data.jsonl\",\n    \"metric\": \"exact_match\",\n    \"few_shot\": 5\n}\n</code></pre>"},{"location":"api/evalchemy-api/#result-analysis","title":"Result Analysis","text":""},{"location":"api/evalchemy-api/#metric-interpretation","title":"Metric Interpretation","text":"<p>Understanding Results</p> <p>Key Metrics:</p> <ul> <li>Accuracy: Percentage of correctly answered questions</li> <li>F1 Score: Harmonic mean of precision and recall</li> <li>Exact Match: Strict equality comparison for answers</li> <li>BLEU Score: Text similarity for generative tasks</li> </ul>"},{"location":"api/evalchemy-api/#comparative-analysis","title":"Comparative Analysis","text":"<p>Results include comparative analysis against baseline models:</p> <pre><code>{\n    \"model\": \"Qwen/Qwen2-0.5B\",\n    \"benchmark\": \"arc_easy\",\n    \"accuracy\": 0.785,\n    \"baseline_comparison\": {\n        \"random_baseline\": 0.25,\n        \"human_performance\": 0.95,\n        \"relative_performance\": 0.713\n    }\n}\n</code></pre>"},{"location":"api/evalchemy-api/#integration-with-vllm-eval-pipeline","title":"Integration with VLLM Eval Pipeline","text":""},{"location":"api/evalchemy-api/#automated-pipeline-integration","title":"Automated Pipeline Integration","text":"<p>Evalchemy integrates seamlessly with the broader VLLM evaluation ecosystem:</p> <pre><code># Integration with aggregation pipeline\npython scripts/aggregate_metrics.py \\\n    --include-evalchemy \\\n    --results-dir results\n</code></pre> <p>Pipeline Compatibility</p> <p>Evalchemy results are automatically compatible with the VLLM evaluation aggregation system and ClickHouse analytics pipeline.</p>"},{"location":"api/evalchemy-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/evalchemy-api/#common-issues","title":"Common Issues","text":"<p>Troubleshooting Guide</p> <p>API Connection Issues:</p> <ul> <li>Verify VLLM server is running and accessible</li> <li>Check endpoint URL format and network connectivity</li> <li>Validate API authentication if required</li> </ul> <p>Memory Issues:</p> <ul> <li>Reduce batch size for memory-constrained environments</li> <li>Monitor system memory usage during evaluation</li> <li>Consider using swap space for large evaluations</li> </ul> <p>Performance Issues:</p> <ul> <li>Optimize batch size based on server capacity</li> <li>Configure appropriate retry policies</li> <li>Monitor server resource utilization</li> </ul>"},{"location":"api/evalchemy-api/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging for troubleshooting:</p> <pre><code>docker run --rm \\\n    --network host \\\n    -e DEBUG_MODE=\"true\" \\\n    -e LOG_LEVEL=\"DEBUG\" \\\n    # ... other environment variables\n    standard-evalchemy:latest\n</code></pre>"},{"location":"api/evalchemy-api/#next-steps","title":"Next Steps","text":"<p>Advanced Usage</p> <p>After successful evaluation:</p> <ul> <li>Analyze results using provided analysis scripts</li> <li>Integrate with monitoring and alerting systems</li> <li>Configure automated evaluation pipelines</li> <li>Develop custom benchmarks for domain-specific evaluation</li> </ul>"},{"location":"api/nvidia-eval-api/","title":"NVIDIA Eval API","text":"<p>The NVIDIA Eval API provides specialized evaluation capabilities for mathematical reasoning and coding tasks using industry-standard benchmarks. This API focuses on rigorous evaluation of large language models across different domains including competitive programming and mathematical problem-solving.</p> <p>Overview</p> <p>NVIDIA Eval supports three main benchmark categories:</p> <ul> <li>LiveCodeBench: Dynamic coding benchmark with recent problems</li> <li>AIME24/AIME25: Mathematical reasoning benchmarks from American Invitational Mathematics Examination</li> <li>Docker Support: Containerized execution for consistent environments</li> </ul>"},{"location":"api/nvidia-eval-api/#prerequisites","title":"Prerequisites","text":"<p>Requirements</p> <p>Before using NVIDIA Eval, ensure you have:</p> <ul> <li>A running VLLM server endpoint (e.g.,<code>http://localhost:8000/v1</code>)</li> <li>Python environment with required dependencies</li> <li>Docker installed (for containerized execution)</li> <li>Sufficient computational resources for inference</li> </ul>"},{"location":"api/nvidia-eval-api/#livecodebench-evaluation","title":"LiveCodeBench Evaluation","text":"<p>LiveCodeBench provides a dynamic evaluation platform for coding tasks with problems sourced from recent competitive programming contests, ensuring minimal data contamination.</p> <p>About LiveCodeBench</p> <p>LiveCodeBench is continuously updated with new problems, making it ideal for evaluating code generation capabilities without training data leakage concerns.</p>"},{"location":"api/nvidia-eval-api/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"api/nvidia-eval-api/#1-dataset-preparation","title":"1. Dataset Preparation","text":"<p>First, download the LiveCodeBench dataset:</p> <pre><code>python download_livecodebench.py\n</code></pre> <p>Dataset Location</p> <p>The dataset will be saved to<code>data/livecodebench_problems.jsonl</code> and contains coding problems with test cases.</p>"},{"location":"api/nvidia-eval-api/#2-model-inference","title":"2. Model Inference","text":"<p>Run inference against your model endpoint:</p> <pre><code>python inference.py \\\n    --api-base http://localhost:8000/v1 \\\n    --datapath data/livecodebench_problems.jsonl \\\n    --model-type opt125m \\\n    --output-folder \"./results_livecodebench\"\n</code></pre> <p>Parameter Explanation:</p> <ul> <li><code>--api-base</code>: Your VLLM server endpoint</li> <li><code>--datapath</code>: Path to the dataset file</li> <li><code>--model-type</code>: Model identifier for logging</li> <li><code>--output-folder</code>: Directory for storing inference results</li> </ul>"},{"location":"api/nvidia-eval-api/#3-evaluation","title":"3. Evaluation","text":"<p>Evaluate the generated solutions:</p> <pre><code>python evaluate_livecodebench.py \\\n --question-path data/livecodebench_problems.jsonl \\\n --generation-path results_livecodebench/\n</code></pre> <p>Expected Output</p> <p>The evaluation will generate accuracy metrics, execution success rates, and detailed analysis of coding performance.</p>"},{"location":"api/nvidia-eval-api/#aime-mathematical-benchmarks","title":"AIME Mathematical Benchmarks","text":"<p>The American Invitational Mathematics Examination (AIME) benchmarks test advanced mathematical reasoning capabilities across algebra, geometry, number theory, and combinatorics.</p> <p>Benchmark Details</p> <ul> <li>AIME24: ~30 challenging mathematical problems from 2024</li> <li>AIME25: ~15 problems from 2025</li> <li>Both datasets require multi-step reasoning and exact numerical answers</li> </ul>"},{"location":"api/nvidia-eval-api/#aime24-evaluation","title":"AIME24 Evaluation","text":""},{"location":"api/nvidia-eval-api/#1-model-inference","title":"1. Model Inference","text":"<pre><code>python inference.py \\\n    --api-base http://localhost:8000/v1 \\\n    --datapath data/aime24.jsonl \\\n    --model-type opt125m \\\n    --output-folder \"./results_aime24\"\n</code></pre>"},{"location":"api/nvidia-eval-api/#2-evaluation","title":"2. Evaluation","text":"<pre><code>python evaluate_aime.py \\\n    --question-path data/aime24.jsonl \\\n    --generation-path results_aime24\n</code></pre>"},{"location":"api/nvidia-eval-api/#aime25-evaluation","title":"AIME25 Evaluation","text":""},{"location":"api/nvidia-eval-api/#1-model-inference_1","title":"1. Model Inference","text":"<pre><code>python inference.py \\\n    --api-base http://localhost:8000/v1 \\\n    --datapath data/aime25.jsonl \\\n    --model-type opt125m \\\n    --output-folder \"./results_aime25\"\n</code></pre>"},{"location":"api/nvidia-eval-api/#2-evaluation_1","title":"2. Evaluation","text":"<pre><code>python evaluate_aime.py \\\n    --question-path data/aime25.jsonl \\\n    --generation-path results_aime25\n</code></pre> <p>Mathematical Evaluation</p> <p>AIME evaluations use exact match scoring for numerical answers. The evaluation script handles various answer formats and provides detailed problem-by-problem analysis.</p>"},{"location":"api/nvidia-eval-api/#docker-deployment","title":"Docker Deployment","text":"<p>For consistent execution environments and simplified deployment, use the Docker-based approach:</p>"},{"location":"api/nvidia-eval-api/#building-the-image","title":"Building the Image","text":"<pre><code>docker build -f docker/nvidia-eval.Dockerfile -t nvidia-benchmark:latest .\n</code></pre> <p>Build Requirements</p> <p>The Docker build process requires significant resources and may take several minutes. Ensure you have adequate disk space and memory.</p>"},{"location":"api/nvidia-eval-api/#running-evaluations","title":"Running Evaluations","text":"<pre><code>docker run --rm \\\n  --network host \\\n  -v $(pwd)/results:/app/results \\\n  -e MODEL_ENDPOINT=\"http://localhost:8080/v1\" \\\n  -e MODEL_NAME=\"facebook/opt-125m\" \\\n  -e OUTPUT_DIR=\"/app/results\" \\\n  -e EVAL_TYPE=\"aime\" \\\n  -e MAX_TOKENS=\"512\" \\\n  nvidia-benchmark:latest\n</code></pre> <p>Environment Variables:</p> <ul> <li><code>MODEL_ENDPOINT</code>: VLLM server endpoint URL</li> <li><code>OUTPUT_DIR</code>: Container output directory</li> <li><code>EVAL_TYPE</code>: Benchmark type (<code>aime</code>, <code>lcb</code>, <code>both</code>)</li> <li><code>MAX_TOKENS</code>: Maximum output sequence length</li> <li>Volume mount maps local <code>output</code> directory to container workspace</li> </ul> <p>Network Configuration</p> <p>The<code>--network host</code> flag enables the container to access the host's VLLM server. Adjust network settings based on your deployment architecture.</p>"},{"location":"api/nvidia-eval-api/#output-structure","title":"Output Structure","text":"<p>After running evaluations, you'll find results organized as:</p> <pre><code>results/\n\u251c\u2500\u2500 results_livecodebench/     # LiveCodeBench inference results\n\u251c\u2500\u2500 results_aime24/            # AIME24 inference results\n\u251c\u2500\u2500 results_aime25/            # AIME25 inference results\n\u251c\u2500\u2500 evaluation_results.json    # Aggregated metrics\n\u2514\u2500\u2500 logs/                      # Detailed execution logs\n</code></pre> <p>Next Steps</p> <p>Use the generated results for model comparison, performance analysis, and integration with the broader VLLM evaluation pipeline.</p>"},{"location":"api/vllm-benchmark-api/","title":"VLLM Benchmark API","text":"<p>The VLLM Benchmark API provides comprehensive performance evaluation capabilities for VLLM-served language models using the official VLLM <code>benchmark_serving.py</code> tool. This sophisticated benchmarking system supports multi-scenario configuration, detailed statistical analysis, and result standardization for integration with the broader evaluation pipeline.</p> <p>Benchmark Overview</p> <p>VLLM Benchmark provides:</p> <ul> <li>Multi-Scenario Testing: JSON-based configuration for complex benchmark scenarios</li> <li>Performance Metrics: TTFT, TPOT, ITL, E2EL with detailed percentile analysis</li> <li>Concurrency Testing: Configurable concurrent request handling evaluation  </li> <li>Result Analysis: Sophisticated statistical analysis and result standardization</li> <li>Integration Support: Standardized output for pipeline integration</li> </ul>"},{"location":"api/vllm-benchmark-api/#prerequisites","title":"Prerequisites","text":"<p>Requirements</p> <p>Before running VLLM benchmarks, ensure:</p> <ul> <li>VLLM server is running and accessible</li> <li>Docker is installed for containerized benchmarking</li> <li>Sufficient system resources for load testing</li> <li>Network connectivity between benchmark client and VLLM server</li> </ul>"},{"location":"api/vllm-benchmark-api/#performance-metrics-explained","title":"Performance Metrics Explained","text":""},{"location":"api/vllm-benchmark-api/#key-performance-indicators","title":"Key Performance Indicators","text":"<p>Understanding Metrics</p> <p>Throughput Metrics:</p> <ul> <li>Request Throughput: Completed requests per second</li> <li>Output Token Throughput: Generated tokens per second</li> <li>Total Token Throughput: Combined input/output tokens per second</li> </ul> <p>Latency Metrics:</p> <ul> <li>TTFT (Time to First Token): Latency until first token generation</li> <li>TPOT (Time per Output Token): Average time between subsequent tokens</li> <li>ITL (Inter-token Latency): Real-time token generation intervals</li> <li>E2EL (End-to-End Latency): Complete request processing time</li> </ul>"},{"location":"api/vllm-benchmark-api/#configuration-system","title":"Configuration System","text":"<p>VLLM benchmarks use a JSON-based configuration system located at <code>configs/vllm_benchmark.json</code>:</p> <pre><code>{\n  \"name\": \"VLLM Performance Benchmark\",\n  \"version\": \"2.0.0\", \n  \"description\": \"VLLM \uc11c\ube59 \uc131\ub2a5 \uce21\uc815 \ubca4\uce58\ub9c8\ud06c\",\n  \"defaults\": {\n    \"backend\": \"openai-chat\",\n    \"endpoint_path\": \"/v1/chat/completions\",\n    \"dataset_type\": \"random\",\n    \"percentile_metrics\": \"ttft,tpot,itl,e2el\",\n    \"metric_percentiles\": \"25,50,75,90,95,99\"\n  },\n  \"scenarios\": [\n    {\n      \"name\": \"basic_performance\",\n      \"description\": \"\uae30\ubcf8 \uc131\ub2a5 \uce21\uc815\",\n      \"max_concurrency\": 1,\n      \"random_input_len\": 1024,\n      \"random_output_len\": 1024\n    }\n  ],\n  \"thresholds\": {\n    \"ttft_p95_ms\": 200,\n    \"tpot_mean_ms\": 50,\n    \"throughput_min\": 10,\n    \"success_rate\": 0.95\n  }\n}\n</code></pre>"},{"location":"api/vllm-benchmark-api/#script-based-execution","title":"Script-Based Execution","text":"<p>The primary benchmarking tool is <code>eval/vllm-benchmark/run_vllm_benchmark.sh</code>:</p>"},{"location":"api/vllm-benchmark-api/#basic-usage","title":"Basic Usage","text":"<pre><code>cd eval/vllm-benchmark\n./run_vllm_benchmark.sh\n</code></pre>"},{"location":"api/vllm-benchmark-api/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>MODEL_ENDPOINT</code>: VLLM server base URL (default: <code>http://localhost:8000</code>)</li> <li><code>CONFIG_PATH</code>: Configuration file path (default: <code>configs/vllm_benchmark.json</code>)</li> <li><code>OUTPUT_DIR</code>: Results output directory (default: <code>/app/results</code>)</li> <li><code>REQUEST_RATE</code>: Request rate for load testing (default: <code>1.0</code>)</li> <li><code>MODEL_NAME</code>: Model identifier</li> <li><code>SERVED_MODEL_NAME</code>: Model name as configured in VLLM server</li> <li><code>TOKENIZER</code>: Tokenizer specification (default: <code>gpt2</code>)</li> </ul>"},{"location":"api/vllm-benchmark-api/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>MODEL_ENDPOINT=\"http://localhost:8080\" \\\nCONFIG_PATH=\"configs/custom_benchmark.json\" \\\nOUTPUT_DIR=\"./benchmark_results\" \\\nMODEL_NAME=\"Qwen/Qwen2-0.5B\" \\\n./run_vllm_benchmark.sh\n</code></pre>"},{"location":"api/vllm-benchmark-api/#docker-based-benchmarking","title":"Docker-Based Benchmarking","text":""},{"location":"api/vllm-benchmark-api/#building-the-benchmark-image","title":"Building the Benchmark Image","text":"<pre><code>docker build -f docker/vllm-benchmark.Dockerfile -t vllm-benchmark:latest .\n</code></pre>"},{"location":"api/vllm-benchmark-api/#running-containerized-benchmarks","title":"Running Containerized Benchmarks","text":"<pre><code>docker run --rm \\\n  --network host \\\n  -e MODEL_ENDPOINT=\"http://localhost:8080\" \\\n  -e MODEL_NAME=\"Qwen/Qwen2-0.5B\" \\\n  -e TOKENIZER=\"gpt2\" \\\n  -e OUTPUT_DIR=\"/app/results\" \\\n  -e PARSED_DIR=\"/app/parsed\" \\\n  -v $(pwd)/results:/app/results \\\n  -v $(pwd)/parsed:/app/parsed \\\n  vllm-benchmark:latest\n</code></pre> <p>Model Endpoint Validation</p> <p>The script automatically validates the model endpoint and extracts the model ID from <code>/v1/models</code> before running benchmarks.</p>"},{"location":"api/vllm-benchmark-api/#scenario-configuration","title":"Scenario Configuration","text":""},{"location":"api/vllm-benchmark-api/#scenario-parameters","title":"Scenario Parameters","text":"<p>Each scenario in the configuration supports the following parameters:</p> <p>Core Parameters:</p> <ul> <li><code>name</code>: Unique scenario identifier</li> <li><code>description</code>: Human-readable scenario description</li> <li><code>max_concurrency</code>: Maximum concurrent requests</li> <li><code>num_prompts</code>: Total number of prompts to process</li> <li><code>random_input_len</code>: Input token length for random dataset</li> <li><code>random_output_len</code>: Expected output token length</li> </ul> <p>Advanced Parameters:</p> <ul> <li><code>backend</code>: Backend type (<code>openai-chat</code>, <code>openai</code>, <code>tgi</code>)</li> <li><code>endpoint_path</code>: API endpoint path (e.g., <code>/v1/chat/completions</code>)</li> <li><code>dataset_type</code>: Dataset type (<code>random</code>, <code>synthetic</code>)</li> <li><code>percentile_metrics</code>: Metrics to calculate percentiles for</li> <li><code>metric_percentiles</code>: Percentile values to compute</li> </ul>"},{"location":"api/vllm-benchmark-api/#multi-scenario-example","title":"Multi-Scenario Example","text":"<pre><code>{\n  \"scenarios\": [\n    {\n      \"name\": \"light_load\",\n      \"description\": \"Light load testing\",\n      \"max_concurrency\": 1,\n      \"num_prompts\": 10,\n      \"random_input_len\": 512,\n      \"random_output_len\": 128\n    },\n    {\n      \"name\": \"heavy_load\", \n      \"description\": \"Heavy load testing\",\n      \"max_concurrency\": 8,\n      \"num_prompts\": 100,\n      \"random_input_len\": 2048,\n      \"random_output_len\": 512\n    }\n  ]\n}\n</code></pre>"},{"location":"api/vllm-benchmark-api/#sample-benchmark-output","title":"Sample Benchmark Output","text":""},{"location":"api/vllm-benchmark-api/#performance-results-analysis","title":"Performance Results Analysis","text":"<p>When you run the benchmark, you'll see comprehensive performance metrics:</p> <pre><code>Starting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: 1\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:15&lt;00:00, 15.13s/it]\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nMaximum request concurrency:             1         \nBenchmark duration (s):                  15.13     \nTotal input tokens:                      1024      \nTotal generated tokens:                  1024      \nRequest throughput (req/s):              0.07      \nOutput token throughput (tok/s):         67.69     \nTotal Token throughput (tok/s):          135.37    \n---------------Time to First Token----------------\nMean TTFT (ms):                          47.19     \nMedian TTFT (ms):                        47.19     \nP25 TTFT (ms):                           47.19     \nP50 TTFT (ms):                           47.19     \nP75 TTFT (ms):                           47.19     \nP90 TTFT (ms):                           47.19     \nP95 TTFT (ms):                           47.19     \nP99 TTFT (ms):                           47.19     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          14.74     \nMedian TPOT (ms):                        14.74     \nP25 TPOT (ms):                           14.74     \nP50 TPOT (ms):                           14.74     \nP75 TPOT (ms):                           14.74     \nP90 TPOT (ms):                           14.74     \nP95 TPOT (ms):                           14.74     \nP99 TPOT (ms):                           14.74     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           14.73     \nMedian ITL (ms):                         14.62     \nP25 ITL (ms):                            14.33     \nP50 ITL (ms):                            14.62     \nP75 ITL (ms):                            15.01     \nP90 ITL (ms):                            15.71     \nP95 ITL (ms):                            16.34     \nP99 ITL (ms):                            20.46     \n----------------End-to-end Latency----------------\nMean E2EL (ms):                          15126.62  \nMedian E2EL (ms):                        15126.62  \nP25 E2EL (ms):                           15126.62  \nP50 E2EL (ms):                           15126.62  \nP75 E2EL (ms):                           15126.62  \nP90 E2EL (ms):                           15126.62  \nP95 E2EL (ms):                           15126.62  \nP99 E2EL (ms):                           15126.62  \n==================================================\n</code></pre>"},{"location":"api/vllm-benchmark-api/#result-interpretation","title":"Result Interpretation","text":"<p>Performance Analysis</p> <p>Key Insights from Results:</p> <ul> <li>Throughput: 67.69 tokens/sec output rate indicates model generation speed</li> <li>TTFT: 47.19ms first token latency shows response initiation speed  </li> <li>TPOT: 14.74ms per token indicates consistent generation performance</li> <li>Percentiles: P90, P95, P99 values help identify tail latency behavior</li> </ul>"},{"location":"api/vllm-benchmark-api/#load-testing-scenarios","title":"Load Testing Scenarios","text":""},{"location":"api/vllm-benchmark-api/#single-client-performance","title":"Single Client Performance","text":"<p>The basic benchmark runs single-client scenarios to establish baseline performance characteristics.</p>"},{"location":"api/vllm-benchmark-api/#multi-client-concurrency","title":"Multi-Client Concurrency","text":"<p>Scaling Testing</p> <p>For production readiness assessment:</p> <ol> <li>Start with single-client baseline</li> <li>Gradually increase concurrency levels</li> <li>Monitor performance degradation patterns</li> <li>Identify optimal concurrency for your hardware</li> </ol>"},{"location":"api/vllm-benchmark-api/#stress-testing","title":"Stress Testing","text":"<pre><code># High-concurrency stress test\ndocker run --rm \\\n  --network host \\\n  -e MODEL_ENDPOINT=\"http://localhost:8080\" \\\n  -e MODEL_NAME=\"Qwen/Qwen2-0.5B\" \\\n  -e TOKENIZER=\"gpt2\" \\\n  -e OUTPUT_DIR=\"/results\" \\\n  -e PARSED_DIR=\"/parsed\" \\\n  -e CONCURRENCY_LEVELS=\"50,100\" \\\n  -e DURATION=\"600\" \\\n  -v $(pwd)/results:/results \\\n  -v $(pwd)/parsed:/parsed \\\n  vllm-benchmark:latest\n</code></pre>"},{"location":"api/vllm-benchmark-api/#result-analysis-and-processing","title":"Result Analysis and Processing","text":""},{"location":"api/vllm-benchmark-api/#automated-analysis","title":"Automated Analysis","text":"<p>The benchmark system includes sophisticated analysis capabilities via <code>eval/vllm-benchmark/analyze_vllm_results.py</code>:</p> <pre><code># Analyze specific result file\npython eval/vllm-benchmark/analyze_vllm_results.py /path/to/results.json\n\n# Analyze entire results directory\npython eval/vllm-benchmark/analyze_vllm_results.py /path/to/results/directory\n</code></pre>"},{"location":"api/vllm-benchmark-api/#output-directory-structure","title":"Output Directory Structure","text":"<pre><code>benchmark_results/\n\u251c\u2500\u2500 vllm_benchmark_main_TIMESTAMP.log           # Main execution log\n\u251c\u2500\u2500 scenario_NAME_TIMESTAMP.log                 # Per-scenario logs\n\u251c\u2500\u2500 scenario_NAME_TIMESTAMP/                    # Raw VLLM results\n\u2502   \u2514\u2500\u2500 benchmark_results.json\n\u2514\u2500\u2500 parsed/                                     # Standardized results\n    \u2514\u2500\u2500 SCENARIO_NAME_TIMESTAMP_standardized.json\n</code></pre>"},{"location":"api/vllm-benchmark-api/#result-standardization","title":"Result Standardization","text":"<p>Results are automatically standardized for pipeline integration:</p> <pre><code>python scripts/standardize_vllm_benchmark.py \\\n  results.json \\\n  --output_file standardized.json \\\n  --task_name \"performance_test\" \\\n  --config_path configs/vllm_benchmark.json\n</code></pre> <p>Analysis Features</p> <p>The analysis system provides:</p> <ul> <li>Statistical Summary: Mean, median, percentiles for all metrics</li> <li>Performance Thresholds: Automatic threshold validation</li> <li>Trend Analysis: Multi-run performance comparison</li> <li>Export Formats: JSON, CSV output for further processing</li> </ul>"},{"location":"api/vllm-benchmark-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/vllm-benchmark-api/#common-issues","title":"Common Issues","text":"<p>Troubleshooting Guide</p> <p>Connection Issues:</p> <ul> <li>Verify VLLM server is running and accessible</li> <li>Check network connectivity with <code>curl</code> test</li> <li>Ensure firewall rules allow benchmark traffic</li> </ul> <p>Performance Issues:</p> <ul> <li>Monitor GPU/CPU utilization during benchmarks</li> <li>Check for resource contention with other processes</li> <li>Verify model loading completed successfully</li> </ul> <p>Result Inconsistencies:</p> <ul> <li>Run multiple benchmark iterations for statistical stability</li> <li>Account for system warm-up time in measurements</li> <li>Monitor system load during benchmark execution</li> </ul>"},{"location":"api/vllm-benchmark-api/#integration-with-vllm-eval-pipeline","title":"Integration with VLLM Eval Pipeline","text":""},{"location":"api/vllm-benchmark-api/#pipeline-integration","title":"Pipeline Integration","text":"<p>VLLM benchmark results integrate seamlessly with the evaluation aggregation system:</p> <pre><code># Include VLLM benchmark results in aggregation\npython scripts/aggregate_metrics.py \\\n  --include-vllm-benchmark \\\n  --results-dir eval/vllm-benchmark/parsed\n</code></pre>"},{"location":"api/vllm-benchmark-api/#performance-monitoring","title":"Performance Monitoring","text":"<p>Monitoring Integration</p> <p>Standardized results support:</p> <ul> <li>ClickHouse analytics database storage</li> <li>Grafana dashboard visualization</li> <li>Prometheus metrics export</li> <li>Automated regression detection</li> </ul>"},{"location":"api/vllm-benchmark-api/#best-practices","title":"Best Practices","text":""},{"location":"api/vllm-benchmark-api/#benchmark-configuration","title":"Benchmark Configuration","text":"<p>Optimization Tips</p> <p>For Accurate Results:</p> <ul> <li>Run multiple iterations for statistical stability</li> <li>Allow adequate warm-up time before measurements</li> <li>Monitor system resources during benchmarking</li> <li>Use consistent hardware configurations</li> </ul> <p>For Production Testing:</p> <ul> <li>Configure scenarios matching production workloads</li> <li>Test various concurrency levels progressively</li> <li>Establish baseline performance thresholds</li> <li>Automate benchmark execution in CI/CD pipelines</li> </ul>"},{"location":"api/vllm-benchmark-api/#troubleshooting_1","title":"Troubleshooting","text":"<p>Common Issues</p> <p>Configuration Issues:</p> <ul> <li>Verify JSON configuration syntax</li> <li>Ensure model endpoint accessibility</li> <li>Check parameter compatibility with VLLM version</li> </ul> <p>Performance Issues:</p> <ul> <li>Monitor GPU memory utilization</li> <li>Check for CPU bottlenecks during high concurrency</li> <li>Validate network connectivity stability</li> </ul> <p>Result Issues:</p> <ul> <li>Ensure sufficient disk space for results</li> <li>Verify write permissions to output directories</li> <li>Check for incomplete benchmark runs in logs</li> </ul>"},{"location":"api/vllm-benchmark-api/#next-steps","title":"Next Steps","text":"<p>Advanced Usage</p> <p>After successful benchmarking:</p> <ul> <li>Analyze results using the provided analysis tools</li> <li>Configure performance thresholds for automated validation</li> <li>Integrate with monitoring and alerting systems</li> <li>Create custom scenarios for specific use cases</li> <li>Set up automated performance regression testing</li> </ul>"},{"location":"architecture/system-overview/","title":"System Overview","text":"<p>The VLLM Evaluation System provides a comprehensive solution for evaluating large language models through a dual architecture approach that supports both local development workflows and scalable production deployments.</p> <p>Architecture Highlights</p> <ul> <li>\ud83c\udfd7\ufe0f Dual Architecture: CLI-based local development + Kubernetes production</li> <li>\ud83d\udd0c Unified Interface: Single CLI for all evaluation frameworks</li> <li>\ud83e\uddea Flexible Deployment: From laptop to enterprise scale</li> <li>\ud83d\udcca Standardized Results: Consistent metrics across all execution modes</li> </ul>"},{"location":"architecture/system-overview/#architecture-overview","title":"Architecture Overview","text":""},{"location":"architecture/system-overview/#dual-execution-paradigm","title":"Dual Execution Paradigm","text":"<p>The system operates through two complementary architectural approaches:</p> <pre><code>graph TB\n    User[\"\ud83d\udc64 User\"] --&gt; CLI[\"\ud83d\udda5\ufe0f CLI Interface\"]\n    User --&gt; K8s[\"\u2638\ufe0f Kubernetes Interface\"]\n\n    CLI --&gt; LocalExec[\"\ud83d\udcbb Local Execution\"]\n    CLI --&gt; K8sGen[\"\ud83c\udfd7\ufe0f K8s Job Generation\"]\n\n    LocalExec --&gt; Adapters[\"\ud83d\udd0c Framework Adapters\"]\n    K8sGen --&gt; ArgoWF[\"\ud83c\udf0a Argo Workflows\"]\n\n    Adapters --&gt; Frameworks[\"\ud83e\uddea Evaluation Frameworks\"]\n    ArgoWF --&gt; Containers[\"\ud83d\udce6 Containerized Runners\"]\n\n    Frameworks --&gt; Results[\"\ud83d\udcca Results\"]\n    Containers --&gt; ClickHouse[\"\ud83d\uddc4\ufe0f ClickHouse\"]\n\n    Results --&gt; Analysis[\"\ud83d\udcc8 Local Analysis\"]\n    ClickHouse --&gt; Grafana[\"\ud83d\udcca Grafana Dashboards\"]</code></pre>"},{"location":"architecture/system-overview/#1-cli-based-architecture-development-local","title":"1. CLI-Based Architecture (Development &amp; Local)","text":"<p>Purpose: Rapid development, testing, and local evaluation</p> <p>Components:</p> <ul> <li>CLI Interface: Typer-based command system with Rich output</li> <li>Framework Adapters: Direct integration with evaluation systems</li> <li>Configuration Management: TOML-based profiles and settings</li> <li>Local Results: JSON/CSV output with immediate analysis</li> </ul> <p>Usage Patterns:</p> <ul> <li>Model development and iteration</li> <li>Quick quality assessments</li> <li>Configuration testing</li> <li>Local benchmarking</li> </ul>"},{"location":"architecture/system-overview/#2-kubernetes-native-architecture-production","title":"2. Kubernetes-Native Architecture (Production)","text":"<p>Purpose: Scalable, automated, production evaluation pipelines</p> <p>Components:</p> <ul> <li>Argo Workflows: Orchestration and pipeline management</li> <li>Containerized Runners: Isolated evaluation environments</li> <li>ClickHouse: Centralized metrics storage</li> <li>Grafana: Real-time monitoring and visualization</li> <li>Event-Driven Triggers: Automated evaluation on model updates</li> </ul> <p>Usage Patterns: - Continuous integration/deployment - Production model monitoring - Large-scale benchmark campaigns - Team collaboration and reporting</p>"},{"location":"architecture/system-overview/#system-components","title":"System Components","text":""},{"location":"architecture/system-overview/#core-cli-system","title":"Core CLI System","text":"<pre><code>graph LR\n    CLI[\"CLI Entry Point\"] --&gt; Commands[\"Command Modules\"]\n    Commands --&gt; Run[\"Run Commands\"]\n    Commands --&gt; Config[\"Config Management\"]\n    Commands --&gt; System[\"System Utils\"]\n    Commands --&gt; Results[\"Results Management\"]\n\n    Run --&gt; Adapters[\"Framework Adapters\"]\n    Adapters --&gt; Base[\"Base Adapter\"]\n    Adapters --&gt; Evalchemy[\"Evalchemy\"]\n    Adapters --&gt; NVIDIA[\"NVIDIA Eval\"]\n    Adapters --&gt; VLLM[\"VLLM Benchmark\"]\n    Adapters --&gt; Deepeval[\"Deepeval\"]\n\n    Config --&gt; ConfigCore[\"Config Core\"]\n    ConfigCore --&gt; TOML[\"TOML Files\"]\n    ConfigCore --&gt; Validation[\"Validation\"]\n    ConfigCore --&gt; Profiles[\"Profile Management\"]</code></pre>"},{"location":"architecture/system-overview/#cli-architecture-principles","title":"CLI Architecture Principles","text":"<ol> <li>Adapter Pattern: Unified interface across evaluation frameworks</li> <li>Plugin Architecture: Easy extension with new frameworks</li> <li>Configuration Hierarchy: Environment-specific settings</li> <li>Rich User Experience: Progress indicators, colored output, help system</li> <li>Validation First: Comprehensive prerequisite checking</li> </ol>"},{"location":"architecture/system-overview/#kubernetes-production-system","title":"Kubernetes Production System","text":"<pre><code>graph TB\n    GHCR[\"\ud83d\udce6 GHCR Registry\"] --&gt; Trigger[\"\u26a1 Argo Events\"]\n    Trigger --&gt; Workflow[\"\ud83c\udf0a Argo Workflow\"]\n\n    Workflow --&gt; Prepare[\"\ud83d\udccb Dataset Prep\"]\n    Prepare --&gt; Deepeval[\"\ud83e\udde0 Deepeval Runner\"]\n    Prepare --&gt; Evalchemy[\"\ud83d\udcda Evalchemy Runner\"]\n    Prepare --&gt; NVIDIA[\"\ud83d\udd2c NVIDIA Runner\"]\n\n    Deepeval --&gt; Aggregate[\"\ud83d\udcca Metrics Aggregation\"]\n    Evalchemy --&gt; Aggregate\n    NVIDIA --&gt; Aggregate\n\n    Aggregate --&gt; ClickHouse[\"\ud83d\uddc4\ufe0f ClickHouse\"]\n    ClickHouse --&gt; Grafana[\"\ud83d\udcc8 Grafana\"]\n    Aggregate --&gt; Teams[\"\ud83d\udce2 Teams Notification\"]</code></pre>"},{"location":"architecture/system-overview/#production-architecture-principles","title":"Production Architecture Principles","text":"<ol> <li>Event-Driven: Automated triggers from model updates</li> <li>Containerized Isolation: Each framework runs in dedicated containers</li> <li>Pipeline Orchestration: Argo Workflows manage execution order</li> <li>Centralized Storage: ClickHouse for metrics, MinIO for artifacts</li> <li>Observability: Comprehensive monitoring and alerting</li> </ol>"},{"location":"architecture/system-overview/#framework-integration","title":"Framework Integration","text":""},{"location":"architecture/system-overview/#evaluation-frameworks","title":"Evaluation Frameworks","text":"<p>The system integrates with multiple evaluation frameworks through a unified adapter pattern:</p> Framework Purpose CLI Adapter K8s Container Metrics Output Evalchemy Academic benchmarks (MMLU, ARC, HellaSwag) \u2705 <code>EvAlchemyAdapter</code> \u2705 <code>evalchemy.Dockerfile</code> Accuracy, F1 scores NVIDIA Eval Mathematical reasoning (AIME, LiveCodeBench) \u2705 <code>NVIDIAAdapter</code> \u2705 <code>nvidia-eval.Dockerfile</code> Problem-solving accuracy VLLM Benchmark Performance testing (TTFT, TPOT, throughput) \u2705 <code>VLLMBenchmarkAdapter</code> \u2705 <code>vllm-benchmark.Dockerfile</code> Latency, throughput metrics Deepeval Custom metrics and RAG evaluation \u2705 <code>DeepevalAdapter</code> \u2705 <code>deepeval.Dockerfile</code> Quality scores, relevance"},{"location":"architecture/system-overview/#adapter-architecture","title":"Adapter Architecture","text":"<pre><code># Base adapter interface\nclass BaseEvaluationAdapter:\n    def validate_prerequisites(self) -&gt; bool:\n        \"\"\"Check dependencies and configuration\"\"\"\n\n    def prepare_execution(self, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Prepare evaluation parameters\"\"\"\n\n    def execute_evaluation(self, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Run the actual evaluation\"\"\"\n\n    def parse_results(self, raw_results: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Standardize result format\"\"\"\n</code></pre>"},{"location":"architecture/system-overview/#data-flow-and-processing","title":"Data Flow and Processing","text":""},{"location":"architecture/system-overview/#cli-execution-flow","title":"CLI Execution Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant CLI as CLI Interface\n    participant CM as Config Manager\n    participant A as Adapter\n    participant F as Framework\n    participant R as Results\n\n    U-&gt;&gt;CLI: vllm-eval run evalchemy my-model\n    CLI-&gt;&gt;CM: Load configuration\n    CM-&gt;&gt;CLI: Configuration data\n    CLI-&gt;&gt;A: Initialize adapter\n    A-&gt;&gt;A: Validate prerequisites\n    A-&gt;&gt;F: Execute evaluation\n    F-&gt;&gt;A: Raw results\n    A-&gt;&gt;R: Standardized results\n    R-&gt;&gt;CLI: Formatted output\n    CLI-&gt;&gt;U: Display results + summary</code></pre>"},{"location":"architecture/system-overview/#kubernetes-execution-flow","title":"Kubernetes Execution Flow","text":"<pre><code>sequenceDiagram\n    participant GH as GHCR\n    participant AE as Argo Events\n    participant AW as Argo Workflows\n    participant P as Pod Runners\n    participant CH as ClickHouse\n    participant G as Grafana\n\n    GH-&gt;&gt;AE: Image push event\n    AE-&gt;&gt;AW: Trigger workflow\n    AW-&gt;&gt;P: Start evaluation pods\n    P-&gt;&gt;P: Run frameworks\n    P-&gt;&gt;CH: Store metrics\n    CH-&gt;&gt;G: Update dashboards\n    AW-&gt;&gt;AE: Completion notification</code></pre>"},{"location":"architecture/system-overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/system-overview/#cli-kubernetes-integration","title":"CLI \u2194 Kubernetes Integration","text":"<p>The CLI and Kubernetes systems are designed to complement each other:</p> <ol> <li>Configuration Compatibility: CLI TOML configs can generate K8s Job specs</li> <li>Result Format Standardization: Both systems produce compatible output</li> <li>Development \u2192 Production: CLI workflows transfer to K8s seamlessly</li> <li>Hybrid Execution: CLI can submit jobs to K8s clusters</li> </ol> <pre><code># CLI generates Kubernetes job from configuration\nvllm-eval config export --format kubernetes &gt; evaluation-job.yaml\nkubectl apply -f evaluation-job.yaml\n\n# CLI can submit to remote Kubernetes\nvllm-eval --profile production run all my-model --target kubernetes\n</code></pre>"},{"location":"architecture/system-overview/#result-standardization","title":"Result Standardization","text":"<p>Both execution modes produce standardized result formats:</p> <pre><code>{\n  \"metadata\": {\n    \"framework\": \"evalchemy\",\n    \"model_name\": \"my-model\", \n    \"execution_time\": \"2024-08-27T10:30:00Z\",\n    \"execution_mode\": \"cli|kubernetes\"\n  },\n  \"metrics\": {\n    \"accuracy\": 0.85,\n    \"f1_score\": 0.82,\n    \"framework_specific\": {}\n  },\n  \"artifacts\": {\n    \"logs\": \"path/to/logs\",\n    \"raw_results\": \"path/to/raw\"\n  }\n}\n</code></pre>"},{"location":"architecture/system-overview/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"architecture/system-overview/#local-development","title":"Local Development","text":"<pre><code># Quick setup for development\npip install -e .\nvllm-eval setup\nvllm-eval run evalchemy my-model\n</code></pre> <p>Characteristics: - Immediate feedback - Full debugging capabilities - Resource limited by local machine - Perfect for model development</p>"},{"location":"architecture/system-overview/#staging-environment","title":"Staging Environment","text":"<pre><code># Hybrid approach: CLI + lightweight K8s\nvllm-eval --profile staging run all my-model --parallel\n</code></pre> <p>Characteristics: - Validates production workflows - Limited resource allocation - Subset of production evaluations - Integration testing</p>"},{"location":"architecture/system-overview/#production-environment","title":"Production Environment","text":"<pre><code># Full Kubernetes deployment\nmake kind-deploy\nmake helm-install\nmake submit-workflow\n</code></pre> <p>Characteristics: - Fully automated pipelines - Scalable resource allocation - Comprehensive monitoring - Team collaboration features</p>"},{"location":"architecture/system-overview/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/system-overview/#frontend-interface","title":"Frontend &amp; Interface","text":"<ul> <li>CLI Framework: Typer with Rich for terminal UI</li> <li>Configuration: TOML with Pydantic validation</li> <li>Logging: Structured logging with configurable levels</li> </ul>"},{"location":"architecture/system-overview/#backend-processing","title":"Backend &amp; Processing","text":"<ul> <li>Language: Python 3.11+ with type hints</li> <li>Containerization: Docker with multi-stage builds</li> <li>Orchestration: Kubernetes with Argo Workflows</li> <li>Event Processing: Argo Events for triggers</li> </ul>"},{"location":"architecture/system-overview/#storage-monitoring","title":"Storage &amp; Monitoring","text":"<ul> <li>Metrics Storage: ClickHouse for time-series data</li> <li>Artifact Storage: MinIO S3-compatible storage</li> <li>Visualization: Grafana dashboards</li> <li>Monitoring: Prometheus metrics</li> </ul>"},{"location":"architecture/system-overview/#development-quality","title":"Development &amp; Quality","text":"<ul> <li>Package Management: pip with requirements.txt</li> <li>Code Quality: ruff, mypy, pre-commit hooks</li> <li>Testing: pytest with coverage reporting</li> <li>Documentation: MkDocs with Material theme</li> </ul>"},{"location":"architecture/system-overview/#security-model","title":"Security Model","text":""},{"location":"architecture/system-overview/#access-control","title":"Access Control","text":"<ul> <li>RBAC: Minimal Kubernetes service account permissions</li> <li>Secrets: External Secret Operator or Kubernetes Secrets</li> <li>Network: Pod security policies and network policies</li> </ul>"},{"location":"architecture/system-overview/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption: TLS for all network communication</li> <li>Audit Logging: Comprehensive action logging</li> <li>Data Retention: Configurable log and result retention</li> </ul>"},{"location":"architecture/system-overview/#compliance","title":"Compliance","text":"<ul> <li>Log Retention: \u2265 90 days for audit purposes</li> <li>Access Logging: All configuration and execution actions</li> <li>Secret Management: No hardcoded credentials</li> </ul>"},{"location":"architecture/system-overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/system-overview/#scalability-limits","title":"Scalability Limits","text":"Execution Mode Concurrent Evaluations Resource Limits Typical Duration CLI Local 1-4 (CPU cores) Local machine resources 30-120 minutes CLI Remote 1-10 (endpoint limits) Network + endpoint capacity 30-180 minutes Kubernetes 10-100+ (cluster size) K8s cluster resources 15-120 minutes"},{"location":"architecture/system-overview/#performance-requirements","title":"Performance Requirements","text":"<ul> <li>Evaluation Pipeline: Complete within 2 hours</li> <li>High Concurrency: Support for long context models</li> <li>Resource Efficiency: Optimal CPU/GPU utilization</li> </ul>"},{"location":"architecture/system-overview/#future-architecture-evolution","title":"Future Architecture Evolution","text":""},{"location":"architecture/system-overview/#planned-enhancements","title":"Planned Enhancements","text":"<ol> <li>Multi-Cloud Support: AWS, GCP, Azure deployment options</li> <li>Edge Computing: Distributed evaluation across edge nodes</li> <li>Real-time Streaming: Live evaluation result streaming</li> <li>Advanced Analytics: ML-powered result analysis and recommendations</li> </ol>"},{"location":"architecture/system-overview/#extension-points","title":"Extension Points","text":"<ol> <li>Custom Adapters: Plugin system for new evaluation frameworks</li> <li>Custom Metrics: User-defined evaluation criteria</li> <li>Custom Workflows: Configurable evaluation pipelines</li> <li>Custom Storage: Alternative storage backends</li> </ol>"},{"location":"architecture/system-overview/#related-documentation","title":"Related Documentation","text":""},{"location":"architecture/system-overview/#core-documentation","title":"Core Documentation","text":"<ul> <li>CLI Guide - Complete CLI usage and configuration</li> <li>API Reference - Framework-specific integration details</li> <li>User Guide - Getting started with evaluations</li> </ul>"},{"location":"architecture/system-overview/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Component Diagram - Detailed component relationships</li> <li>Data Flow - Data processing and storage flows</li> <li>Deployment Guide - Production deployment procedures</li> <li>Security Model - Security implementation details</li> </ul>"},{"location":"architecture/system-overview/#operational-documentation","title":"Operational Documentation","text":"<ul> <li>Operations Guide - Running and maintaining the system</li> <li>Monitoring Guide - Observability and alerting</li> <li>Troubleshooting - Common issues and solutions</li> </ul> <p>Architecture Overview Complete</p> <p>This system provides flexible, scalable model evaluation through dual architecture approaches.</p> <p>Next Steps: Explore CLI Guide for hands-on usage or Deployment Guide for production setup.</p>"},{"location":"cli/commands/","title":"Commands Reference","text":"<p>Complete reference for all VLLM Evaluation CLI commands.</p>"},{"location":"cli/commands/#core-commands","title":"Core Commands","text":""},{"location":"cli/commands/#vllm-eval-run","title":"<code>vllm-eval run</code>","text":"<p>Execute evaluation frameworks with specified models and configurations.</p>"},{"location":"cli/commands/#subcommands","title":"Subcommands","text":""},{"location":"cli/commands/#vllm-eval-run-evalchemy","title":"<code>vllm-eval run evalchemy</code>","text":"<p>Run Evalchemy benchmark evaluation.</p> <pre><code>vllm-eval run evalchemy &lt;model-name&gt; [OPTIONS]\n</code></pre> <p>Arguments: - <code>model-name</code>: Name or identifier of the model to evaluate</p> <p>Options: - <code>--endpoint TEXT</code>: VLLM server endpoint URL (default: http://localhost:8000/v1) - <code>--batch-size INTEGER</code>: Number of requests to process in parallel (default: 1) - <code>--config TEXT</code>: Configuration profile to use (default: default) - <code>--dry-run</code>: Show what would be executed without running - <code>--output-dir PATH</code>: Directory to save results (default: ./results)</p> <p>Examples: <pre><code># Basic evaluation\nvllm-eval run evalchemy my-model\n\n# With custom endpoint and batch size\nvllm-eval run evalchemy my-model --endpoint http://localhost:8000/v1 --batch-size 4\n\n# Using production configuration\nvllm-eval run evalchemy my-model --config production\n</code></pre></p>"},{"location":"cli/commands/#vllm-eval-run-nvidia","title":"<code>vllm-eval run nvidia</code>","text":"<p>Run NVIDIA evaluation suite (AIME, LiveCodeBench).</p> <pre><code>vllm-eval run nvidia &lt;model-name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--benchmark TEXT</code>: Benchmark to run (aime, livecodebench) - <code>--gpus INTEGER</code>: Number of GPUs to use (default: 1) - <code>--out-seq-len INTEGER</code>: Maximum output sequence length (default: 2048)</p>"},{"location":"cli/commands/#vllm-eval-run-vllm-benchmark","title":"<code>vllm-eval run vllm-benchmark</code>","text":"<p>Run VLLM performance benchmarks.</p> <pre><code>vllm-eval run vllm-benchmark &lt;model-name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--scenario TEXT</code>: Benchmark scenario (performance, throughput, latency) - <code>--concurrency INTEGER</code>: Number of concurrent requests (default: 10) - <code>--duration INTEGER</code>: Test duration in seconds (default: 300)</p>"},{"location":"cli/commands/#vllm-eval-run-deepeval","title":"<code>vllm-eval run deepeval</code>","text":"<p>Run Deepeval testing framework.</p> <pre><code>vllm-eval run deepeval &lt;model-name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--suite TEXT</code>: Test suite to run (default, rag, custom) - <code>--metrics TEXT</code>: Comma-separated metrics (precision, recall, faithfulness)</p>"},{"location":"cli/commands/#vllm-eval-run-all","title":"<code>vllm-eval run all</code>","text":"<p>Run all enabled frameworks sequentially.</p> <pre><code>vllm-eval run all &lt;model-name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--frameworks TEXT</code>: Comma-separated list of frameworks to run - <code>--continue-on-error</code>: Continue running other frameworks if one fails - <code>--parallel</code>: Run frameworks in parallel (experimental)</p>"},{"location":"cli/commands/#configuration-commands","title":"Configuration Commands","text":""},{"location":"cli/commands/#vllm-eval-config","title":"<code>vllm-eval config</code>","text":"<p>Manage configuration profiles and settings.</p>"},{"location":"cli/commands/#subcommands_1","title":"Subcommands","text":""},{"location":"cli/commands/#vllm-eval-config-show","title":"<code>vllm-eval config show</code>","text":"<p>Display current configuration.</p> <pre><code>vllm-eval config show [OPTIONS]\n</code></pre> <p>Options: - <code>--profile TEXT</code>: Show specific profile (default: current) - <code>--format TEXT</code>: Output format (yaml, json, table)</p>"},{"location":"cli/commands/#vllm-eval-config-create","title":"<code>vllm-eval config create</code>","text":"<p>Create a new configuration profile.</p> <pre><code>vllm-eval config create &lt;profile-name&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--from-profile TEXT</code>: Copy settings from existing profile - <code>--interactive</code>: Use interactive setup wizard</p>"},{"location":"cli/commands/#vllm-eval-config-validate","title":"<code>vllm-eval config validate</code>","text":"<p>Validate configuration syntax and connectivity.</p> <pre><code>vllm-eval config validate [OPTIONS]\n</code></pre> <p>Options: - <code>--profile TEXT</code>: Validate specific profile - <code>--test-endpoints</code>: Test endpoint connectivity</p>"},{"location":"cli/commands/#system-commands","title":"System Commands","text":""},{"location":"cli/commands/#vllm-eval-doctor","title":"<code>vllm-eval doctor</code>","text":"<p>Comprehensive system diagnostics and health checks.</p> <pre><code>vllm-eval doctor [OPTIONS]\n</code></pre> <p>Options: - <code>--verbose</code>: Show detailed diagnostic information - <code>--fix</code>: Attempt to fix common issues automatically</p>"},{"location":"cli/commands/#vllm-eval-setup","title":"<code>vllm-eval setup</code>","text":"<p>Interactive setup wizard for initial configuration.</p> <pre><code>vllm-eval setup [OPTIONS]\n</code></pre> <p>Options: - <code>--no-interactive</code>: Use default values without prompts - <code>--force</code>: Overwrite existing configuration</p>"},{"location":"cli/commands/#results-commands","title":"Results Commands","text":""},{"location":"cli/commands/#vllm-eval-results","title":"<code>vllm-eval results</code>","text":"<p>Manage and analyze evaluation results.</p>"},{"location":"cli/commands/#subcommands_2","title":"Subcommands","text":""},{"location":"cli/commands/#vllm-eval-results-list","title":"<code>vllm-eval results list</code>","text":"<p>List available evaluation results.</p> <pre><code>vllm-eval results list [OPTIONS]\n</code></pre>"},{"location":"cli/commands/#vllm-eval-results-show","title":"<code>vllm-eval results show</code>","text":"<p>Display detailed results for a specific run.</p> <pre><code>vllm-eval results show &lt;run-id&gt; [OPTIONS]\n</code></pre>"},{"location":"cli/commands/#vllm-eval-results-export","title":"<code>vllm-eval results export</code>","text":"<p>Export results in various formats.</p> <pre><code>vllm-eval results export &lt;run-id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--format TEXT</code>: Export format (json, csv, xlsx) - <code>--output PATH</code>: Output file path</p>"},{"location":"cli/commands/#global-options","title":"Global Options","text":"<p>All commands support these global options:</p> <ul> <li><code>--help</code>: Show help message and exit</li> <li><code>--version</code>: Show version information</li> <li><code>--verbose</code>: Enable verbose output</li> <li><code>--quiet</code>: Suppress non-error output</li> <li><code>--config-file PATH</code>: Use specific configuration file</li> </ul>"},{"location":"cli/commands/#examples","title":"Examples","text":""},{"location":"cli/commands/#common-workflows","title":"Common Workflows","text":"<pre><code># Initial setup\nvllm-eval setup\n\n# Quick evaluation with defaults\nvllm-eval run evalchemy my-model\n\n# Production evaluation with full configuration\nvllm-eval run all my-model --config production --output-dir ./prod-results\n\n# Troubleshooting\nvllm-eval doctor --verbose\nvllm-eval config validate --test-endpoints\n</code></pre> <p>For more detailed information, see: - Configuration Guide - Troubleshooting</p>"},{"location":"cli/configuration/","title":"Configuration Guide","text":"<p>The VLLM Evaluation CLI uses TOML-based configuration files to manage settings across different environments and evaluation scenarios.</p>"},{"location":"cli/configuration/#configuration-file-location","title":"Configuration File Location","text":"<p>Default configuration locations (in order of precedence):</p> <ol> <li>Path specified with <code>--config-file</code> option</li> <li><code>./vllm-eval.toml</code> (current directory)</li> <li><code>~/.config/vllm-eval/config.toml</code> (user config)</li> <li><code>/etc/vllm-eval/config.toml</code> (system config)</li> </ol>"},{"location":"cli/configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"cli/configuration/#example-configuration","title":"Example Configuration","text":"<pre><code># Global settings\nprofile_name = \"default\"\ndescription = \"Default VLLM evaluation configuration\"\nlog_level = \"INFO\"\n\n[model]\nname = \"my-model\"\nendpoint = \"http://localhost:8000/v1/completions\"\nmax_tokens = 2048\nbatch_size = 1\n\n[system]\nresults_dir = \"./results\"\nlogs_dir = \"./logs\"\ntimeout_default = 3600\n\n[evaluation]\n# Framework configurations\n[evaluation.evalchemy]\nenabled = true\ntasks = [\"mmlu\", \"arc_easy\", \"hellaswag\"]\n\n[evaluation.nvidia_eval]\nenabled = true\nbenchmark = \"aime\"\ngpus = 1\n\n[evaluation.vllm_benchmark]\nenabled = true\nscenario = \"performance\"\nconcurrency = 10\n\n[evaluation.deepeval]\nenabled = true\nsuite = \"default\"\nmetrics = [\"precision\", \"recall\"]\n</code></pre>"},{"location":"cli/configuration/#profile-management","title":"Profile Management","text":""},{"location":"cli/configuration/#creating-profiles","title":"Creating Profiles","text":"<pre><code># Create new profile\nvllm-eval config create production\n\n# Create from existing profile\nvllm-eval config create testing --from-profile default\n</code></pre>"},{"location":"cli/configuration/#using-profiles","title":"Using Profiles","text":"<pre><code># Use profile for single command\nvllm-eval --profile production run evalchemy my-model\n\n# Set default profile\nvllm-eval config set-default production\n</code></pre>"},{"location":"cli/configuration/#framework-configuration","title":"Framework Configuration","text":""},{"location":"cli/configuration/#evalchemy","title":"Evalchemy","text":"<pre><code>[evaluation.evalchemy]\nenabled = true\nendpoint = \"http://localhost:8000/v1/completions\"\nbatch_size = 4\ntimeout = 3600\ntasks = [\"mmlu\", \"arc_easy\", \"arc_challenge\", \"hellaswag\"]\n</code></pre>"},{"location":"cli/configuration/#nvidia-eval","title":"NVIDIA Eval","text":"<pre><code>[evaluation.nvidia_eval]\nenabled = true\nbenchmark = \"aime\"  # or \"livecodebench\"\ngpus = 2\nout_seq_len = 4096\n</code></pre>"},{"location":"cli/configuration/#vllm-benchmark","title":"VLLM Benchmark","text":"<pre><code>[evaluation.vllm_benchmark]\nenabled = true\nscenario = \"performance\"\nconcurrency = 20\nduration = 600\n</code></pre>"},{"location":"cli/configuration/#deepeval","title":"Deepeval","text":"<pre><code>[evaluation.deepeval]\nenabled = true\nsuite = \"rag\"\nmetrics = [\"faithfulness\", \"answer_relevancy\"]\n</code></pre>"},{"location":"cli/configuration/#environment-variables","title":"Environment Variables","text":"<p>Configuration supports environment variable substitution:</p> <pre><code>[model]\nendpoint = \"${VLLM_ENDPOINT:-http://localhost:8000/v1}\"\napi_key = \"${API_KEY}\"\n\n[system]\nresults_dir = \"${RESULTS_DIR:-./results}\"\n</code></pre>"},{"location":"cli/configuration/#validation","title":"Validation","text":"<pre><code># Validate current configuration\nvllm-eval config validate\n\n# Test endpoint connectivity\nvllm-eval config validate --test-endpoints\n</code></pre> <p>For more information, see: - Commands Reference - Troubleshooting</p>"},{"location":"cli/overview/","title":"CLI Overview","text":"<p>The VLLM Evaluation CLI (<code>vllm-eval</code>) is a unified command-line interface for running model evaluations across multiple frameworks. It provides a streamlined way to execute benchmarks, manage configurations, and analyze results without dealing with framework-specific setup complexities.</p> <p>Key Features</p> <ul> <li>\ud83d\ude80 Unified Interface: Single CLI for all evaluation frameworks</li> <li>\ud83d\udd27 Configuration Management: TOML-based profiles and settings</li> <li>\ud83d\udcca Rich Output: Beautiful terminal displays with progress indicators</li> <li>\ud83e\uddea Multiple Frameworks: Support for Evalchemy, NVIDIA Eval, VLLM Benchmark, and Deepeval</li> <li>\ud83d\udd0d System Diagnostics: Built-in health checks and troubleshooting</li> <li>\ud83d\udccb Results Management: Standardized result formats and comparison tools</li> </ul>"},{"location":"cli/overview/#supported-evaluation-frameworks","title":"Supported Evaluation Frameworks","text":"<p>The CLI integrates with multiple evaluation frameworks through a unified adapter pattern:</p>"},{"location":"cli/overview/#evalchemy","title":"Evalchemy","text":"<ul> <li>Purpose: Comprehensive benchmark suite (MMLU, HumanEval, ARC, HellaSwag)</li> <li>Specialization: Academic benchmarks and standard evaluations</li> <li>Output: Detailed accuracy metrics and performance analysis</li> </ul>"},{"location":"cli/overview/#nvidia-eval","title":"NVIDIA Eval","text":"<ul> <li>Purpose: Mathematical reasoning and coding benchmarks</li> <li>Specialization: AIME 2024 and LiveCodeBench evaluations</li> <li>Output: Problem-solving accuracy and reasoning capabilities</li> </ul>"},{"location":"cli/overview/#vllm-benchmark","title":"VLLM Benchmark","text":"<ul> <li>Purpose: Performance and throughput testing</li> <li>Specialization: TTFT (Time to First Token), TPOT (Time Per Output Token), throughput</li> <li>Output: Performance metrics and latency analysis</li> </ul>"},{"location":"cli/overview/#deepeval","title":"Deepeval","text":"<ul> <li>Purpose: Custom metrics and RAG evaluation</li> <li>Specialization: Context relevance, faithfulness, answer relevancy</li> <li>Output: Quality assessment for retrieval-augmented generation</li> </ul>"},{"location":"cli/overview/#quick-start","title":"Quick Start","text":""},{"location":"cli/overview/#installation","title":"Installation","text":"<pre><code># Install from project directory\ncd /path/to/vllm-eval-public\npip install -e .\n</code></pre>"},{"location":"cli/overview/#setup","title":"Setup","text":"<pre><code># Run interactive setup wizard\nvllm-eval setup\n\n# Check installation and system status\nvllm-eval doctor\n\n# View available commands\nvllm-eval --help\n</code></pre>"},{"location":"cli/overview/#basic-usage","title":"Basic Usage","text":"<pre><code># Run Evalchemy evaluation\nvllm-eval run evalchemy my-model --endpoint http://localhost:8000/v1\n\n# Run all enabled frameworks\nvllm-eval run all my-model\n\n# Run with custom configuration profile\nvllm-eval run evalchemy my-model --config production --batch-size 4\n\n# Dry run to preview execution\nvllm-eval run evalchemy my-model --dry-run\n</code></pre>"},{"location":"cli/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Commands Reference - Detailed command documentation</li> <li>Configuration Guide - Setup and profile management  </li> <li>Troubleshooting - Common issues and solutions</li> <li>API Documentation - Framework-specific details</li> </ul>"},{"location":"cli/troubleshooting/","title":"Troubleshooting","text":"<p>This guide covers common issues and solutions for the VLLM Evaluation CLI.</p>"},{"location":"cli/troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":""},{"location":"cli/troubleshooting/#system-health-check","title":"System Health Check","text":"<p>Start with the built-in diagnostic tool:</p> <pre><code># Run comprehensive system diagnostics\nvllm-eval doctor\n\n# Verbose diagnostic output\nvllm-eval doctor --verbose\n\n# Check system status\nvllm-eval system status\n</code></pre>"},{"location":"cli/troubleshooting/#configuration-validation","title":"Configuration Validation","text":"<pre><code># Validate current configuration\nvllm-eval config validate\n\n# Test endpoint connectivity\nvllm-eval config validate --test-endpoints\n</code></pre>"},{"location":"cli/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"cli/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"cli/troubleshooting/#issue-vllm-eval-command-not-found","title":"Issue: <code>vllm-eval: command not found</code>","text":"<p>Solutions:</p> <ol> <li> <p>Install in development mode: <pre><code>cd /path/to/vllm-eval-public\npip install -e .\n</code></pre></p> </li> <li> <p>Check installation: <pre><code>pip list | grep vllm-eval\nwhich vllm-eval\n</code></pre></p> </li> </ol>"},{"location":"cli/troubleshooting/#issue-configuration-file-not-found","title":"Issue: Configuration file not found","text":"<p>Solutions:</p> <ol> <li> <p>Run setup wizard: <pre><code>vllm-eval setup\n</code></pre></p> </li> <li> <p>Create minimal configuration: <pre><code>mkdir -p ~/.config/vllm-eval\ncat &gt; ~/.config/vllm-eval/config.toml &lt;&lt; EOF\nprofile_name = \"default\"\n\n[model]\nname = \"my-model\"\nendpoint = \"http://localhost:8000/v1/completions\"\n\n[evaluation.evalchemy]\nenabled = true\nEOF\n</code></pre></p> </li> </ol>"},{"location":"cli/troubleshooting/#connectivity-issues","title":"Connectivity Issues","text":""},{"location":"cli/troubleshooting/#issue-connection-refused-to-model-endpoint","title":"Issue: Connection refused to model endpoint","text":"<p>Solutions:</p> <ol> <li> <p>Check if server is running: <pre><code>curl http://localhost:8000/health\n</code></pre></p> </li> <li> <p>Start VLLM server: <pre><code>vllm serve my-model --port 8000\n</code></pre></p> </li> <li> <p>Update endpoint in config: <pre><code>[model]\nendpoint = \"http://correct-host:8000/v1/completions\"\n</code></pre></p> </li> </ol>"},{"location":"cli/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"cli/troubleshooting/#issue-slow-evaluation-execution","title":"Issue: Slow evaluation execution","text":"<p>Solutions:</p> <ol> <li> <p>Increase batch size: <pre><code>[evaluation.evalchemy]\nbatch_size = 8\n</code></pre></p> </li> <li> <p>Use parallel execution: <pre><code>vllm-eval run all my-model --parallel\n</code></pre></p> </li> </ol>"},{"location":"cli/troubleshooting/#issue-out-of-memory-errors","title":"Issue: Out of memory errors","text":"<p>Solutions:</p> <ol> <li> <p>Reduce batch size: <pre><code>[evaluation.evalchemy]\nbatch_size = 1\n</code></pre></p> </li> <li> <p>Limit sequence length: <pre><code>[model]\nmax_tokens = 1024\n</code></pre></p> </li> </ol>"},{"location":"cli/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"cli/troubleshooting/#built-in-help","title":"Built-in Help","text":"<pre><code># General help\nvllm-eval --help\n\n# Command-specific help\nvllm-eval run --help\nvllm-eval run evalchemy --help\n</code></pre>"},{"location":"cli/troubleshooting/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug output\nvllm-eval --verbose run evalchemy my-model\n\n# Full debug mode\nvllm-eval --debug run evalchemy my-model\n</code></pre>"},{"location":"cli/troubleshooting/#log-files","title":"Log Files","text":"<ul> <li>System logs: <code>~/.config/vllm-eval/logs/</code></li> <li>Evaluation logs: <code>./logs/</code> (or configured logs directory)</li> </ul> <pre><code># View recent errors\ntail -f ~/.config/vllm-eval/logs/evaluation.log\n</code></pre>"},{"location":"cli/troubleshooting/#error-codes","title":"Error Codes","text":"Code Description Solution <code>CONFIG_001</code> Configuration file not found Run <code>vllm-eval setup</code> <code>CONN_001</code> Connection refused Start model server <code>AUTH_001</code> Authentication failed Set correct API key <code>GPU_001</code> CUDA out of memory Reduce batch size <p>For more information, see: - Commands Reference - Configuration Guide</p>"},{"location":"developer/development-setup/","title":"Development Setup","text":"<p>Comprehensive guide for setting up a development environment for the VLLM Evaluation System, including CLI development, framework integration, and contribution workflows.</p> <p>Development Environment</p> <p>This guide covers:</p> <ul> <li>\ud83d\udcbb Local Development: CLI and framework development</li> <li>\ud83d\udd27 Tool Setup: Required and optional development tools  </li> <li>\ud83e\uddea Testing: Unit tests, integration tests, and validation</li> <li>\ud83d\ude80 Contribution: Code standards and submission process</li> </ul>"},{"location":"developer/development-setup/#prerequisites","title":"Prerequisites","text":""},{"location":"developer/development-setup/#required-tools","title":"Required Tools","text":"<ul> <li>Python 3.11+ with pip</li> <li>Docker Desktop or OrbStack (for macOS)</li> <li>Git with GitHub access</li> <li>Make (usually pre-installed on macOS/Linux)</li> </ul>"},{"location":"developer/development-setup/#optional-tools-recommended","title":"Optional Tools (Recommended)","text":"<ul> <li>kubectl (for Kubernetes development)</li> <li>Helm 3+ (for chart development)</li> <li>pre-commit (for code quality)</li> <li>ruff and mypy (for linting and type checking)</li> <li>BuildKit (for advanced Docker builds)</li> </ul>"},{"location":"developer/development-setup/#quick-setup-5-minutes","title":"\ud83d\ude80 Quick Setup (5 Minutes)","text":""},{"location":"developer/development-setup/#1-repository-setup","title":"1. Repository Setup","text":"<pre><code># Clone and navigate\ngit clone https://github.com/thakicloud/vllm-eval-public.git\ncd vllm-eval-public\n\n# Setup development environment\nmake dev-setup\n\n# Install CLI in development mode\npip install -e .\n\n# Verify installation\nvllm-eval --help\nvllm-eval doctor\n</code></pre>"},{"location":"developer/development-setup/#2-development-tools-setup","title":"2. Development Tools Setup","text":"<pre><code># Install development dependencies\npip install -r requirements-dev.txt\n\n# Setup pre-commit hooks (optional but recommended)\npre-commit install\n\n# Verify tools\nruff --version\nmypy --version\n</code></pre>"},{"location":"developer/development-setup/#3-test-your-setup","title":"3. Test Your Setup","text":"<pre><code># Run basic tests\nmake run-tests\n\n# Test CLI functionality\nvllm-eval setup --no-interactive\nvllm-eval config validate\n</code></pre>"},{"location":"developer/development-setup/#cli-development-workflow","title":"\ud83d\udcbb CLI Development Workflow","text":""},{"location":"developer/development-setup/#project-structure","title":"Project Structure","text":"<pre><code>vllm_eval_cli/\n\u251c\u2500\u2500 __init__.py              # Package initialization\n\u251c\u2500\u2500 main.py                 # CLI entry point (Typer app)\n\u251c\u2500\u2500 commands/\n\u2502   \u251c\u2500\u2500 run.py              # Run command implementation\n\u2502   \u251c\u2500\u2500 config.py           # Configuration management\n\u2502   \u251c\u2500\u2500 system.py           # System diagnostics\n\u2502   \u251c\u2500\u2500 results.py          # Results management\n\u2502   \u2514\u2500\u2500 setup.py            # Setup wizard\n\u251c\u2500\u2500 adapters/\n\u2502   \u251c\u2500\u2500 base.py             # Base adapter interface\n\u2502   \u251c\u2500\u2500 evalchemy.py        # Evalchemy integration\n\u2502   \u251c\u2500\u2500 nvidia.py           # NVIDIA Eval integration\n\u2502   \u251c\u2500\u2500 vllm_benchmark.py   # VLLM Benchmark integration\n\u2502   \u2514\u2500\u2500 deepeval.py         # Deepeval integration\n\u251c\u2500\u2500 core/\n\u2502   \u2514\u2500\u2500 config.py           # Configuration system\n\u251c\u2500\u2500 ui/\n\u2502   \u2514\u2500\u2500 console.py          # Rich console utilities\n\u2514\u2500\u2500 utils/\n    \u2514\u2500\u2500 __init__.py         # Utility functions\n</code></pre>"},{"location":"developer/development-setup/#development-patterns","title":"Development Patterns","text":""},{"location":"developer/development-setup/#1-adding-new-commands","title":"1. Adding New Commands","text":"<pre><code># In vllm_eval_cli/commands/new_command.py\nimport typer\nfrom rich.console import Console\n\napp = typer.Typer()\nconsole = Console()\n\n@app.command()\ndef my_new_command(\n    param: str = typer.Argument(..., help=\"Description\"),\n    option: bool = typer.Option(False, \"--flag\", help=\"Optional flag\"),\n) -&gt; None:\n    \"\"\"Description of the new command\"\"\"\n    console.print(f\"[green]Executing new command with {param}[/green]\")\n    # Implementation here\n\n# Register in main.py\nfrom vllm_eval_cli.commands import new_command\napp.add_typer(new_command.app, name=\"new-command\")\n</code></pre>"},{"location":"developer/development-setup/#2-creating-new-adapters","title":"2. Creating New Adapters","text":"<pre><code># In vllm_eval_cli/adapters/my_framework.py\nfrom typing import Dict, Any, List\nfrom .base import BaseEvaluationAdapter\n\nclass MyFrameworkAdapter(BaseEvaluationAdapter):\n    \"\"\"Adapter for MyFramework evaluation system\"\"\"\n\n    def validate_prerequisites(self) -&gt; bool:\n        \"\"\"Validate that MyFramework is available\"\"\"\n        # Check dependencies, endpoints, etc.\n        return True\n\n    def prepare_execution(self, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Prepare execution parameters\"\"\"\n        return {\n            \"endpoint\": kwargs.get(\"endpoint\"),\n            \"model_name\": kwargs.get(\"model_name\"),\n            # Framework-specific params\n        }\n\n    def execute_evaluation(self, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Execute the actual evaluation\"\"\"\n        # Implementation here\n        return {\"status\": \"success\", \"results\": {}}\n\n    def parse_results(self, raw_results: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Parse and standardize results\"\"\"\n        return {\n            \"framework\": \"my_framework\",\n            \"metrics\": {},\n            \"metadata\": {}\n        }\n</code></pre>"},{"location":"developer/development-setup/#3-configuration-management","title":"3. Configuration Management","text":"<pre><code># Working with configuration system\nfrom vllm_eval_cli.core.config import ConfigManager\n\n# Load configuration\nconfig_manager = ConfigManager(profile=\"development\")\nconfig = config_manager.get_config()\n\n# Access framework-specific settings\nevalchemy_config = config.get(\"evaluation\", {}).get(\"evalchemy\", {})\nendpoint = evalchemy_config.get(\"endpoint\")\n\n# Validate configuration\nif config_manager.validate_config():\n    console.print(\"[green]Configuration valid[/green]\")\n</code></pre>"},{"location":"developer/development-setup/#development-environment-details","title":"\ud83d\udd27 Development Environment Details","text":""},{"location":"developer/development-setup/#python-environment-setup","title":"Python Environment Setup","text":"<pre><code># Create virtual environment (optional but recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -r requirements-dev.txt\npip install -r requirements-test.txt\n\n# Install CLI in editable mode\npip install -e .\n</code></pre>"},{"location":"developer/development-setup/#docker-development","title":"Docker Development","text":"<pre><code># Build development images\nmake build-images\n\n# Test specific framework containers\ndocker build -f docker/evalchemy.Dockerfile -t evalchemy:dev .\ndocker run --rm evalchemy:dev --help\n\n# Local registry for testing\nmake push-images  # Pushes to local registry\n</code></pre>"},{"location":"developer/development-setup/#kubernetes-development","title":"Kubernetes Development","text":"<pre><code># Setup local cluster\nmake kind-deploy\n\n# Install development charts\nhelm install clickhouse charts/clickhouse --values charts/clickhouse/values.yaml\nhelm install grafana charts/grafana --values charts/grafana/values.yaml\n\n# Test Kubernetes jobs\nkubectl apply -f k8s/evalchemy-job.yaml\nkubectl logs -f job/evalchemy-evaluation\n</code></pre>"},{"location":"developer/development-setup/#testing-and-validation","title":"\ud83e\uddea Testing and Validation","text":""},{"location":"developer/development-setup/#testing-hierarchy","title":"Testing Hierarchy","text":"<ol> <li>Unit Tests: Individual component testing</li> <li>Integration Tests: Adapter and system integration</li> <li>End-to-End Tests: Complete evaluation workflows</li> <li>Performance Tests: Load and benchmark testing</li> </ol>"},{"location":"developer/development-setup/#running-tests","title":"Running Tests","text":"<pre><code># All tests\nmake run-tests\n\n# Specific test categories\npytest tests/unit/          # Unit tests\npytest tests/integration/   # Integration tests\npytest tests/e2e/          # End-to-end tests\n\n# CLI-specific tests\npytest tests/cli/           # CLI command tests\npytest tests/adapters/      # Adapter tests\n\n# With coverage\npytest --cov=vllm_eval_cli --cov-report=html\n</code></pre>"},{"location":"developer/development-setup/#testing-cli-commands","title":"Testing CLI Commands","text":"<pre><code># Example CLI test\nfrom typer.testing import CliRunner\nfrom vllm_eval_cli.main import app\n\ndef test_cli_version():\n    runner = CliRunner()\n    result = runner.invoke(app, [\"version\"])\n    assert result.exit_code == 0\n    assert \"VLLM Evaluation CLI\" in result.stdout\n\ndef test_cli_config_show():\n    runner = CliRunner()\n    result = runner.invoke(app, [\"config\", \"show\"])\n    # Test configuration display\n</code></pre>"},{"location":"developer/development-setup/#testing-adapters","title":"Testing Adapters","text":"<pre><code># Example adapter test\nimport pytest\nfrom vllm_eval_cli.adapters.evalchemy import EvAlchemyAdapter\n\ndef test_evalchemy_adapter_validation():\n    adapter = EvAlchemyAdapter()\n    # Test with mock configuration\n    assert adapter.validate_prerequisites()\n\n@pytest.mark.integration\ndef test_evalchemy_adapter_execution():\n    # Integration test with actual endpoint\n    pass\n</code></pre>"},{"location":"developer/development-setup/#debugging-and-profiling","title":"\ud83d\udd0d Debugging and Profiling","text":""},{"location":"developer/development-setup/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging\nexport VLLM_EVAL_DEBUG=1\nvllm-eval --debug run evalchemy my-model\n\n# Component-specific debugging\nexport VLLM_EVAL_DEBUG_ADAPTERS=1\nexport VLLM_EVAL_DEBUG_CONFIG=1\n</code></pre>"},{"location":"developer/development-setup/#profiling-performance","title":"Profiling Performance","text":"<pre><code># Profile CLI execution\nimport cProfile\nimport pstats\n\ndef profile_command():\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    # Run CLI command\n\n    profiler.disable()\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    stats.print_stats()\n</code></pre>"},{"location":"developer/development-setup/#memory-monitoring","title":"Memory Monitoring","text":"<pre><code># Monitor memory usage during evaluation\nmemory_profiler vllm-eval run evalchemy my-model\n\n# Check for memory leaks\nvalgrind --tool=memcheck python -m vllm_eval_cli.main\n</code></pre>"},{"location":"developer/development-setup/#build-and-distribution","title":"\ud83d\udce6 Build and Distribution","text":""},{"location":"developer/development-setup/#package-building","title":"Package Building","text":"<pre><code># Build source distribution\npython -m build --sdist\n\n# Build wheel\npython -m build --wheel\n\n# Check package\ntwine check dist/*\n</code></pre>"},{"location":"developer/development-setup/#container-building","title":"Container Building","text":"<pre><code># Build all images\nmake build-images\n\n# Build specific framework\ndocker build -f docker/evalchemy.Dockerfile -t evalchemy:latest .\n\n# Multi-platform builds\ndocker buildx build --platform linux/amd64,linux/arm64 .\n</code></pre>"},{"location":"developer/development-setup/#release-process","title":"Release Process","text":"<pre><code># Tag release\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\ngit push origin v1.0.0\n\n# Build and upload to PyPI (maintainers only)\ntwine upload dist/*\n</code></pre>"},{"location":"developer/development-setup/#code-quality-and-standards","title":"\u2699\ufe0f Code Quality and Standards","text":""},{"location":"developer/development-setup/#code-formatting","title":"Code Formatting","text":"<pre><code># Format code with ruff\nruff format vllm_eval_cli/\n\n# Check formatting\nruff check vllm_eval_cli/\n\n# Fix auto-fixable issues\nruff check --fix vllm_eval_cli/\n</code></pre>"},{"location":"developer/development-setup/#type-checking","title":"Type Checking","text":"<pre><code># Run mypy type checking\nmypy vllm_eval_cli/\n\n# Check specific files\nmypy vllm_eval_cli/main.py\n</code></pre>"},{"location":"developer/development-setup/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># Install hooks\npre-commit install\n\n# Run hooks manually\npre-commit run --all-files\n\n# Update hooks\npre-commit autoupdate\n</code></pre>"},{"location":"developer/development-setup/#commit-message-format","title":"Commit Message Format","text":"<pre><code>\ud83d\ude80 feat(cli): add new evaluation command\n\u2699\ufe0f config(setup): update dependency requirements  \n\ud83d\udc1b fix(adapters): resolve connection timeout issue\n\ud83d\udcda docs(api): update framework integration guide\n\u2728 improve(performance): optimize batch processing\n\ud83e\uddea test(integration): add end-to-end workflow tests\n</code></pre>"},{"location":"developer/development-setup/#contribution-workflow","title":"\ud83d\ude80 Contribution Workflow","text":""},{"location":"developer/development-setup/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork repository on GitHub, then:\ngit clone https://github.com/YOUR_USERNAME/vllm-eval-public.git\ncd vllm-eval-public\ngit remote add upstream https://github.com/thakicloud/vllm-eval-public.git\n</code></pre>"},{"location":"developer/development-setup/#2-create-feature-branch","title":"2. Create Feature Branch","text":"<pre><code># Create and switch to feature branch\ngit checkout -b feature/new-adapter\n\n# Keep up to date with main\ngit fetch upstream\ngit rebase upstream/main\n</code></pre>"},{"location":"developer/development-setup/#3-development-and-testing","title":"3. Development and Testing","text":"<pre><code># Make changes and test thoroughly\nmake dev-setup\nvllm-eval doctor\nmake run-tests\n\n# Test your specific changes\npytest tests/adapters/test_new_adapter.py -v\n</code></pre>"},{"location":"developer/development-setup/#4-submit-pull-request","title":"4. Submit Pull Request","text":"<pre><code># Commit changes\ngit add .\ngit commit -m \"\ud83d\ude80 feat(adapters): add new framework adapter\"\n\n# Push and create PR\ngit push origin feature/new-adapter\n# Create PR on GitHub\n</code></pre>"},{"location":"developer/development-setup/#troubleshooting-development-issues","title":"\ud83d\udee0\ufe0f Troubleshooting Development Issues","text":""},{"location":"developer/development-setup/#common-issues","title":"Common Issues","text":""},{"location":"developer/development-setup/#1-import-errors","title":"1. Import Errors","text":"<pre><code># Ensure CLI is installed in development mode\npip install -e .\n\n# Check Python path\npython -c \"import sys; print('\\n'.join(sys.path))\"\n\n# Verify package installation\npip list | grep vllm-eval\n</code></pre>"},{"location":"developer/development-setup/#2-configuration-problems","title":"2. Configuration Problems","text":"<pre><code># Reset configuration\nrm -rf ~/.config/vllm-eval/\nvllm-eval setup --force\n\n# Validate configuration\nvllm-eval config validate --test-endpoints\n</code></pre>"},{"location":"developer/development-setup/#3-dependency-conflicts","title":"3. Dependency Conflicts","text":"<pre><code># Clean environment\npip uninstall vllm-eval-cli -y\npip install -e . --force-reinstall\n\n# Check for conflicts\npip check\n</code></pre>"},{"location":"developer/development-setup/#4-docker-issues","title":"4. Docker Issues","text":"<pre><code># Reset Docker state\ndocker system prune -f\nmake build-images\n\n# Check Docker daemon\ndocker info\n</code></pre>"},{"location":"developer/development-setup/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check CLI Guide and API Reference</li> <li>Issues: Search existing GitHub issues</li> <li>Debugging: Use <code>vllm-eval doctor --verbose</code></li> <li>Community: Engage in project discussions</li> </ul>"},{"location":"developer/development-setup/#whats-next","title":"\ud83d\udd17 What's Next?","text":""},{"location":"developer/development-setup/#explore-development-areas","title":"Explore Development Areas","text":"<ul> <li>Testing Guide - Comprehensive testing strategies</li> <li>CLI Commands - Understanding CLI architecture</li> <li>API Integration - Framework-specific development</li> <li>Architecture Overview - System design</li> </ul>"},{"location":"developer/development-setup/#advanced-development","title":"Advanced Development","text":"<ul> <li>Custom Adapters: Integrate new evaluation frameworks</li> <li>Performance Optimization: Improve evaluation speed and accuracy</li> <li>Kubernetes Integration: Enhance orchestration capabilities</li> <li>Monitoring and Observability: Add metrics and monitoring</li> </ul> <p>Development Environment Ready!</p> <p>You now have a complete development setup for the VLLM Evaluation System.</p> <p>Quick Test: <code>vllm-eval doctor &amp;&amp; make run-tests</code></p> <p>Start Developing: Choose an area to contribute and check existing issues for ideas!</p>"},{"location":"developer/testing-guide/","title":"Testing Guide","text":"<p>Comprehensive testing strategies for the VLLM Evaluation System, covering CLI components, framework adapters, and end-to-end workflows.</p> <p>Testing Philosophy</p> <ul> <li>\ud83e\uddea Test Pyramid: Unit \u2192 Integration \u2192 E2E testing approach</li> <li>\u2699\ufe0f CLI Testing: Command validation and user experience testing</li> <li>\ud83d\udd0c Adapter Testing: Framework integration and compatibility testing</li> <li>\ud83d\ude80 Automation: CI/CD integration and regression prevention</li> </ul>"},{"location":"developer/testing-guide/#testing-architecture","title":"Testing Architecture","text":""},{"location":"developer/testing-guide/#test-categories","title":"Test Categories","text":"<pre><code>Testing Pyramid:\n\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502      E2E Tests      \u2502  &lt;- Full workflows, real endpoints\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502  Integration Tests   \u2502  &lt;- Adapter + framework integration\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502      Unit Tests      \u2502  &lt;- CLI commands, config, utilities\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer/testing-guide/#unit-testing","title":"\ud83e\uddea Unit Testing","text":""},{"location":"developer/testing-guide/#cli-command-testing","title":"CLI Command Testing","text":"<pre><code># tests/unit/test_cli_commands.py\nfrom typer.testing import CliRunner\nfrom vllm_eval_cli.main import app\n\ndef test_version_command():\n    runner = CliRunner()\n    result = runner.invoke(app, [\"version\"])\n    assert result.exit_code == 0\n    assert \"VLLM Evaluation CLI\" in result.stdout\n\ndef test_config_show():\n    runner = CliRunner()\n    with mock_config():\n        result = runner.invoke(app, [\"config\", \"show\"])\n        assert result.exit_code == 0\n        assert \"profile_name\" in result.stdout\n\ndef test_run_dry_run():\n    runner = CliRunner()\n    result = runner.invoke(app, [\n        \"run\", \"evalchemy\", \"test-model\", \"--dry-run\"\n    ])\n    assert result.exit_code == 0\n    assert \"Would execute\" in result.stdout\n</code></pre>"},{"location":"developer/testing-guide/#configuration-testing","title":"Configuration Testing","text":"<pre><code># tests/unit/test_config.py\nfrom vllm_eval_cli.core.config import ConfigManager\n\ndef test_config_loading():\n    config_manager = ConfigManager()\n    config = config_manager.get_config()\n    assert \"profile_name\" in config\n\ndef test_config_validation():\n    config_manager = ConfigManager()\n    assert config_manager.validate_config()\n\ndef test_profile_switching():\n    config_manager = ConfigManager(profile=\"test\")\n    config = config_manager.get_config()\n    assert config[\"profile_name\"] == \"test\"\n</code></pre>"},{"location":"developer/testing-guide/#running-unit-tests","title":"Running Unit Tests","text":"<pre><code># All unit tests\npytest tests/unit/ -v\n\n# Specific test categories\npytest tests/unit/test_cli_commands.py -v\npytest tests/unit/test_config.py -v\n\n# With coverage\npytest tests/unit/ --cov=vllm_eval_cli --cov-report=html\n</code></pre>"},{"location":"developer/testing-guide/#integration-testing","title":"\ud83d\udd0c Integration Testing","text":""},{"location":"developer/testing-guide/#adapter-testing","title":"Adapter Testing","text":"<pre><code># tests/integration/test_adapters.py\nimport pytest\nfrom vllm_eval_cli.adapters.evalchemy import EvAlchemyAdapter\n\nclass TestEvAlchemyAdapter:\n    def setup_method(self):\n        self.adapter = EvAlchemyAdapter()\n\n    def test_prerequisite_validation(self):\n        assert self.adapter.validate_prerequisites()\n\n    @pytest.mark.integration\n    def test_execution_with_mock_server(self):\n        with mock_vllm_server():\n            result = self.adapter.execute_evaluation(\n                model_name=\"test-model\",\n                endpoint=\"http://localhost:8000/v1\",\n                tasks=[\"arc_easy\"],\n                limit=5\n            )\n            assert result[\"status\"] == \"success\"\n            assert \"metrics\" in result\n</code></pre>"},{"location":"developer/testing-guide/#framework-integration-testing","title":"Framework Integration Testing","text":"<pre><code># tests/integration/test_framework_integration.py\n@pytest.mark.integration\ndef test_evalchemy_integration():\n    \"\"\"Test actual Evalchemy execution with mock server\"\"\"\n    with MockVLLMServer() as server:\n        result = run_cli_command([\n            \"run\", \"evalchemy\", \"test-model\",\n            \"--endpoint\", server.url,\n            \"--tasks\", \"arc_easy\",\n            \"--limit\", \"5\"\n        ])\n        assert result.exit_code == 0\n        assert \"Evaluation completed\" in result.output\n\n@pytest.mark.integration\ndef test_multiple_frameworks():\n    \"\"\"Test running multiple frameworks\"\"\"\n    with MockVLLMServer() as server:\n        result = run_cli_command([\n            \"run\", \"all\", \"test-model\",\n            \"--frameworks\", \"evalchemy,deepeval\",\n            \"--endpoint\", server.url\n        ])\n        assert result.exit_code == 0\n</code></pre>"},{"location":"developer/testing-guide/#end-to-end-testing","title":"\ud83d\ude80 End-to-End Testing","text":""},{"location":"developer/testing-guide/#full-workflow-testing","title":"Full Workflow Testing","text":"<pre><code># tests/e2e/test_evaluation_workflows.py\n@pytest.mark.e2e\n@pytest.mark.slow\ndef test_complete_evaluation_workflow():\n    \"\"\"Test complete evaluation from setup to results\"\"\"\n    # Setup\n    with temporary_config() as config_dir:\n        # Initialize CLI\n        run_setup_wizard(config_dir)\n\n        # Run evaluation\n        result = run_evaluation(\n            framework=\"evalchemy\",\n            model=\"test-model\",\n            config_dir=config_dir\n        )\n\n        # Verify results\n        assert result.success\n        assert result.results_file.exists()\n\n        # Verify result format\n        results = load_results(result.results_file)\n        assert \"metadata\" in results\n        assert \"metrics\" in results\n</code></pre>"},{"location":"developer/testing-guide/#performance-testing","title":"Performance Testing","text":"<pre><code># tests/performance/test_performance.py\n@pytest.mark.performance\ndef test_evaluation_performance():\n    \"\"\"Test evaluation performance metrics\"\"\"\n    import time\n\n    start_time = time.time()\n\n    with MockVLLMServer() as server:\n        result = run_cli_command([\n            \"run\", \"evalchemy\", \"test-model\",\n            \"--endpoint\", server.url,\n            \"--tasks\", \"arc_easy\",\n            \"--batch-size\", \"4\"\n        ])\n\n    duration = time.time() - start_time\n\n    # Performance assertions\n    assert result.exit_code == 0\n    assert duration &lt; 300  # Should complete within 5 minutes\n\n    # Memory usage checks\n    assert server.peak_memory_mb &lt; 1000\n</code></pre>"},{"location":"developer/testing-guide/#test-infrastructure","title":"\ud83d\udee0\ufe0f Test Infrastructure","text":""},{"location":"developer/testing-guide/#mock-server-setup","title":"Mock Server Setup","text":"<pre><code># tests/utils/mock_server.py\nimport threading\nimport time\nfrom flask import Flask, request, jsonify\n\nclass MockVLLMServer:\n    def __init__(self, port=8000):\n        self.port = port\n        self.app = Flask(__name__)\n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.app.route(\"/health\")\n        def health():\n            return {\"status\": \"ok\"}\n\n        @self.app.route(\"/v1/completions\", methods=[\"POST\"])\n        def completions():\n            data = request.json\n            return {\n                \"id\": \"mock-completion\",\n                \"object\": \"text_completion\",\n                \"choices\": [{\n                    \"text\": \"Mock response\",\n                    \"index\": 0,\n                    \"finish_reason\": \"length\"\n                }]\n            }\n\n    def __enter__(self):\n        self.thread = threading.Thread(\n            target=self.app.run,\n            kwargs={\"host\": \"localhost\", \"port\": self.port}\n        )\n        self.thread.start()\n        time.sleep(0.1)  # Wait for server to start\n        return self\n\n    def __exit__(self, *args):\n        # Cleanup server\n        pass\n</code></pre>"},{"location":"developer/testing-guide/#test-configuration-management","title":"Test Configuration Management","text":"<pre><code># tests/conftest.py\nimport pytest\nimport tempfile\nfrom pathlib import Path\n\n@pytest.fixture\ndef temp_config_dir():\n    \"\"\"Create temporary configuration directory\"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        yield Path(temp_dir)\n\n@pytest.fixture\ndef mock_config():\n    \"\"\"Provide mock configuration\"\"\"\n    return {\n        \"profile_name\": \"test\",\n        \"model\": {\n            \"name\": \"test-model\",\n            \"endpoint\": \"http://localhost:8000/v1\"\n        },\n        \"evaluation\": {\n            \"evalchemy\": {\"enabled\": True}\n        }\n    }\n</code></pre>"},{"location":"developer/testing-guide/#cicd-integration","title":"\ud83d\ude80 CI/CD Integration","text":""},{"location":"developer/testing-guide/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<pre><code># .github/workflows/test.yml\nname: Test Suite\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.11\", \"3.12\"]\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements-dev.txt\n        pip install -e .\n\n    - name: Run unit tests\n      run: pytest tests/unit/ -v --cov=vllm_eval_cli\n\n    - name: Run integration tests\n      run: pytest tests/integration/ -v -m integration\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n</code></pre>"},{"location":"developer/testing-guide/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n- repo: local\n  hooks:\n  - id: pytest-unit\n    name: Run unit tests\n    entry: pytest tests/unit/\n    language: python\n    pass_filenames: false\n    always_run: true\n\n  - id: pytest-cli\n    name: Run CLI tests\n    entry: pytest tests/unit/test_cli_commands.py -v\n    language: python\n    pass_filenames: false\n    files: ^vllm_eval_cli/\n</code></pre>"},{"location":"developer/testing-guide/#test-execution","title":"\u2699\ufe0f Test Execution","text":""},{"location":"developer/testing-guide/#running-test-suites","title":"Running Test Suites","text":"<pre><code># Quick unit tests\nmake test-unit\n\n# Integration tests (requires Docker)\nmake test-integration\n\n# Full test suite\nmake test-all\n\n# Performance tests\nmake test-performance\n\n# Specific framework tests\npytest tests/ -k \"evalchemy\" -v\npytest tests/ -k \"nvidia\" -v\n\n# Tests with markers\npytest tests/ -m \"unit\" -v\npytest tests/ -m \"integration\" -v\npytest tests/ -m \"e2e\" -v --slow\n</code></pre>"},{"location":"developer/testing-guide/#test-configuration","title":"Test Configuration","text":"<pre><code># pytest.ini\n[tool:pytest]\nmarkers =\n    unit: Unit tests\n    integration: Integration tests\n    e2e: End-to-end tests\n    slow: Slow tests\n    performance: Performance tests\n\naddopts = -v --strict-markers --tb=short\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\n</code></pre>"},{"location":"developer/testing-guide/#quality-metrics","title":"\ud83d\udcca Quality Metrics","text":""},{"location":"developer/testing-guide/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Unit Tests: \u2265 90% code coverage</li> <li>Integration Tests: \u2265 80% adapter coverage</li> <li>E2E Tests: \u2265 70% workflow coverage</li> </ul>"},{"location":"developer/testing-guide/#test-performance-targets","title":"Test Performance Targets","text":"<ul> <li>Unit Tests: &lt; 30 seconds total</li> <li>Integration Tests: &lt; 5 minutes total</li> <li>E2E Tests: &lt; 30 minutes total</li> </ul>"},{"location":"developer/testing-guide/#quality-gates","title":"Quality Gates","text":"<pre><code># Coverage check\npytest --cov=vllm_eval_cli --cov-fail-under=90\n\n# Performance check\npytest tests/performance/ --benchmark-only\n\n# Security check\nbandit -r vllm_eval_cli/\n</code></pre>"},{"location":"developer/testing-guide/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"developer/testing-guide/#test-organization","title":"Test Organization","text":"<ol> <li>Mirror source structure in test directories</li> <li>Use descriptive test names that explain intent</li> <li>Group related tests in classes</li> <li>Use fixtures for common setup</li> <li>Mock external dependencies appropriately</li> </ol>"},{"location":"developer/testing-guide/#test-writing-guidelines","title":"Test Writing Guidelines","text":"<pre><code># Good test example\ndef test_evalchemy_adapter_executes_with_valid_config():\n    \"\"\"Test that EvAlchemy adapter executes successfully with valid configuration\"\"\"\n    # Arrange\n    adapter = EvAlchemyAdapter()\n    config = {\"endpoint\": \"http://localhost:8000\", \"tasks\": [\"arc_easy\"]}\n\n    # Act\n    result = adapter.execute_evaluation(**config)\n\n    # Assert\n    assert result[\"status\"] == \"success\"\n    assert \"metrics\" in result\n    assert len(result[\"metrics\"]) &gt; 0\n</code></pre>"},{"location":"developer/testing-guide/#continuous-testing","title":"Continuous Testing","text":"<pre><code># Watch mode for development\npytest-watch -- tests/unit/\n\n# Automated testing on file changes\nfind . -name \"*.py\" | entr -c pytest tests/unit/\n</code></pre> <p>Testing Excellence</p> <p>Comprehensive testing ensures reliable CLI functionality and smooth user experience.</p> <p>Remember: Test early, test often, and maintain high coverage for critical components.</p>"},{"location":"operations/troubleshooting/","title":"Operations Troubleshooting","text":"<p>Comprehensive troubleshooting guide for operating the VLLM Evaluation System in production environments. Covers both CLI-based local operations and Kubernetes-native deployments with systematic diagnostic approaches.</p> <p>Operational Scope</p> <ul> <li>\ud83d\udd27 CLI Operations: Local and remote CLI execution issues</li> <li>\u2638\ufe0f Kubernetes Operations: Production cluster troubleshooting</li> <li>\ud83d\udcca Performance Issues: System performance and optimization</li> <li>\ud83d\uded1 Integration Problems: Cross-system connectivity and data flow</li> </ul>"},{"location":"operations/troubleshooting/#emergency-procedures","title":"\ud83d\ude91 Emergency Procedures","text":""},{"location":"operations/troubleshooting/#critical-system-failure","title":"Critical System Failure","text":"<pre><code># 1. Immediate Assessment\nvllm-eval doctor --verbose\nkubectl get pods -A --field-selector=status.phase!=Running\n\n# 2. Stop Problematic Processes\nvllm-eval system clean --force\nkubectl delete jobs --all -n vllm-eval\n\n# 3. System Recovery\nmake system-reset\nvllm-eval setup --force\n</code></pre>"},{"location":"operations/troubleshooting/#service-degradation","title":"Service Degradation","text":"<pre><code># Check system health\nvllm-eval system status\nkubectl top nodes\nkubectl top pods -n vllm-eval\n\n# Scale down if needed\nkubectl scale deployment --replicas=0 -n vllm-eval --all\n</code></pre>"},{"location":"operations/troubleshooting/#diagnostic-methodology","title":"\ud83d\udd0d Diagnostic Methodology","text":""},{"location":"operations/troubleshooting/#systematic-troubleshooting-approach","title":"Systematic Troubleshooting Approach","text":"<ol> <li>Identify Scope: CLI, Kubernetes, or integrated system issue</li> <li>Gather Information: Logs, metrics, system state</li> <li>Isolate Problem: Narrow down to specific component</li> <li>Test Hypothesis: Validate root cause theory</li> <li>Implement Fix: Apply solution with verification</li> <li>Document Resolution: Record solution for future reference</li> </ol>"},{"location":"operations/troubleshooting/#diagnostic-commands","title":"Diagnostic Commands","text":""},{"location":"operations/troubleshooting/#cli-diagnostics","title":"CLI Diagnostics","text":"<pre><code># System health check\nvllm-eval doctor --verbose\n\n# Configuration validation\nvllm-eval config validate --test-endpoints\n\n# System status\nvllm-eval system status --verbose\n\n# Recent logs\nvllm-eval system logs --tail 100\n\n# Performance metrics\nvllm-eval system metrics --last 24h\n</code></pre>"},{"location":"operations/troubleshooting/#kubernetes-diagnostics","title":"Kubernetes Diagnostics","text":"<pre><code># Cluster overview\nkubectl cluster-info\nkubectl get nodes -o wide\nkubectl get pods -A\n\n# Resource usage\nkubectl top nodes\nkubectl top pods -A\n\n# Event monitoring\nkubectl get events --sort-by='.lastTimestamp' -A\n\n# Workflow status\nargo list -n argo\nargo get &lt;workflow-name&gt; -n argo\n</code></pre>"},{"location":"operations/troubleshooting/#cli-operations-issues","title":"\ud83d\udcbb CLI Operations Issues","text":""},{"location":"operations/troubleshooting/#installation-and-setup-problems","title":"Installation and Setup Problems","text":""},{"location":"operations/troubleshooting/#issue-cli-command-not-found","title":"Issue: CLI Command Not Found","text":"<p>Symptoms: <pre><code>$ vllm-eval --help\nzsh: command not found: vllm-eval\n</code></pre></p> <p>Diagnosis: <pre><code># Check installation\npip list | grep vllm-eval\nwhich vllm-eval\necho $PATH\n</code></pre></p> <p>Solutions: <pre><code># Reinstall CLI\ncd /path/to/vllm-eval-public\npip uninstall vllm-eval -y\npip install -e .\n\n# Verify installation\nvllm-eval --version\nvllm-eval doctor\n</code></pre></p>"},{"location":"operations/troubleshooting/#issue-configuration-errors","title":"Issue: Configuration Errors","text":"<p>Symptoms: - <code>Configuration file not found</code> - <code>Invalid TOML syntax</code> - <code>Environment variable not resolved</code></p> <p>Diagnosis: <pre><code># Check configuration files\nls -la ~/.config/vllm-eval/\nvllm-eval config show --format json\nvllm-eval config validate --check-syntax\n</code></pre></p> <p>Solutions: <pre><code># Reset configuration\nrm -rf ~/.config/vllm-eval/\nvllm-eval setup --force\n\n# Fix TOML syntax\nvllm-eval config validate --check-syntax\nvllm-eval config edit  # Opens editor for manual fix\n\n# Set environment variables\nexport VLLM_ENDPOINT=\"http://localhost:8000/v1\"\nexport MODEL_NAME=\"my-model\"\n</code></pre></p>"},{"location":"operations/troubleshooting/#execution-and-performance-issues","title":"Execution and Performance Issues","text":""},{"location":"operations/troubleshooting/#issue-evaluation-timeouts","title":"Issue: Evaluation Timeouts","text":"<p>Symptoms: - Evaluations hanging indefinitely - Timeout errors after long waits - Partial results with incomplete frameworks</p> <p>Diagnosis: <pre><code># Check running processes\nps aux | grep vllm-eval\nvllm-eval system status\n\n# Monitor resource usage\ntop -p $(pgrep -f vllm-eval)\n\n# Check endpoint connectivity\ncurl -I http://localhost:8000/health\nvllm-eval config validate --test-endpoints\n</code></pre></p> <p>Solutions: <pre><code># Increase timeouts\nvllm-eval --profile production run evalchemy my-model --timeout 7200\n\n# Reduce batch size\nvllm-eval run evalchemy my-model --batch-size 1\n\n# Use dry run to test configuration\nvllm-eval run evalchemy my-model --dry-run\n\n# Kill hanging processes\npkill -f vllm-eval\nvllm-eval system clean\n</code></pre></p>"},{"location":"operations/troubleshooting/#issue-memory-and-resource-issues","title":"Issue: Memory and Resource Issues","text":"<p>Symptoms: - Out of memory errors - System slowdown during evaluation - Disk space warnings</p> <p>Diagnosis: <pre><code># Check system resources\nfree -h\ndf -h\ntop\n\n# Check evaluation logs\nvllm-eval system logs --grep \"memory\\|disk\\|resource\"\n\n# Monitor during execution\nwatch -n 5 'free -h &amp;&amp; df -h'\n</code></pre></p> <p>Solutions: <pre><code># Reduce memory usage\nvllm-eval run evalchemy my-model --batch-size 1 --limit 100\n\n# Clean up results\nvllm-eval system clean\nrm -rf ~/.config/vllm-eval/cache/*\n\n# Use streaming output\nvllm-eval run evalchemy my-model --stream-results\n\n# Configure resource limits\nvllm-eval config edit  # Set memory_limit, disk_limit\n</code></pre></p>"},{"location":"operations/troubleshooting/#framework-specific-issues","title":"Framework-Specific Issues","text":""},{"location":"operations/troubleshooting/#evalchemy-issues","title":"Evalchemy Issues","text":"<p>Common Problems: - Task not found errors - API compatibility issues - Slow evaluation speed</p> <p>Solutions: <pre><code># Check available tasks\nvllm-eval run evalchemy --help | grep -A 20 \"Available tasks\"\n\n# Test with minimal tasks\nvllm-eval run evalchemy my-model --tasks arc_easy --limit 10\n\n# Update Evalchemy\npip install --upgrade evalchemy\n\n# Check endpoint compatibility\ncurl -X POST http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"test\", \"prompt\": \"Hello\", \"max_tokens\": 5}'\n</code></pre></p>"},{"location":"operations/troubleshooting/#nvidia-eval-issues","title":"NVIDIA Eval Issues","text":"<p>Common Problems: - GPU not available - CUDA compatibility issues - Model loading failures</p> <p>Solutions: <pre><code># Check GPU availability\nnvidia-smi\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n# Use CPU mode\nvllm-eval run nvidia my-model --gpus 0\n\n# Check CUDA version\nnvcc --version\npython -c \"import torch; print(torch.version.cuda)\"\n</code></pre></p>"},{"location":"operations/troubleshooting/#kubernetes-operations-issues","title":"\u2638\ufe0f Kubernetes Operations Issues","text":""},{"location":"operations/troubleshooting/#cluster-and-node-issues","title":"Cluster and Node Issues","text":""},{"location":"operations/troubleshooting/#issue-node-resource-exhaustion","title":"Issue: Node Resource Exhaustion","text":"<p>Symptoms: - Pods stuck in Pending state - Node pressure warnings - Failed pod scheduling</p> <p>Diagnosis: <pre><code># Check node status\nkubectl describe nodes\nkubectl top nodes\n\n# Check resource requests and limits\nkubectl describe pods -n vllm-eval\n\n# Check node conditions\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.conditions[?(@.type==\"Ready\")].status}{\"\\n\"}{end}'\n</code></pre></p> <p>Solutions: <pre><code># Scale down unnecessary pods\nkubectl scale deployment --replicas=0 -n default --all\n\n# Clean up completed jobs\nkubectl delete jobs --field-selector status.successful=1 -n vllm-eval\n\n# Add more nodes (cloud environments)\n# kubectl scale --replicas=3 nodepool/evaluation-nodes\n\n# Adjust resource requests\nkubectl patch deployment evaluation-runner -n vllm-eval -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"runner\",\"resources\":{\"requests\":{\"memory\":\"1Gi\",\"cpu\":\"0.5\"}}}]}}}}'\n</code></pre></p>"},{"location":"operations/troubleshooting/#issue-persistent-volume-problems","title":"Issue: Persistent Volume Problems","text":"<p>Symptoms: - Pods failing to start due to volume issues - Data persistence problems - Storage capacity warnings</p> <p>Diagnosis: <pre><code># Check PV and PVC status\nkubectl get pv,pvc -A\nkubectl describe pvc -n vllm-eval\n\n# Check storage class\nkubectl get storageclass\nkubectl describe storageclass standard\n</code></pre></p> <p>Solutions: <pre><code># Clean up unused PVCs\nkubectl delete pvc --all -n vllm-eval --force\n\n# Increase PVC size\nkubectl patch pvc data-pvc -n vllm-eval -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}'\n\n# Use different storage class\nkubectl patch pvc data-pvc -n vllm-eval -p '{\"spec\":{\"storageClassName\":\"fast-ssd\"}}'\n</code></pre></p>"},{"location":"operations/troubleshooting/#argo-workflows-issues","title":"Argo Workflows Issues","text":""},{"location":"operations/troubleshooting/#issue-workflow-failures","title":"Issue: Workflow Failures","text":"<p>Symptoms: - Workflows stuck in running state - Step failures with unclear errors - Resource timeout issues</p> <p>Diagnosis: <pre><code># Check workflow status\nargo list -n argo\nargo get &lt;workflow-name&gt; -n argo\nargo logs &lt;workflow-name&gt; -n argo\n\n# Check workflow events\nkubectl describe workflow &lt;workflow-name&gt; -n argo\n\n# Check Argo controller\nkubectl logs -n argo deployment/workflow-controller\n</code></pre></p> <p>Solutions: <pre><code># Restart failed workflow\nargo resubmit &lt;workflow-name&gt; -n argo\n\n# Stop running workflow\nargo stop &lt;workflow-name&gt; -n argo\n\n# Clean up old workflows\nargo delete --older 7d -n argo\n\n# Restart Argo controller\nkubectl rollout restart deployment/workflow-controller -n argo\n</code></pre></p>"},{"location":"operations/troubleshooting/#issue-argo-events-not-triggering","title":"Issue: Argo Events Not Triggering","text":"<p>Symptoms: - Workflows not starting automatically - Event source connection issues - Sensor not responding to events</p> <p>Diagnosis: <pre><code># Check event sources\nkubectl get eventsources -n argo-events\nkubectl describe eventsource webhook-event-source -n argo-events\n\n# Check sensors\nkubectl get sensors -n argo-events\nkubectl logs -l app=sensor-controller -n argo-events\n\n# Test webhook manually\ncurl -X POST http://&lt;webhook-url&gt;/webhook \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ref\": \"refs/heads/main\", \"repository\": {\"name\": \"test\"}}'\n</code></pre></p> <p>Solutions: <pre><code># Restart event source\nkubectl delete eventsource webhook-event-source -n argo-events\nkubectl apply -f k8s/argo-events/event-source.yaml\n\n# Restart sensor\nkubectl rollout restart deployment/webhook-sensor -n argo-events\n\n# Check network connectivity\nkubectl exec -it &lt;sensor-pod&gt; -n argo-events -- wget -O- http://github.com\n</code></pre></p>"},{"location":"operations/troubleshooting/#database-and-storage-issues","title":"Database and Storage Issues","text":""},{"location":"operations/troubleshooting/#issue-clickhouse-connection-problems","title":"Issue: ClickHouse Connection Problems","text":"<p>Symptoms: - Metrics not being stored - Query timeouts - Connection refused errors</p> <p>Diagnosis: <pre><code># Check ClickHouse pod status\nkubectl get pods -l app=clickhouse -n vllm-eval\nkubectl logs -l app=clickhouse -n vllm-eval\n\n# Test connection\nkubectl exec -it clickhouse-0 -n vllm-eval -- clickhouse-client --query \"SELECT 1\"\n\n# Check service\nkubectl describe service clickhouse -n vllm-eval\n</code></pre></p> <p>Solutions: <pre><code># Restart ClickHouse\nkubectl rollout restart statefulset/clickhouse -n vllm-eval\n\n# Check disk space\nkubectl exec -it clickhouse-0 -n vllm-eval -- df -h\n\n# Optimize tables\nkubectl exec -it clickhouse-0 -n vllm-eval -- clickhouse-client --query \"OPTIMIZE TABLE evaluation_metrics\"\n\n# Scale ClickHouse\nkubectl scale statefulset clickhouse --replicas=2 -n vllm-eval\n</code></pre></p>"},{"location":"operations/troubleshooting/#performance-troubleshooting","title":"\ud83d\udcca Performance Troubleshooting","text":""},{"location":"operations/troubleshooting/#evaluation-performance-issues","title":"Evaluation Performance Issues","text":""},{"location":"operations/troubleshooting/#issue-slow-evaluation-speed","title":"Issue: Slow Evaluation Speed","text":"<p>Root Causes: - Low batch sizes - Network latency - Endpoint throttling - Resource contention</p> <p>Optimization Strategies:</p> <pre><code># Increase batch size\nvllm-eval run evalchemy my-model --batch-size 8\n\n# Use parallel execution\nvllm-eval run all my-model --parallel\n\n# Optimize endpoint\nvllm serve my-model --tensor-parallel-size 2 --max-num-seqs 32\n\n# Use local caching\nvllm-eval run evalchemy my-model --cache-responses\n</code></pre>"},{"location":"operations/troubleshooting/#issue-high-resource-usage","title":"Issue: High Resource Usage","text":"<p>Monitoring Commands: <pre><code># Monitor CLI resource usage\ntop -p $(pgrep -f vllm-eval)\n\n# Monitor Kubernetes resources\nkubectl top pods -n vllm-eval\nkubectl describe nodes\n\n# Monitor network usage\niftop  # or nethogs\n</code></pre></p> <p>Optimization: <pre><code># Limit concurrent evaluations\nvllm-eval config edit  # Set max_concurrent_jobs = 2\n\n# Use resource limits in Kubernetes\nkubectl patch deployment evaluation-runner -n vllm-eval -p '{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"containers\": [{\n          \"name\": \"runner\",\n          \"resources\": {\n            \"limits\": {\"memory\": \"4Gi\", \"cpu\": \"2\"},\n            \"requests\": {\"memory\": \"2Gi\", \"cpu\": \"1\"}\n          }\n        }]\n      }\n    }\n  }\n}'\n</code></pre></p>"},{"location":"operations/troubleshooting/#system-performance-monitoring","title":"System Performance Monitoring","text":""},{"location":"operations/troubleshooting/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<p>CLI Performance: - Evaluation completion time - Memory usage during execution - Disk I/O for results storage - Network latency to endpoints</p> <p>Kubernetes Performance: - Pod resource utilization - Node resource allocation - Network bandwidth usage - Storage I/O patterns</p>"},{"location":"operations/troubleshooting/#performance-monitoring-setup","title":"Performance Monitoring Setup","text":"<pre><code># Enable performance monitoring\nvllm-eval system enable-monitoring\n\n# Install monitoring tools in Kubernetes\nhelm install prometheus prometheus-community/kube-prometheus-stack -n monitoring\nhelm install grafana grafana/grafana -n monitoring\n\n# Setup custom dashboards\nkubectl apply -f k8s/monitoring/dashboards/\n</code></pre>"},{"location":"operations/troubleshooting/#integration-troubleshooting","title":"\ud83d\udd17 Integration Troubleshooting","text":""},{"location":"operations/troubleshooting/#cli-kubernetes-integration-issues","title":"CLI \u2194 Kubernetes Integration Issues","text":""},{"location":"operations/troubleshooting/#issue-configuration-sync-problems","title":"Issue: Configuration Sync Problems","text":"<p>Symptoms: - CLI and K8s using different configurations - Results format inconsistencies - Profile mismatch errors</p> <p>Solutions: <pre><code># Sync CLI config to Kubernetes\nvllm-eval config export --format kubernetes &gt; k8s-config.yaml\nkubectl apply -f k8s-config.yaml\n\n# Validate configuration compatibility\nvllm-eval config validate --target kubernetes\n\n# Use shared configuration storage\nkubectl create configmap vllm-eval-config --from-file=~/.config/vllm-eval/\n</code></pre></p>"},{"location":"operations/troubleshooting/#issue-result-format-inconsistencies","title":"Issue: Result Format Inconsistencies","text":"<p>Symptoms: - Different result schemas from CLI vs K8s - Aggregation failures - Dashboard display issues</p> <p>Solutions: <pre><code># Standardize result format\nvllm-eval results standardize --input ./results/ --output ./standardized/\n\n# Validate result schema\nvllm-eval results validate --schema ./schemas/result-schema.json\n\n# Convert legacy results\nvllm-eval results migrate --from v1.0 --to v2.0\n</code></pre></p>"},{"location":"operations/troubleshooting/#external-system-integration","title":"External System Integration","text":""},{"location":"operations/troubleshooting/#issue-model-server-connectivity","title":"Issue: Model Server Connectivity","text":"<p>Common Problems: - Authentication failures - Network timeouts - API version mismatches</p> <p>Diagnosis and Solutions: <pre><code># Test basic connectivity\ncurl -v http://model-server:8000/health\n\n# Test API compatibility\ncurl -X POST http://model-server:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${API_KEY}\" \\\n  -d '{\"model\": \"test\", \"prompt\": \"Hello\", \"max_tokens\": 5}'\n\n# Check API version\ncurl http://model-server:8000/v1/models\n\n# Update endpoint configuration\nvllm-eval config edit  # Update endpoint URLs\n</code></pre></p>"},{"location":"operations/troubleshooting/#monitoring-and-alerting","title":"\ud83d\udcca Monitoring and Alerting","text":""},{"location":"operations/troubleshooting/#production-monitoring-setup","title":"Production Monitoring Setup","text":""},{"location":"operations/troubleshooting/#key-metrics-dashboard","title":"Key Metrics Dashboard","text":"<pre><code># Grafana dashboard configuration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-eval-dashboard\ndata:\n  dashboard.json: |\n    {\n      \"dashboard\": {\n        \"title\": \"VLLM Evaluation System\",\n        \"panels\": [\n          {\n            \"title\": \"Evaluation Success Rate\",\n            \"type\": \"stat\",\n            \"targets\": [{\n              \"expr\": \"rate(vllm_eval_success_total[5m])\"\n            }]\n          },\n          {\n            \"title\": \"Average Evaluation Time\",\n            \"type\": \"graph\",\n            \"targets\": [{\n              \"expr\": \"avg(vllm_eval_duration_seconds)\"\n            }]\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"operations/troubleshooting/#alerting-rules","title":"Alerting Rules","text":"<pre><code># Prometheus alerting rules\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: vllm-eval-alerts\nspec:\n  groups:\n  - name: vllm-eval\n    rules:\n    - alert: EvaluationFailureRate\n      expr: rate(vllm_eval_failures_total[5m]) &gt; 0.1\n      for: 5m\n      annotations:\n        summary: High evaluation failure rate\n        description: \"Evaluation failure rate is {{ $value }} over 5 minutes\"\n\n    - alert: LongRunningEvaluation\n      expr: vllm_eval_duration_seconds &gt; 7200\n      for: 0s\n      annotations:\n        summary: Evaluation taking too long\n        description: \"Evaluation {{ $labels.job }} has been running for {{ $value }} seconds\"\n</code></pre>"},{"location":"operations/troubleshooting/#log-analysis","title":"Log Analysis","text":""},{"location":"operations/troubleshooting/#centralized-logging","title":"Centralized Logging","text":"<pre><code># Setup log aggregation\nkubectl apply -f k8s/logging/fluentd.yaml\nkubectl apply -f k8s/logging/elasticsearch.yaml\nkubectl apply -f k8s/logging/kibana.yaml\n\n# Query logs\ncurl -X POST \"elasticsearch:9200/vllm-eval-logs/_search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": {\n      \"bool\": {\n        \"must\": [\n          {\"range\": {\"@timestamp\": {\"gte\": \"now-1h\"}}},\n          {\"match\": {\"level\": \"ERROR\"}}\n        ]\n      }\n    }\n  }'\n</code></pre>"},{"location":"operations/troubleshooting/#log-analysis-patterns","title":"Log Analysis Patterns","text":"<pre><code># Common error patterns\ngrep -E \"ERROR|FAIL|TIMEOUT\" /var/log/vllm-eval/*.log\n\n# Performance analysis\ngrep -E \"duration|latency|throughput\" /var/log/vllm-eval/*.log | awk '{print $NF}' | sort -n\n\n# Resource usage patterns\ngrep -E \"memory|cpu|disk\" /var/log/vllm-eval/*.log\n</code></pre>"},{"location":"operations/troubleshooting/#recovery-procedures","title":"\ud83d\udd04 Recovery Procedures","text":""},{"location":"operations/troubleshooting/#system-recovery-playbook","title":"System Recovery Playbook","text":""},{"location":"operations/troubleshooting/#level-1-service-recovery","title":"Level 1: Service Recovery","text":"<pre><code># 1. Stop all evaluations\nvllm-eval system stop-all\nkubectl delete jobs --all -n vllm-eval\n\n# 2. Clear caches and temporary files\nvllm-eval system clean --all\nkubectl delete pods -l temp=true -n vllm-eval\n\n# 3. Restart core services\nvllm-eval system restart\nkubectl rollout restart deployment -n vllm-eval\n\n# 4. Validate system health\nvllm-eval doctor --verbose\nkubectl get pods -n vllm-eval\n</code></pre>"},{"location":"operations/troubleshooting/#level-2-configuration-recovery","title":"Level 2: Configuration Recovery","text":"<pre><code># 1. Backup current configuration\ncp -r ~/.config/vllm-eval/ ~/.config/vllm-eval.backup/\nkubectl get configmaps -n vllm-eval -o yaml &gt; k8s-config-backup.yaml\n\n# 2. Reset to defaults\nvllm-eval setup --reset --force\nkubectl delete configmaps --all -n vllm-eval\n\n# 3. Restore from backup\nvllm-eval config import ~/.config/vllm-eval.backup/production.toml\nkubectl apply -f k8s-config-backup.yaml\n\n# 4. Validate configuration\nvllm-eval config validate --all-profiles\n</code></pre>"},{"location":"operations/troubleshooting/#level-3-full-system-recovery","title":"Level 3: Full System Recovery","text":"<pre><code># 1. Complete teardown\nvllm-eval uninstall --purge\nmake kind-teardown\n\n# 2. Fresh installation\nmake kind-deploy\nmake helm-install\npip install -e .\n\n# 3. Restore data from backups\nvllm-eval system restore --from-backup /path/to/backup/\nkubectl apply -f backup/k8s-resources/\n\n# 4. Comprehensive validation\nmake run-tests\nvllm-eval system validate --comprehensive\n</code></pre>"},{"location":"operations/troubleshooting/#documentation-and-resources","title":"\ud83d\udcda Documentation and Resources","text":""},{"location":"operations/troubleshooting/#troubleshooting-resources","title":"Troubleshooting Resources","text":"<ul> <li>CLI Troubleshooting - CLI-specific issues</li> <li>System Overview - Architecture reference</li> <li>Configuration Guide - Configuration troubleshooting</li> <li>API Documentation - Framework-specific issues</li> </ul>"},{"location":"operations/troubleshooting/#external-resources","title":"External Resources","text":"<ul> <li>Kubernetes Troubleshooting: k8s.io/docs/tasks/debug-application-cluster/</li> <li>Argo Workflows: argoproj.github.io/argo-workflows/troubleshooting/</li> <li>Docker Issues: docs.docker.com/config/troubleshoot/</li> </ul>"},{"location":"operations/troubleshooting/#support-escalation","title":"Support Escalation","text":"<pre><code># Generate support bundle\nvllm-eval system support-bundle --include-logs --include-config\n\n# System information for support\nvllm-eval system info --verbose &gt; system-info.txt\nkubectl cluster-info dump &gt; cluster-info.yaml\n</code></pre> <p>Production Safety</p> <p>Always test troubleshooting procedures in staging environments first.</p> <p>Create backups before making significant changes to production systems.</p> <p>Document incidents for future reference and team knowledge sharing.</p> <p>Operational Excellence</p> <p>You now have comprehensive troubleshooting capabilities for both CLI and Kubernetes operations.</p> <p>Remember: Systematic diagnosis leads to faster resolution and better system reliability.</p>"},{"location":"user/benchmark-configuration/","title":"Benchmark Configuration# Benchmark Configuration","text":"<p>Comprehensive guide to configuring benchmarks and evaluation parameters using the VLLM Evaluation CLI. Learn how to customize evaluations for different frameworks, manage configuration profiles, and optimize settings for your specific use cases.</p> <p>Configuration Management</p> <ul> <li>\u2699\ufe0f Profile-Based: Environment-specific configurations</li> <li>\ud83d\udccb TOML Format: Human-readable configuration files</li> <li>\u2705 Validation: Built-in configuration and connectivity testing</li> <li>\ud83d\udd04 Inheritance: Profile inheritance and override capabilities</li> </ul>"},{"location":"user/benchmark-configuration/#configuration-overview","title":"Configuration Overview","text":""},{"location":"user/benchmark-configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>The CLI uses a hierarchical configuration system with multiple sources:</p> <ol> <li>Command-line arguments (highest priority)</li> <li>Profile-specific TOML files</li> <li>Default configuration</li> <li>Environment variables</li> </ol> <pre><code># Configuration precedence example\nvllm-eval --config ./custom.toml \\\n          --profile production \\\n          run evalchemy my-model \\\n          --batch-size 4         # CLI args override config\n</code></pre>"},{"location":"user/benchmark-configuration/#profile-management","title":"\u2699\ufe0f Profile Management","text":""},{"location":"user/benchmark-configuration/#creating-and-managing-profiles","title":"Creating and Managing Profiles","text":"<pre><code># Create new profile\nvllm-eval config create development\nvllm-eval config create production\nvllm-eval config create ci-cd\n\n# List available profiles\nvllm-eval config list-profiles\n\n# Show profile configuration\nvllm-eval config show --profile production\n\n# Set default profile\nvllm-eval config set-default production\n\n# Copy profile with modifications\nvllm-eval config create staging --from-profile production\n</code></pre>"},{"location":"user/benchmark-configuration/#profile-structure","title":"Profile Structure","text":"<p>Configuration files are stored in <code>~/.config/vllm-eval/</code>:</p> <pre><code>~/.config/vllm-eval/\n\u251c\u2500\u2500 config.toml              # Default configuration\n\u251c\u2500\u2500 development.toml         # Development profile  \n\u251c\u2500\u2500 production.toml          # Production profile\n\u251c\u2500\u2500 ci-cd.toml              # CI/CD profile\n\u2514\u2500\u2500 logs/                   # Configuration logs\n</code></pre>"},{"location":"user/benchmark-configuration/#framework-configuration","title":"\ud83d\udccb Framework Configuration","text":""},{"location":"user/benchmark-configuration/#evalchemy-configuration","title":"Evalchemy Configuration","text":""},{"location":"user/benchmark-configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>[evaluation.evalchemy]\nenabled = true\nendpoint = \"http://localhost:8000/v1/completions\"\nbatch_size = 4\ntimeout = 3600\n\n# Task selection\ntasks = [\n    \"mmlu\",           # Massive Multitask Language Understanding  \n    \"arc_easy\",       # AI2 Reasoning Challenge (Easy)\n    \"arc_challenge\",  # AI2 Reasoning Challenge (Hard)\n    \"hellaswag\",      # Common sense reasoning\n    \"humaneval\",      # Code generation\n]\n\n# Advanced configuration\nnum_fewshot = 0\nlimit = null              # No limit on test cases\ninclude_path = \"./custom-tasks/\"\n</code></pre>"},{"location":"user/benchmark-configuration/#task-specific-settings","title":"Task-Specific Settings","text":"<pre><code>[evaluation.evalchemy.tasks]\n# MMLU configuration\nmmlu = {\n    enabled = true,\n    num_fewshot = 5,\n    limit = 1000,\n    categories = [\"humanities\", \"social_sciences\", \"stem\"]\n}\n\n# HumanEval configuration  \nhumaneval = {\n    enabled = true,\n    num_fewshot = 0,\n    temperature = 0.0,\n    max_tokens = 512\n}\n\n# Custom task configuration\ncustom_reasoning = {\n    enabled = true,\n    task_file = \"./custom-tasks/reasoning.json\",\n    metric = \"exact_match\"\n}\n</code></pre>"},{"location":"user/benchmark-configuration/#generation-parameters","title":"Generation Parameters","text":"<pre><code>[evaluation.evalchemy.generation]\ntemperature = 0.0\ntop_p = 1.0\nmax_tokens = 2048\nstop_sequences = [\"\\n\\n\", \"###\"]\n\n# Model-specific overrides\n[evaluation.evalchemy.model_overrides]\n\"gpt-4\" = { temperature = 0.1, max_tokens = 4096 }\n\"claude-3\" = { temperature = 0.0, top_p = 0.95 }\n</code></pre>"},{"location":"user/benchmark-configuration/#nvidia-eval-configuration","title":"NVIDIA Eval Configuration","text":""},{"location":"user/benchmark-configuration/#aime-benchmark","title":"AIME Benchmark","text":"<pre><code>[evaluation.nvidia_eval]\nenabled = true\nbenchmark = \"aime\"         # or \"livecodebench\"\nmodel_path = \"/path/to/model\"  # Optional, auto-detect if not specified\ngpus = 2\nout_seq_len = 4096\ntimeout = 7200\n\n# AIME-specific settings\n[evaluation.nvidia_eval.aime]\nnum_problems = 30          # Number of problems to evaluate\ntemperature = 0.0\ntop_p = 1.0 \nmax_new_tokens = 2048\nuse_sampling = false       # Deterministic generation\n\n# Problem selection\nproblem_years = [2023, 2024]  # Specific years\ndifficulty_range = [1, 15]     # Problem difficulty (1-15)\n</code></pre>"},{"location":"user/benchmark-configuration/#livecodebench-configuration","title":"LiveCodeBench Configuration","text":"<pre><code>[evaluation.nvidia_eval.livecodebench]\ncontest_range = \"2024-01-01:2024-12-31\"\nlanguages = [\"python\", \"java\", \"cpp\"]\nmax_context_length = 8192\ngenerate_tests = true\n\n# Language-specific settings\n[evaluation.nvidia_eval.livecodebench.python]\nrunner = \"python3.11\"\ntimeout = 30              # seconds per test\nmemory_limit = \"512MB\"\n\n[evaluation.nvidia_eval.livecodebench.java]\nrunner = \"openjdk-17\"\ntimeout = 45\nmemory_limit = \"1GB\"\n</code></pre>"},{"location":"user/benchmark-configuration/#vllm-benchmark-configuration","title":"VLLM Benchmark Configuration","text":""},{"location":"user/benchmark-configuration/#performance-testing","title":"Performance Testing","text":"<pre><code>[evaluation.vllm_benchmark]\nenabled = true\nendpoint = \"http://localhost:8000\"\nscenario = \"performance\"   # performance, throughput, latency\n\n# Load testing parameters\nconcurrency = 20\nduration = 600            # seconds\nwarmup_time = 60         # seconds\nrequest_rate = 10        # requests per second (0 = max rate)\n\n# Request configuration\ninput_length = 1024\noutput_length = 128\nmax_tokens = 2048\n</code></pre>"},{"location":"user/benchmark-configuration/#scenario-specific-settings","title":"Scenario-Specific Settings","text":"<pre><code>[evaluation.vllm_benchmark.scenarios]\n# Performance scenario: balanced load testing\nperformance = {\n    concurrency = 20,\n    duration = 600,\n    request_rate = 10,\n    metrics = [\"ttft\", \"tpot\", \"throughput\", \"latency\"]\n}\n\n# Throughput scenario: maximum throughput testing\nthroughput = {\n    concurrency = 50,\n    duration = 300,\n    request_rate = 0,      # Maximum rate\n    input_lengths = [512, 1024, 2048],\n    output_lengths = [64, 128, 256]\n}\n\n# Latency scenario: low-latency testing\nlatency = {\n    concurrency = 1,\n    duration = 180,\n    request_rate = 1,\n    input_length = 512,\n    output_length = 32,\n    percentiles = [50, 90, 95, 99]\n}\n</code></pre>"},{"location":"user/benchmark-configuration/#deepeval-configuration","title":"Deepeval Configuration","text":""},{"location":"user/benchmark-configuration/#rag-evaluation","title":"RAG Evaluation","text":"<pre><code>[evaluation.deepeval]\nenabled = true\nendpoint = \"http://localhost:8000/v1/completions\"\nsuite = \"rag\"             # default, rag, custom\n\n# Metrics selection\nmetrics = [\n    \"faithfulness\",        # RAG faithfulness\n    \"answer_relevancy\",    # Answer relevance\n    \"context_precision\",   # Context precision\n    \"context_recall\"       # Context recall\n]\n\n# Test data configuration\ntest_cases_file = \"./test_cases.json\"\ncontext_file = \"./knowledge_base.json\"\nmax_test_cases = 100\n</code></pre>"},{"location":"user/benchmark-configuration/#custom-metrics-configuration","title":"Custom Metrics Configuration","text":"<pre><code>[evaluation.deepeval.custom_metrics]\n# Custom faithfulness threshold\nfaithfulness = {\n    threshold = 0.7,\n    model = \"gpt-4\",\n    include_reason = true\n}\n\n# Custom relevancy metric\nrelevancy = {\n    threshold = 0.8,\n    strict_mode = true,\n    async_mode = false\n}\n\n# Custom G-Eval metric\ncustom_correctness = {\n    name = \"Correctness\",\n    criteria = \"Determine whether the actual output is factually correct based on the expected output.\",\n    evaluation_steps = [\n        \"Check if the main facts in the actual output align with the expected output\",\n        \"Verify that there are no factual errors or contradictions\",\n        \"Assess the completeness of the information provided\"\n    ],\n    evaluation_params = [\"factual_accuracy\", \"completeness\", \"consistency\"]\n}\n</code></pre>"},{"location":"user/benchmark-configuration/#environment-specific-configurations","title":"\ud83c\udfe2 Environment-Specific Configurations","text":""},{"location":"user/benchmark-configuration/#development-profile","title":"Development Profile","text":"<pre><code># ~/.config/vllm-eval/development.toml\nprofile_name = \"development\"\ndescription = \"Development environment settings\"\nlog_level = \"DEBUG\"\ndry_run = false\n\n[model]\nname = \"test-model\"\nendpoint = \"http://localhost:8000/v1/completions\"\nbatch_size = 1\n\n[system]\nresults_dir = \"./dev-results\"\ntimeout_default = 1800    # Shorter timeouts for dev\nmax_concurrent_jobs = 1\n\n[evaluation.evalchemy]\nenabled = true\ntasks = [\"arc_easy\"]      # Subset for faster iteration\nbatch_size = 1\ntimeout = 1800\nlimit = 50               # Limit test cases for speed\n\n[evaluation.deepeval]\nenabled = true\nmetrics = [\"faithfulness\"] # Single metric for quick tests\nmax_test_cases = 10\n\n[evaluation.nvidia_eval]\nenabled = false           # Skip GPU-intensive tests in dev\n\n[evaluation.vllm_benchmark]\nenabled = true\nscenario = \"latency\"\nconcurrency = 1\nduration = 60            # Short performance test\n</code></pre>"},{"location":"user/benchmark-configuration/#production-profile","title":"Production Profile","text":"<pre><code># ~/.config/vllm-eval/production.toml\nprofile_name = \"production\"\ndescription = \"Production evaluation settings\"\nlog_level = \"INFO\"\ndry_run = false\n\n[model]\nname = \"${MODEL_NAME}\"    # Environment variable\nendpoint = \"${VLLM_ENDPOINT:-https://api.production.com/v1/completions}\"\nbatch_size = 8\n\n[system]\nresults_dir = \"${RESULTS_DIR:-/shared/results}\"\ntimeout_default = 7200\nmax_concurrent_jobs = 4\nlog_retention_days = 90\n\n[evaluation.evalchemy]\nenabled = true\ntasks = [\"mmlu\", \"arc_easy\", \"arc_challenge\", \"hellaswag\", \"humaneval\"]\nbatch_size = 8\ntimeout = 7200\n\n[evaluation.nvidia_eval]\nenabled = true\nbenchmark = \"aime\"\ngpus = 4\ntimeout = 10800\n\n[evaluation.vllm_benchmark]\nenabled = true\nscenario = \"performance\"\nconcurrency = 50\nduration = 1800\n\n[evaluation.deepeval]\nenabled = true\nmetrics = [\"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\"]\nmax_test_cases = 1000\n\n[output]\nformat = \"json\"\ncompress_results = true\ninclude_raw_responses = false  # Save storage in production\n</code></pre>"},{"location":"user/benchmark-configuration/#cicd-profile","title":"CI/CD Profile","text":"<pre><code># ~/.config/vllm-eval/ci-cd.toml\nprofile_name = \"ci-cd\"\ndescription = \"Continuous integration settings\"\nlog_level = \"INFO\"\ndry_run = false\n\n[model]\nname = \"${CI_MODEL_NAME}\"\nendpoint = \"${CI_VLLM_ENDPOINT}\"\nbatch_size = 2\n\n[system]\nresults_dir = \"${CI_RESULTS_DIR:-./ci-results}\"\ntimeout_default = 3600    # Limit CI time\nmax_concurrent_jobs = 2\n\n[evaluation.evalchemy]\nenabled = true\ntasks = [\"arc_easy\", \"mmlu\"]  # Essential tasks only\nbatch_size = 2\ntimeout = 3600\nlimit = 200              # Limit for CI speed\n\n[evaluation.nvidia_eval]\nenabled = false          # Skip in CI for time\n\n[evaluation.vllm_benchmark]\nenabled = true\nscenario = \"latency\"\nconcurrency = 5\nduration = 300\n\n[evaluation.deepeval]\nenabled = false          # Skip in CI for speed\n\n[output]\nformat = \"json\"\ncompress_results = true\nexport_to_artifact = true\n</code></pre>"},{"location":"user/benchmark-configuration/#advanced-configuration-patterns","title":"\ud83d\udcca Advanced Configuration Patterns","text":""},{"location":"user/benchmark-configuration/#variable-substitution","title":"Variable Substitution","text":"<pre><code># Environment variables with defaults\n[model]\nendpoint = \"${VLLM_ENDPOINT:-http://localhost:8000/v1}\"\napi_key = \"${API_KEY}\"    # Required environment variable\nmodel_name = \"${MODEL_NAME:-default-model}\"\n\n# Cross-reference within config\n[evaluation.evalchemy]\nendpoint = \"${model.endpoint}\"\nbatch_size = \"${model.batch_size:-1}\"\n\n# Computed values\n[system]\nresults_dir = \"${HOME}/.vllm-eval/results/${model.name}\"\nlog_file = \"${system.results_dir}/evaluation.log\"\n</code></pre>"},{"location":"user/benchmark-configuration/#conditional-configuration","title":"Conditional Configuration","text":"<pre><code># GPU availability detection\n[evaluation.nvidia_eval]\nenabled = \"${HAS_GPU:-false}\"  # Enable only if GPU available\ngpus = \"${GPU_COUNT:-1}\"\n\n# Environment-based settings\n[evaluation.evalchemy]\ntasks = \"${EVALCHEMY_TASKS:-[mmlu,arc_easy]}\"\nlimit = \"${CI:-false}\" == \"true\" ? 100 : null  # Limit in CI\n</code></pre>"},{"location":"user/benchmark-configuration/#profile-inheritance","title":"Profile Inheritance","text":"<pre><code># staging.toml inherits from production.toml\n[inherit]\nfrom = \"production\"\n\n# Override specific settings\n[evaluation.evalchemy]\nlimit = 500              # Reduced test set for staging\n\n[evaluation.vllm_benchmark]\nduration = 900           # Shorter performance test\n</code></pre>"},{"location":"user/benchmark-configuration/#configuration-validation","title":"\u2705 Configuration Validation","text":""},{"location":"user/benchmark-configuration/#built-in-validation","title":"Built-in Validation","text":"<pre><code># Validate configuration syntax and completeness\nvllm-eval config validate\n\n# Test endpoint connectivity\nvllm-eval config validate --test-endpoints\n\n# Validate specific profile\nvllm-eval config validate --profile production\n\n# Check configuration resolution (with variables)\nvllm-eval config show --expand-variables\n</code></pre>"},{"location":"user/benchmark-configuration/#custom-validation-rules","title":"Custom Validation Rules","text":"<pre><code>[validation]\n# Endpoint requirements\nrequire_https_in_production = true\nallow_localhost_in_dev = true\n\n# Resource limits\nmax_batch_size = 32\nmax_concurrent_jobs = 10\nmax_timeout = 14400      # 4 hours\n\n# Framework requirements\nrequire_at_least_one_framework = true\nwarn_on_gpu_requirements = true\n</code></pre>"},{"location":"user/benchmark-configuration/#validation-output","title":"Validation Output","text":"<pre><code>vllm-eval config validate\n</code></pre> <pre><code>\u2705 Configuration Validation Results\n\n\ud83d\udccb Profile: production\n\u2705 Syntax: Valid TOML format\n\u2705 Schema: All required fields present\n\u2705 Variables: All environment variables resolved\n\u26a0\ufe0f  Endpoints: \n  \u2705 evalchemy: http://api.prod.com/v1 (200 OK)\n  \u274c deepeval: http://api.prod.com/v1 (Connection timeout)\n\u2705 Resources: Within limits\n\u2705 Frameworks: 3/4 enabled and validated\n\n\ud83d\udea8 Issues Found:\n  \u274c deepeval endpoint unreachable\n  \u26a0\ufe0f  nvidia_eval requires GPU but none detected\n\n\ud83d\udca1 Suggestions:\n  - Check deepeval endpoint configuration\n  - Set nvidia_eval.enabled = false for CPU-only environments\n  - Consider increasing timeout for large models\n</code></pre>"},{"location":"user/benchmark-configuration/#configuration-workflows","title":"\ud83d\udd04 Configuration Workflows","text":""},{"location":"user/benchmark-configuration/#development-to-production","title":"Development to Production","text":"<pre><code># 1. Develop and test locally\nvllm-eval --profile development run evalchemy my-model --dry-run\nvllm-eval --profile development run evalchemy my-model --limit 10\n\n# 2. Create staging configuration\nvllm-eval config create staging --from-profile development\nvllm-eval config edit staging  # Adjust for staging environment\n\n# 3. Test staging configuration\nvllm-eval --profile staging config validate --test-endpoints\nvllm-eval --profile staging run all my-model --dry-run\n\n# 4. Deploy to production\nvllm-eval config create production --from-profile staging\nvllm-eval --profile production config validate\n</code></pre>"},{"location":"user/benchmark-configuration/#team-configuration-sharing","title":"Team Configuration Sharing","text":"<pre><code># Export configuration for sharing\nvllm-eval config export --profile production &gt; team-config.toml\n\n# Import team configuration\nvllm-eval config import team-config.toml --profile team-shared\n\n# Version control integration\ncp ~/.config/vllm-eval/production.toml ./configs/\ngit add configs/production.toml\ngit commit -m \"Add production evaluation configuration\"\n</code></pre>"},{"location":"user/benchmark-configuration/#automated-configuration-management","title":"Automated Configuration Management","text":"<pre><code># Generate configuration from template\nenvsubst &lt; config-template.toml &gt; ~/.config/vllm-eval/auto-generated.toml\n\n# Validate in CI/CD\nvllm-eval config validate --profile ci-cd --exit-on-error\n\n# Deploy configuration\nkubectl create configmap vllm-eval-config --from-file=production.toml\n</code></pre>"},{"location":"user/benchmark-configuration/#configuration-troubleshooting","title":"\ud83d\udee0\ufe0f Configuration Troubleshooting","text":""},{"location":"user/benchmark-configuration/#common-issues","title":"Common Issues","text":""},{"location":"user/benchmark-configuration/#1-configuration-not-found","title":"1. Configuration Not Found","text":"<pre><code># Error: Configuration file not found\n# Solution: Run setup wizard\nvllm-eval setup --force\n\n# Or create minimal configuration\nmkdir -p ~/.config/vllm-eval\necho 'profile_name = \"default\"' &gt; ~/.config/vllm-eval/config.toml\n</code></pre>"},{"location":"user/benchmark-configuration/#2-invalid-toml-syntax","title":"2. Invalid TOML Syntax","text":"<pre><code># Error: TOML parsing failed\n# Check syntax\nvllm-eval config validate --check-syntax\n\n# Common issues:\n# - Missing quotes around strings\n# - Invalid escape sequences\n# - Mismatched brackets\n</code></pre>"},{"location":"user/benchmark-configuration/#3-environment-variable-resolution","title":"3. Environment Variable Resolution","text":"<pre><code># Error: Environment variable not found\n# Check variable expansion\nvllm-eval config show --expand-variables\n\n# Set missing variables\nexport VLLM_ENDPOINT=\"http://localhost:8000/v1\"\nexport MODEL_NAME=\"my-model\"\n</code></pre>"},{"location":"user/benchmark-configuration/#4-endpoint-connectivity","title":"4. Endpoint Connectivity","text":"<pre><code># Error: Connection refused\n# Test endpoints\nvllm-eval config validate --test-endpoints --verbose\n\n# Manual testing\ncurl http://localhost:8000/health\ncurl -X POST http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"test\", \"prompt\": \"Hello\", \"max_tokens\": 5}'\n</code></pre>"},{"location":"user/benchmark-configuration/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging for configuration\nexport VLLM_EVAL_DEBUG_CONFIG=1\nvllm-eval --debug config show\n\n# Trace configuration loading\nvllm-eval --debug --verbose config validate\n</code></pre>"},{"location":"user/benchmark-configuration/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":""},{"location":"user/benchmark-configuration/#configuration-references","title":"Configuration References","text":"<ul> <li>CLI Configuration Guide - Detailed CLI configuration documentation</li> <li>CLI Commands Reference - Command-line usage patterns</li> <li>CLI Troubleshooting - Configuration problem resolution</li> </ul>"},{"location":"user/benchmark-configuration/#framework-documentation","title":"Framework Documentation","text":"<ul> <li>Evalchemy API - Evalchemy configuration details</li> <li>NVIDIA Eval API - NVIDIA evaluation setup</li> <li>VLLM Benchmark API - Performance testing configuration</li> <li>Deepeval API - Custom metrics configuration</li> </ul>"},{"location":"user/benchmark-configuration/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Result Interpretation - Understanding evaluation results</li> <li>Architecture Overview - System architecture</li> <li>Operations Guide - Production configuration management</li> </ul> <p>Configuration Mastery</p> <p>You now have comprehensive knowledge of benchmark configuration management.</p> <p>Quick Start: <code>vllm-eval config create my-profile &amp;&amp; vllm-eval config edit my-profile</code></p> <p>Next: Explore Result Interpretation to understand your evaluation results.</p>"},{"location":"user/getting-started/","title":"Getting Started","text":"<p>Welcome to the VLLM Evaluation System! This guide will help you get started with evaluating your language models using our unified CLI interface and comprehensive evaluation frameworks.</p> <p>Quick Overview</p> <p>The VLLM Evaluation System provides:</p> <ul> <li>\ud83d\ude80 Unified CLI: Single interface for all evaluation frameworks</li> <li>\ud83e\uddea Multiple Frameworks: Evalchemy, NVIDIA Eval, VLLM Benchmark, Deepeval</li> <li>\u2699\ufe0f Flexible Deployment: Local CLI or Kubernetes production setup</li> <li>\ud83d\udcca Rich Results: Standardized metrics and comparison tools</li> </ul>"},{"location":"user/getting-started/#choose-your-setup-path","title":"Choose Your Setup Path","text":""},{"location":"user/getting-started/#option-1-cli-only-setup-recommended-for-getting-started","title":"Option 1: CLI-Only Setup (Recommended for Getting Started)","text":"<p>Perfect for: - Local development and testing - Quick model evaluations - Learning the system - Rapid iteration</p>"},{"location":"user/getting-started/#option-2-full-kubernetes-setup","title":"Option 2: Full Kubernetes Setup","text":"<p>Perfect for: - Production deployments - Continuous evaluation pipelines - Scalable processing - Team collaboration</p>"},{"location":"user/getting-started/#cli-only-setup-5-minutes","title":"\ud83d\ude80 CLI-Only Setup (5 Minutes)","text":""},{"location":"user/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Docker (for model serving)</li> <li>Git</li> </ul>"},{"location":"user/getting-started/#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/thakicloud/vllm-eval-public.git\ncd vllm-eval-public\n\n# Install CLI in development mode\npip install -e .\n\n# Verify installation\nvllm-eval --help\n</code></pre>"},{"location":"user/getting-started/#initial-setup","title":"Initial Setup","text":"<pre><code># Run interactive setup wizard\nvllm-eval setup\n\n# Check system status\nvllm-eval doctor\n\n# Validate configuration\nvllm-eval config validate\n</code></pre> <p>The setup wizard will guide you through: - Configuration profile creation - Framework selection and setup - Endpoint configuration - Dependency installation</p>"},{"location":"user/getting-started/#your-first-evaluation","title":"Your First Evaluation","text":""},{"location":"user/getting-started/#step-1-start-a-model-server","title":"Step 1: Start a Model Server","text":"<pre><code># Option A: Use VLLM (recommended)\nvllm serve microsoft/DialoGPT-medium --port 8000\n\n# Option B: Use any OpenAI-compatible server\n# Your server should expose /v1/completions endpoint\n</code></pre>"},{"location":"user/getting-started/#step-2-run-evaluation","title":"Step 2: Run Evaluation","text":"<pre><code># Quick evaluation with Evalchemy\nvllm-eval run evalchemy my-model --endpoint http://localhost:8000/v1\n\n# Or dry-run to see what would happen\nvllm-eval run evalchemy my-model --dry-run\n</code></pre>"},{"location":"user/getting-started/#step-3-view-results","title":"Step 3: View Results","text":"<pre><code># List all results\nvllm-eval results list\n\n# Show detailed results for latest run\nvllm-eval results show &lt;run-id&gt;\n</code></pre>"},{"location":"user/getting-started/#next-steps-with-cli","title":"Next Steps with CLI","text":"<pre><code># Try different frameworks\nvllm-eval run nvidia my-model --benchmark aime\nvllm-eval run vllm-benchmark my-model --scenario performance\nvllm-eval run deepeval my-model --suite rag\n\n# Run all enabled frameworks\nvllm-eval run all my-model\n\n# Create custom configuration profiles\nvllm-eval config create production\nvllm-eval config create testing\n</code></pre>"},{"location":"user/getting-started/#full-kubernetes-setup","title":"\ud83c\udfd7\ufe0f Full Kubernetes Setup","text":""},{"location":"user/getting-started/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker Desktop or OrbStack (for macOS)</li> <li>kubectl</li> <li>Helm 3+</li> <li>Make (for build automation)</li> </ul>"},{"location":"user/getting-started/#installation_1","title":"Installation","text":"<pre><code># Clone repository\ngit clone https://github.com/thakicloud/vllm-eval-public.git\ncd vllm-eval-public\n\n# Setup development environment\nmake dev-setup\n\n# Deploy local Kubernetes cluster with dependencies\nmake kind-deploy\n\n# Install Helm charts (ClickHouse, Grafana, Argo Workflows)\nmake helm-install\n\n# Build and push Docker images\nmake build-images\nmake push-images\n</code></pre>"},{"location":"user/getting-started/#verification","title":"Verification","text":"<pre><code># Check cluster status\nkubectl get pods -A\n\n# Run tests\nmake run-tests\n\n# Submit test workflow\nmake submit-workflow\n\n# Watch workflow execution\nmake watch-workflow\n</code></pre>"},{"location":"user/getting-started/#monitoring-and-visualization","title":"Monitoring and Visualization","text":"<ul> <li>Grafana: Access dashboards for performance metrics</li> <li>ClickHouse: Query evaluation results database</li> <li>Argo Workflows: Monitor evaluation pipeline execution</li> </ul>"},{"location":"user/getting-started/#development-workflow","title":"\ud83d\udee0\ufe0f Development Workflow","text":""},{"location":"user/getting-started/#local-development-with-cli","title":"Local Development with CLI","text":"<pre><code># 1. Setup development environment\nmake dev-setup\npip install -e .\n\n# 2. Start model server\nvllm serve your-model --port 8000\n\n# 3. Develop and test\nvllm-eval run evalchemy your-model --dry-run\nvllm-eval run evalchemy your-model --batch-size 2\n\n# 4. View and analyze results\nvllm-eval results list\nvllm-eval results show &lt;run-id&gt; --format json\n</code></pre>"},{"location":"user/getting-started/#production-deployment","title":"Production Deployment","text":"<pre><code># 1. Test locally with production config\nvllm-eval --profile production run all your-model --dry-run\n\n# 2. Generate Kubernetes jobs from CLI config\nvllm-eval config export --format k8s &gt; evaluation-job.yaml\n\n# 3. Deploy to Kubernetes\nkubectl apply -f evaluation-job.yaml\n\n# 4. Monitor execution\nkubectl logs -f job/evaluation-job\n</code></pre>"},{"location":"user/getting-started/#framework-overview","title":"\ud83d\udcda Framework Overview","text":""},{"location":"user/getting-started/#evalchemy-academic-benchmarks","title":"Evalchemy - Academic Benchmarks","text":"<p>Best for: Standard model evaluation</p> <pre><code># Run comprehensive academic benchmarks\nvllm-eval run evalchemy my-model --tasks mmlu,arc_easy,hellaswag\n</code></pre> <p>Provides: MMLU, ARC, HellaSwag, HumanEval accuracy metrics</p>"},{"location":"user/getting-started/#nvidia-eval-mathematical-reasoning","title":"NVIDIA Eval - Mathematical Reasoning","text":"<p>Best for: Mathematical and coding capabilities</p> <pre><code># Test mathematical reasoning\nvllm-eval run nvidia my-model --benchmark aime\n\n# Test coding capabilities  \nvllm-eval run nvidia my-model --benchmark livecodebench\n</code></pre> <p>Provides: Problem-solving accuracy, reasoning analysis</p>"},{"location":"user/getting-started/#vllm-benchmark-performance-testing","title":"VLLM Benchmark - Performance Testing","text":"<p>Best for: Performance and throughput analysis</p> <pre><code># Performance benchmarking\nvllm-eval run vllm-benchmark my-model --scenario performance\n</code></pre> <p>Provides: TTFT, TPOT, throughput, latency metrics</p>"},{"location":"user/getting-started/#deepeval-custom-metrics","title":"Deepeval - Custom Metrics","text":"<p>Best for: RAG evaluation and custom quality metrics</p> <pre><code># RAG evaluation\nvllm-eval run deepeval my-model --suite rag --metrics faithfulness,relevancy\n</code></pre> <p>Provides: Context relevance, faithfulness, custom metrics</p>"},{"location":"user/getting-started/#quick-reference","title":"\u26a1 Quick Reference","text":""},{"location":"user/getting-started/#essential-commands","title":"Essential Commands","text":"<pre><code># Setup and diagnostics\nvllm-eval setup                    # Initial setup wizard\nvllm-eval doctor                   # System diagnostics\nvllm-eval config validate          # Validate configuration\n\n# Running evaluations\nvllm-eval run evalchemy my-model   # Academic benchmarks\nvllm-eval run all my-model         # All enabled frameworks\nvllm-eval run all my-model --parallel  # Parallel execution\n\n# Configuration management\nvllm-eval config show             # View current config\nvllm-eval config create prod      # Create new profile\nvllm-eval --profile prod run ...  # Use specific profile\n\n# Results management\nvllm-eval results list            # List all results\nvllm-eval results show &lt;id&gt;       # Show detailed results\nvllm-eval results export &lt;id&gt;     # Export results\n</code></pre>"},{"location":"user/getting-started/#common-workflows","title":"Common Workflows","text":"<pre><code># Development workflow\nvllm-eval setup\nvllm-eval run evalchemy my-model --dry-run\nvllm-eval run evalchemy my-model --batch-size 2\nvllm-eval results show &lt;run-id&gt;\n\n# Production workflow\nvllm-eval --profile production config validate\nvllm-eval --profile production run all my-model\nvllm-eval results export &lt;run-id&gt; --format csv\n\n# CI/CD workflow\nvllm-eval --profile ci run evalchemy my-model --timeout 3600\nvllm-eval results show &lt;run-id&gt; --format json &gt; results.json\n</code></pre>"},{"location":"user/getting-started/#whats-next","title":"\ud83d\udd17 What's Next?","text":""},{"location":"user/getting-started/#explore-documentation","title":"Explore Documentation","text":"<ul> <li>CLI Guide - Comprehensive CLI documentation</li> <li>API Reference - Framework-specific details</li> <li>Benchmark Configuration - Advanced configuration</li> <li>Result Interpretation - Understanding your results</li> </ul>"},{"location":"user/getting-started/#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Custom Adapters: Extend the system with your own evaluation frameworks</li> <li>Batch Processing: Orchestrate complex evaluation workflows</li> <li>CI/CD Integration: Automated evaluation in your deployment pipeline</li> <li>Performance Tuning: Optimize for your specific use case</li> </ul>"},{"location":"user/getting-started/#get-help","title":"Get Help","text":"<ul> <li>Built-in Help: <code>vllm-eval --help</code> and <code>vllm-eval COMMAND --help</code></li> <li>System Diagnostics: <code>vllm-eval doctor --verbose</code></li> <li>Configuration Issues: <code>vllm-eval config validate --test-endpoints</code></li> <li>Community: Check project issues and discussions</li> </ul> <p>You're Ready!</p> <p>You now have a working VLLM Evaluation setup. Start with simple evaluations using the CLI, then explore advanced features as your needs grow.</p> <p>Quick Start: <code>vllm-eval run evalchemy my-model</code></p> <p>Next: Explore CLI Commands for detailed usage patterns.</p>"}]}