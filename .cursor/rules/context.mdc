---
description: 
globs: 
alwaysApply: true
---
# Context

## 프로젝트 개요
VLLM 모델 성능 자동 평가 시스템은 쿠버네티스 환경에서 **VLLM** 서빙 모델의 품질을 지속적으로 모니터링하고 평가하는 CI/CD 파이프라인입니다.

## 핵심 배경
- **VLLM**: GPU 메모리 효율을 극대화한 PagedAttention Inference 엔진으로, 버전 업마다 품질 변동 가능성이 높아 자동화된 Regression Test가 필수
- **Deepeval**: PyTest-스타일 단위-테스트형 LLM 평가 프레임워크 (30+ Metrics, RAG & E2E 평가 지원)
- **Evalchemy**: EleutherAI lm-evaluation-harness 기반 Unified Benchmark Runner (표준 벤치마크와 API-기반 모델 지원)

## 비즈니스 임팩트
- **품질 보장**: 모델 릴리스마다 객관적 품질 지표 확보
- **리스크 완화**: 품질 퇴화(regression) 즉시 탐지
- **운영 효율성**: Microsoft Teams를 통한 실시간 알림으로 신속한 대응 가능
- **자동화**: 수동 테스트 대비 시간과 인력 비용 대폭 절감

## 이해관계자
- **제품 오너**: 모델 품질 KPI 모니터링 및 의사결정
- **ML Ops**: 파이프라인 운영 및 성능 최적화
- **리서치 팀**: 평가 메트릭 개발 및 검증
- **플랫폼 Ops**: 인프라 안정성 및 확장성 관리

## 성공 정의
- 릴리스-to-리포트 지연 ≤ 2시간
- 품질 퇴화 감지율 ≥ 95%
- 파이프라인 안정성 실패 < 1%/월
