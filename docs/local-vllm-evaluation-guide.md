# ğŸš€ macOS OrbStack í™˜ê²½ì—ì„œ VLLM ë¡œì»¬ í‰ê°€ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” macOSì˜ OrbStack í™˜ê²½ì—ì„œ VLLM ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³  LLM í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. ğŸ›  ì‚¬ì „ ìš”êµ¬ì‚¬í•­
2. ğŸ”§ OrbStack ì„¤ì¹˜ ë° ì„¤ì •
3. ğŸ¤– VLLM ì„œë²„ ì‹¤í–‰
4. ğŸ”¬ í‰ê°€ í™˜ê²½ êµ¬ì¶•
5. ğŸ§ª Deepeval í‰ê°€ ì‹¤í–‰
6. ğŸ“š Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
7. ğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
8. ğŸš« ë¬¸ì œ í•´ê²°

## ğŸ›  ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **macOS**: 13.0 ì´ìƒ (Apple Silicon ê¶Œì¥)
- **RAM**: ìµœì†Œ 16GB, ê¶Œì¥ 32GB
- **ë””ìŠ¤í¬**: ìµœì†Œ 20GB ì—¬ìœ  ê³µê°„
- **GPU**: Apple Silicon GPU ë˜ëŠ” NVIDIA GPU (ì„ íƒì‚¬í•­)

### í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜

```bash
# Homebrew ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# í•„ìˆ˜ ë„êµ¬ë“¤ ì„¤ì¹˜
brew install python@3.11 git curl jq
```

## ğŸ”§ OrbStack ì„¤ì¹˜ ë° ì„¤ì •

### 1. OrbStack ì„¤ì¹˜

```bash
# OrbStack ì„¤ì¹˜ (Docker Desktop ëŒ€ì‹  ê¶Œì¥)
brew install --cask orbstack

# OrbStack ì‹œì‘
open -a OrbStack

# OrbStack ì‹œì‘ í™•ì¸
while ! docker info > /dev/null 2>&1; do
    echo "OrbStack ì‹œì‘ ëŒ€ê¸° ì¤‘..."
    sleep 3
done
echo "âœ… OrbStackì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
```

### 2. í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-org/vllm-eval.git
cd vllm-eval

# Python ê°€ìƒí™˜ê²½ ìƒì„±
python3.11 -m venv venv
source venv/bin/activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements-dev.txt
pip install -r requirements-deepeval.txt
pip install -r requirements-evalchemy.txt
```

## ğŸ¤– VLLM ì„œë²„ ì‹¤í–‰

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰

```bash
# VLLM ì„œë²„ ì‹¤í–‰ (ì˜ˆ: Qwen2-7B ëª¨ë¸)
docker run -d \
  --name vllm-server \
  --gpus all \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model "Qwen/Qwen2-7B-Instruct" \
  --served-model-name "qwen3-8b" \
  --host 0.0.0.0 \
  --port 8000

# ì„œë²„ ìƒíƒœ í™•ì¸
docker logs vllm-server

# API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-8b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"}
    ],
    "temperature": 0.7,
    "max_tokens": 256,
    "top_p": 0.95,
    "stream": false
  }'
```

### 2. ëª¨ë¸ ì„œë²„ í—¬ìŠ¤ì²´í¬

```bash
# ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:8000/v1/models | jq

# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health
```

## ğŸ”¬ í‰ê°€ í™˜ê²½ êµ¬ì¶•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << 'EOF'
# VLLM ëª¨ë¸ ì—”ë“œí¬ì¸íŠ¸
VLLM_MODEL_ENDPOINT=http://localhost:8000/v1
MODEL_NAME=qwen3-8b

# í‰ê°€ ì„¤ì •
EVAL_CONFIG_PATH=configs/evalchemy.json
OUTPUT_DIR=./test_results
RUN_ID=local_eval_$(date +%Y%m%d_%H%M%S)

# ë¡œê·¸ ì„¤ì •
LOG_LEVEL=INFO
PYTHONPATH=.
EOF

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source .env
```

### 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„

```bash
# ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p test_results

# í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
mkdir -p datasets/raw/local_test_dataset
cat > datasets/raw/local_test_dataset/test.jsonl << 'EOF'
{"input": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?", "expected_output": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.", "context": "í•œêµ­ ì§€ë¦¬ì— ê´€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."}
{"input": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?", "expected_output": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ë ¤ë©´ sort() ë©”ì„œë“œë‚˜ sorted() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "context": "í”„ë¡œê·¸ë˜ë° ê´€ë ¨ ì§ˆë¬¸ì…ë‹ˆë‹¤."}
{"input": "ì§€êµ¬ì˜ ë‘˜ë ˆëŠ” ì–¼ë§ˆë‚˜ ë©ë‹ˆê¹Œ?", "expected_output": "ì§€êµ¬ì˜ ë‘˜ë ˆëŠ” ì•½ 40,075kmì…ë‹ˆë‹¤.", "context": "ì§€êµ¬ê³¼í•™ì— ê´€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."}
EOF
```

## ğŸ§ª Deepeval í‰ê°€ ì‹¤í–‰

### 1. ì»¤ìŠ¤í…€ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

```bash
# ë¡œì»¬ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > scripts/run_local_deepeval.py << 'EOF'
#!/usr/bin/env python3
"""
ë¡œì»¬ VLLM ì„œë²„ë¥¼ ì´ìš©í•œ Deepeval í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import asyncio
from typing import List, Dict, Any
from deepeval import evaluate
from deepeval.models import DeepEvalBaseLLM
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    ContextualPrecisionMetric,
    ContextualRecallMetric,
    ContextualRelevancyMetric,
    AnswerRelevancyMetric
)
import openai
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VLLMModel(DeepEvalBaseLLM):
    """VLLM OpenAI í˜¸í™˜ APIë¥¼ ìœ„í•œ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "qwen3-8b", base_url: str = "http://localhost:8000/v1"):
        self.model_name = model_name
        self.client = openai.OpenAI(
            base_url=base_url,
            api_key="dummy"  # VLLMì—ì„œëŠ” API í‚¤ê°€ í•„ìš”ì—†ìŒ
        )
    
    def load_model(self):
        return self.model_name
    
    def generate(self, prompt: str, schema: Dict = None) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=512
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            return ""
    
    async def a_generate(self, prompt: str, schema: Dict = None) -> str:
        """ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ìƒì„±"""
        return self.generate(prompt, schema)
    
    def get_model_name(self) -> str:
        return self.model_name

def load_test_dataset(file_path: str) -> List[Dict[str, Any]]:
    """JSONL í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
    test_cases = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            test_cases.append(json.loads(line.strip()))
    return test_cases

def create_test_cases(dataset: List[Dict], model: VLLMModel) -> List[LLMTestCase]:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
    test_cases = []
    
    for item in dataset:
        # ëª¨ë¸ë¡œë¶€í„° ì‹¤ì œ ì‘ë‹µ ìƒì„±
        actual_output = model.generate(item["input"])
        
        test_case = LLMTestCase(
            input=item["input"],
            actual_output=actual_output,
            expected_output=item["expected_output"],
            context=[item.get("context", "")]
        )
        test_cases.append(test_case)
        logger.info(f"Created test case: {item['input'][:50]}...")
    
    return test_cases

def main():
    """ë©”ì¸ í‰ê°€ ì‹¤í–‰"""
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = VLLMModel()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_path = "eval/deepeval_tests/datasets/test_local_dataset.jsonl"
    dataset = load_test_dataset(dataset_path)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
    test_cases = create_test_cases(dataset, model)
    
    # í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜
    metrics = [
        AnswerRelevancyMetric(
            threshold=0.7,
            model=model,
            include_reason=True
        ),
        ContextualRelevancyMetric(
            threshold=0.7,
            model=model,
            include_reason=True
        )
    ]
    
    # í‰ê°€ ì‹¤í–‰
    logger.info("Starting evaluation...")
    results = evaluate(
        test_cases=test_cases,
        metrics=metrics,
        print_results=True
    )
    
    # ê²°ê³¼ ì €ì¥
    output_dir = os.getenv("OUTPUT_DIR", "./test_results")
    os.makedirs(output_dir, exist_ok=True)
    
    results_file = f"{output_dir}/deepeval_results_{os.getenv('RUN_ID', 'local')}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_results": [
                {
                    "input": tc.input,
                    "actual_output": tc.actual_output,
                    "expected_output": tc.expected_output,
                    "metrics": {
                        metric.__class__.__name__: {
                            "score": getattr(metric, 'score', None),
                            "threshold": getattr(metric, 'threshold', None),
                            "success": getattr(metric, 'success', None),
                            "reason": getattr(metric, 'reason', None)
                        }
                        for metric in metrics
                    }
                }
                for tc in test_cases
            ]
        }, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Results saved to: {results_file}")
    return results

if __name__ == "__main__":
    main()
EOF

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x scripts/run_local_deepeval.py
```

### 2. Deepeval ì‹¤í–‰

```bash
# Deepeval í‰ê°€ ì‹¤í–‰
python scripts/run_local_deepeval.py

# ê²°ê³¼ í™•ì¸
ls -la test_results/
cat test_results/deepeval_results_*.json | jq
```

## âš¡ Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

### 1. ë¡œì»¬ Evalchemy ì„¤ì •

```bash
# ë¡œì»¬ìš© Evalchemy ì„¤ì • íŒŒì¼ ìƒì„±
cat > eval/evalchemy/configs/local_eval_config.json << 'EOF'
{
  "benchmarks": {
    "arc_easy": {
      "enabled": true,
      "tasks": ["arc_easy"],
      "num_fewshot": 5,
      "batch_size": 4,
      "limit": 10,
      "description": "ARC Easy ë²¤ì¹˜ë§ˆí¬ (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)",
      "metrics": ["acc", "acc_norm"]
    },
    "hellaswag": {
      "enabled": true,
      "tasks": ["hellaswag"],
      "num_fewshot": 10,
      "batch_size": 4,
      "limit": 10,
      "description": "HellaSwag ë²¤ì¹˜ë§ˆí¬ (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)",
      "metrics": ["acc", "acc_norm"]
    }
  }
}
EOF
```

### 2. ë¡œì»¬ Evalchemy ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```bash
# ë¡œì»¬ Evalchemy ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > scripts/run_local_evalchemy.py << 'EOF'
#!/usr/bin/env python3
"""
ë¡œì»¬ VLLM ì„œë²„ë¥¼ ì´ìš©í•œ Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
"""

import os
import json
import subprocess
import logging
from typing import Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_evalchemy_benchmark(config_path: str, output_dir: str) -> Dict[str, Any]:
    """Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    env = os.environ.copy()
    env.update({
        "VLLM_MODEL_ENDPOINT": "http://localhost:8000/v1",
        "MODEL_NAME": "qwen3-8b",
        "OUTPUT_DIR": output_dir,
        "EVAL_CONFIG_PATH": config_path
    })
    
    # lm_eval ëª…ë ¹ì–´ êµ¬ì„±
    cmd = [
        "lm_eval",
        "--model", "openai-chat-completions",
        "--model_args", f"base_url=http://localhost:8000/v1,model={env['MODEL_NAME']},tokenizer={env['MODEL_NAME']}",
        "--tasks", "arc_easy,hellaswag",
        "--num_fewshot", "5",
        "--batch_size", "4",
        "--limit", "10",
        "--output_path", f"{output_dir}/evalchemy_results.json",
        "--log_samples"
    ]
    
    logger.info(f"Running command: {' '.join(cmd)}")
    
    try:
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode == 0:
            logger.info("Evalchemy benchmark completed successfully")
            
            # ê²°ê³¼ íŒŒì¼ ì½ê¸°
            results_file = f"{output_dir}/evalchemy_results.json"
            if os.path.exists(results_file):
                with open(results_file, 'r') as f:
                    results = json.load(f)
                return results
            else:
                logger.warning("Results file not found")
                return {}
        else:
            logger.error(f"Benchmark failed with return code: {result.returncode}")
            logger.error(f"Error output: {result.stderr}")
            return {}
            
    except subprocess.TimeoutExpired:
        logger.error("Benchmark timed out")
        return {}
    except Exception as e:
        logger.error(f"Benchmark failed with exception: {e}")
        return {}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config_path = "eval/evalchemy/configs/local_eval_config.json"
    output_dir = os.getenv("OUTPUT_DIR", "./test_results")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = run_evalchemy_benchmark(config_path, output_dir)
    
    if results:
        logger.info("Benchmark results:")
        for task, metrics in results.get("results", {}).items():
            logger.info(f"  {task}: {metrics}")
    else:
        logger.error("No results obtained")

if __name__ == "__main__":
    main()
EOF

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x scripts/run_local_evalchemy.py
```

### 3. Evalchemy ì‹¤í–‰

```bash
# lm-evaluation-harness ì„¤ì¹˜ (í•„ìš”í•œ ê²½ìš°)
pip install lm-eval[openai]

# Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python scripts/run_local_evalchemy.py

# ê²°ê³¼ í™•ì¸
ls -la test_results/
cat test_results/evalchemy_results.json | jq
```

## ğŸ“Š ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

### 1. ê²°ê³¼ ì§‘ê³„ ìŠ¤í¬ë¦½íŠ¸

```bash
# ê²°ê³¼ ì§‘ê³„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > scripts/aggregate_local_results.py << 'EOF'
#!/usr/bin/env python3
"""
ë¡œì»¬ í‰ê°€ ê²°ê³¼ ì§‘ê³„ ë° ì‹œê°í™”
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_deepeval_results(results_dir: str) -> dict:
    """Deepeval ê²°ê³¼ ë¡œë“œ"""
    results = {}
    for file in os.listdir(results_dir):
        if file.startswith("deepeval_results_") and file.endswith(".json"):
            with open(os.path.join(results_dir, file), 'r') as f:
                results[file] = json.load(f)
    return results

def load_evalchemy_results(results_dir: str) -> dict:
    """Evalchemy ê²°ê³¼ ë¡œë“œ"""
    results = {}
    for file in os.listdir(results_dir):
        if file.startswith("evalchemy_results") and file.endswith(".json"):
            with open(os.path.join(results_dir, file), 'r') as f:
                results[file] = json.load(f)
    return results

def create_summary_report(deepeval_results: dict, evalchemy_results: dict, output_dir: str):
    """í†µí•© ë³´ê³ ì„œ ìƒì„±"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "model_name": os.getenv("MODEL_NAME", "unknown"),
        "summary": {
            "deepeval": {},
            "evalchemy": {}
        }
    }
    
    # Deepeval ê²°ê³¼ ìš”ì•½
    if deepeval_results:
        for filename, data in deepeval_results.items():
            test_results = data.get("test_results", [])
            if test_results:
                # ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
                metrics_summary = {}
                for result in test_results:
                    for metric_name, metric_data in result.get("metrics", {}).items():
                        if metric_name not in metrics_summary:
                            metrics_summary[metric_name] = []
                        if metric_data.get("score") is not None:
                            metrics_summary[metric_name].append(metric_data["score"])
                
                # í‰ê·  ê³„ì‚°
                avg_metrics = {}
                for metric_name, scores in metrics_summary.items():
                    if scores:
                        avg_metrics[metric_name] = {
                            "average_score": sum(scores) / len(scores),
                            "count": len(scores)
                        }
                
                report["summary"]["deepeval"][filename] = avg_metrics
    
    # Evalchemy ê²°ê³¼ ìš”ì•½
    if evalchemy_results:
        for filename, data in evalchemy_results.items():
            results = data.get("results", {})
            summary = {}
            for task, metrics in results.items():
                summary[task] = {
                    "accuracy": metrics.get("acc", 0),
                    "normalized_accuracy": metrics.get("acc_norm", 0)
                }
            report["summary"]["evalchemy"][filename] = summary
    
    # ë³´ê³ ì„œ ì €ì¥
    report_file = f"{output_dir}/evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Summary report saved to: {report_file}")
    return report

def create_visualizations(report: dict, output_dir: str):
    """ê²°ê³¼ ì‹œê°í™”"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # Deepeval ê²°ê³¼ ì‹œê°í™”
        deepeval_data = report["summary"]["deepeval"]
        if deepeval_data:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'Local VLLM Evaluation Results - {report["model_name"]}', fontsize=16)
            
            # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ì‹œê°í™”
            all_scores = []
            all_metrics = []
            
            for filename, metrics in deepeval_data.items():
                for metric_name, metric_data in metrics.items():
                    all_scores.append(metric_data["average_score"])
                    all_metrics.append(metric_name.replace("Metric", ""))
            
            if all_scores:
                axes[0, 0].bar(all_metrics, all_scores)
                axes[0, 0].set_title('Deepeval Metrics Scores')
                axes[0, 0].set_ylabel('Score')
                axes[0, 0].tick_params(axis='x', rotation=45)
        
        # Evalchemy ê²°ê³¼ ì‹œê°í™”
        evalchemy_data = report["summary"]["evalchemy"]
        if evalchemy_data:
            tasks = []
            accuracies = []
            
            for filename, results in evalchemy_data.items():
                for task, metrics in results.items():
                    tasks.append(task)
                    accuracies.append(metrics["accuracy"])
            
            if tasks:
                axes[0, 1].bar(tasks, accuracies)
                axes[0, 1].set_title('Evalchemy Benchmark Accuracies')
                axes[0, 1].set_ylabel('Accuracy')
                axes[0, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        chart_file = f"{output_dir}/evaluation_charts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        logger.info(f"Charts saved to: {chart_file}")
        
    except ImportError:
        logger.warning("matplotlib/seaborn not installed, skipping visualization")
    except Exception as e:
        logger.error(f"Visualization failed: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    output_dir = os.getenv("OUTPUT_DIR", "./test_results")
    
    # ê²°ê³¼ ë¡œë“œ
    deepeval_results = load_deepeval_results(output_dir)
    evalchemy_results = load_evalchemy_results(output_dir)
    
    # í†µí•© ë³´ê³ ì„œ ìƒì„±
    report = create_summary_report(deepeval_results, evalchemy_results, output_dir)
    
    # ì‹œê°í™”
    create_visualizations(report, output_dir)
    
    # ì½˜ì†” ì¶œë ¥
    print("\n=== Local VLLM Evaluation Summary ===")
    print(f"Model: {report['model_name']}")
    print(f"Timestamp: {report['timestamp']}")
    
    if report["summary"]["deepeval"]:
        print("\n--- Deepeval Results ---")
        for filename, metrics in report["summary"]["deepeval"].items():
            print(f"File: {filename}")
            for metric_name, data in metrics.items():
                print(f"  {metric_name}: {data['average_score']:.3f} (n={data['count']})")
    
    if report["summary"]["evalchemy"]:
        print("\n--- Evalchemy Results ---")
        for filename, results in report["summary"]["evalchemy"].items():
            print(f"File: {filename}")
            for task, metrics in results.items():
                print(f"  {task}: {metrics['accuracy']:.3f}")

if __name__ == "__main__":
    main()
EOF

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x scripts/aggregate_local_results.py
```

### 2. ê²°ê³¼ ì§‘ê³„ ì‹¤í–‰

```bash
# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install matplotlib seaborn pandas

# ê²°ê³¼ ì§‘ê³„ ë° ì‹œê°í™”
python scripts/aggregate_local_results.py

# ìƒì„±ëœ íŒŒì¼ í™•ì¸
ls -la test_results/evaluation_*
```

## ğŸ”§ í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```bash
# ì „ì²´ ë¡œì»¬ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > scripts/run_full_local_evaluation.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸš€ ë¡œì»¬ VLLM í‰ê°€ ì‹œì‘"
echo "===================="

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source .env

# 1. VLLM ì„œë²„ ìƒíƒœ í™•ì¸
echo "ğŸ“¡ VLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘..."
if ! curl -f http://localhost:8000/health > /dev/null 2>&1; then
    echo "âŒ VLLM ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”."
    exit 1
fi
echo "âœ… VLLM ì„œë²„ ì •ìƒ ì‘ë™"

# 2. ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p $OUTPUT_DIR

# 3. Deepeval ì‹¤í–‰
echo "ğŸ§ª Deepeval í‰ê°€ ì‹¤í–‰ ì¤‘..."
python scripts/run_local_deepeval.py
echo "âœ… Deepeval ì™„ë£Œ"

# 4. Evalchemy ì‹¤í–‰
echo "âš¡ Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘..."
python scripts/run_local_evalchemy.py
echo "âœ… Evalchemy ì™„ë£Œ"

# 5. ê²°ê³¼ ì§‘ê³„
echo "ğŸ“Š ê²°ê³¼ ì§‘ê³„ ë° ì‹œê°í™” ì¤‘..."
python scripts/aggregate_local_results.py
echo "âœ… ê²°ê³¼ ì§‘ê³„ ì™„ë£Œ"

# 6. ê²°ê³¼ ì¶œë ¥
echo ""
echo "ğŸ‰ ë¡œì»¬ VLLM í‰ê°€ ì™„ë£Œ!"
echo "===================="
echo "ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: $OUTPUT_DIR"
echo "ì£¼ìš” íŒŒì¼:"
echo "  - Deepeval ê²°ê³¼: $OUTPUT_DIR/deepeval_results_*.json"
echo "  - Evalchemy ê²°ê³¼: $OUTPUT_DIR/evalchemy_results.json"
echo "  - í†µí•© ë³´ê³ ì„œ: $OUTPUT_DIR/evaluation_summary_*.json"
echo "  - ì‹œê°í™” ì°¨íŠ¸: $OUTPUT_DIR/evaluation_charts_*.png"
EOF

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x scripts/run_full_local_evaluation.sh
```

## ğŸš¨ ë¬¸ì œ í•´ê²°

### 1. VLLM ì„œë²„ ê´€ë ¨ ë¬¸ì œ

```bash
# ì„œë²„ ë¡œê·¸ í™•ì¸
docker logs vllm-server

# ì„œë²„ ì¬ì‹œì‘
docker restart vllm-server

# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :8000
```

### 2. Python ì˜ì¡´ì„± ë¬¸ì œ

```bash
# ê°€ìƒí™˜ê²½ ì¬ìƒì„±
deactivate
rm -rf venv
python3.11 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements-dev.txt
```

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

```bash
# Docker ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
docker run -d \
  --name vllm-server \
  --memory="8g" \
  --gpus all \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model "Qwen/Qwen2-7B-Instruct" \
  --gpu-memory-utilization 0.8
```

### 4. í‰ê°€ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ê²½ìš°

```bash
# ë¡œê·¸ ë ˆë²¨ ë³€ê²½
export LOG_LEVEL=DEBUG

# ë‹¨ê³„ë³„ ë””ë²„ê¹…
python -c "
import openai
client = openai.OpenAI(base_url='http://localhost:8000/v1', api_key='dummy')
response = client.chat.completions.create(
    model='qwen3-8b',
    messages=[{'role': 'user', 'content': 'Hello!'}]
)
print(response.choices[0].message.content)
"
```

## ğŸ¯ ì‹¤í–‰ ìš”ì•½

### ë°©ë²• 1: í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)

```bash
# í•œ ë²ˆì— ëª¨ë“  ê²ƒì„ ì‹¤í–‰ (VLLM ì„œë²„ ìë™ ê°ì§€)
./scripts/run_complete_local_evaluation.sh
```

### ë°©ë²• 2: ìˆ˜ë™ ë‹¨ê³„ë³„ ì‹¤í–‰

```bash
# 1. VLLM ì„œë²„ ì‹œì‘ (ì„ íƒì‚¬í•­)
docker run -d \
  --name vllm-server \
  --gpus all \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model "Qwen/Qwen2-7B-Instruct" \
  --served-model-name "qwen3-8b"

# 2. ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python scripts/run_simple_deepeval_test.py      # Mock í…ŒìŠ¤íŠ¸
python scripts/run_vllm_deepeval_test.py        # ì‹¤ì œ VLLM í…ŒìŠ¤íŠ¸

# ì „ì²´ í‰ê°€ ì‹¤í–‰
python scripts/run_complete_local_evalchemy.py

# ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python scripts/run_simple_deepeval_test.py
python scripts/run_simple_evalchemy_test.py

# 3. ê²°ê³¼ í™•ì¸
cat test_results/*.json | jq
```

### ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
ğŸš€ macOS OrbStack VLLM ë¡œì»¬ í‰ê°€ í†µí•© ì‹¤í–‰
=============================================
ğŸ“‹ 1. í™˜ê²½ í™•ì¸
ğŸ“‹ 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
âœ… í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ì™„ë£Œ
ğŸ“‹ 3. ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
âœ… ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±: ./test_results
ğŸ“‹ 4. VLLM ì„œë²„ ìƒíƒœ í™•ì¸
âœ… VLLM ì„œë²„ ë°œê²¬: http://localhost:1234
ğŸ“‹ 5. ì‹¤ì œ VLLM ì„œë²„ë¡œ í‰ê°€ ì‹¤í–‰

ğŸ“Š ìµœì¢… ê²°ê³¼:
  ì´ í…ŒìŠ¤íŠ¸: 5
  í‰ê·  ì ìˆ˜: 0.50
  ì„±ê³µë¥ : 50.0%

âœ… ë¡œì»¬ VLLM í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
```

## ğŸ‰ ì •ë¦¬

ì´ ê°€ì´ë“œë¥¼ í†µí•´ macOS OrbStack í™˜ê²½ì—ì„œ VLLM ëª¨ë¸ì˜ ë¡œì»¬ í‰ê°€ë¥¼ ì™„ì „íˆ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### ğŸ”§ ì£¼ìš” ê¸°ëŠ¥
- **ìë™ í™˜ê²½ ê°ì§€**: VLLM ì„œë²„ ì—¬ëŸ¬ í¬íŠ¸ ìë™ íƒì§€
- **Mock ëª¨ë“œ ì§€ì›**: ì„œë²„ ì—†ì´ë„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- **í†µí•© ì‹¤í–‰**: í•œ ë²ˆì˜ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ í‰ê°€ ìˆ˜í–‰
- **ìƒì„¸í•œ ê²°ê³¼ ë¶„ì„**: JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ê²°ê³¼

### ğŸ“Š ìƒì„±ë˜ëŠ” ê²°ê³¼ íŒŒì¼
- `simple_deepeval_results.json`: Mock í…ŒìŠ¤íŠ¸ ê²°ê³¼
- `vllm_deepeval_results.json`: VLLM ì„œë²„ í…ŒìŠ¤íŠ¸ ê²°ê³¼

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„
ì´ ë¡œì»¬ í‰ê°€ ì‹œìŠ¤í…œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í™•ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:
- ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€
- ë‹¤ì–‘í•œ ëª¨ë¸ ë¹„êµ í‰ê°€
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í™•ì¥
- CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© 