# ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” VLLM í‰ê°€ ì‹œìŠ¤í…œì— ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ë°ì´í„°ì…‹ ì¤€ë¹„](#ë°ì´í„°ì…‹-ì¤€ë¹„)
- [ë²¤ì¹˜ë§ˆí¬ ìœ í˜•](#ë²¤ì¹˜ë§ˆí¬-ìœ í˜•)
- [Deepeval ë©”íŠ¸ë¦­ ì¶”ê°€](#deepeval-ë©”íŠ¸ë¦­-ì¶”ê°€)
- [Evalchemy ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€](#evalchemy-ë²¤ì¹˜ë§ˆí¬-ì¶”ê°€)
- [í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ¯ ê°œìš”

VLLM í‰ê°€ ì‹œìŠ¤í…œì€ ë‘ ê°€ì§€ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

- **Deepeval**: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ë° RAG í‰ê°€
- **Evalchemy**: í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ ë° ëŒ€ê·œëª¨ í‰ê°€

ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•  ë•ŒëŠ” í‰ê°€ ëª©ì ê³¼ íŠ¹ì„±ì— ë”°ë¼ ì ì ˆí•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„

### 1. JSONL í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±

ë¨¼ì € ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ JSONL í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤:

```bash
# ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p datasets/raw/custom_benchmark

# í›ˆë ¨ ë°ì´í„° ìƒì„± (train.jsonl)
cat > datasets/raw/custom_benchmark/train.jsonl << 'EOF'
{"question": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?", "answer": "ì„œìš¸", "context": "í•œêµ­ì€ ë™ì•„ì‹œì•„ì— ìœ„ì¹˜í•œ êµ­ê°€ì…ë‹ˆë‹¤.", "category": "geography", "difficulty": "easy"}
{"question": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?", "answer": "sort() ë©”ì„œë“œë‚˜ sorted() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", "context": "íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ì •ë ¬ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.", "category": "programming", "difficulty": "medium"}
{"question": "ì§€êµ¬ì—ì„œ ê°€ì¥ í° ëŒ€ë¥™ì€?", "answer": "ì•„ì‹œì•„", "context": "ì§€êµ¬ëŠ” 7ê°œì˜ ëŒ€ë¥™ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.", "category": "geography", "difficulty": "easy"}
{"question": "HTTPì™€ HTTPSì˜ ì°¨ì´ì ì€?", "answer": "HTTPSëŠ” HTTPì— SSL/TLS ì•”í˜¸í™”ê°€ ì¶”ê°€ëœ í”„ë¡œí† ì½œì…ë‹ˆë‹¤.", "context": "ì›¹ í†µì‹ ì—ì„œ ë³´ì•ˆì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.", "category": "technology", "difficulty": "medium"}
{"question": "ì…°ìµìŠ¤í”¼ì–´ì˜ ëŒ€í‘œì‘ì€?", "answer": "í–„ë¦¿, ë¡œë¯¸ì˜¤ì™€ ì¤„ë¦¬ì—£, ë§¥ë² ìŠ¤ ë“±ì´ ìˆìŠµë‹ˆë‹¤.", "context": "ì…°ìµìŠ¤í”¼ì–´ëŠ” ì˜êµ­ì˜ ëŒ€í‘œì ì¸ ê·¹ì‘ê°€ì…ë‹ˆë‹¤.", "category": "literature", "difficulty": "medium"}
EOF

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (test.jsonl)
cat > datasets/raw/custom_benchmark/test.jsonl << 'EOF'
{"question": "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?", "answer": "ë„ì¿„", "context": "ì¼ë³¸ì€ ë™ì•„ì‹œì•„ì˜ ì„¬ë‚˜ë¼ì…ë‹ˆë‹¤.", "category": "geography", "difficulty": "easy"}
{"question": "ìë°”ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°°ì—´ì„ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?", "answer": "sort() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", "context": "ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ê°œë°œì˜ í•µì‹¬ ì–¸ì–´ì…ë‹ˆë‹¤.", "category": "programming", "difficulty": "medium"}
{"question": "ì„¸ê³„ì—ì„œ ê°€ì¥ ê¸´ ê°•ì€?", "answer": "ë‚˜ì¼ê°•", "context": "ê°•ì€ ì§€êµ¬ì˜ ì¤‘ìš”í•œ ìˆ˜ìì›ì…ë‹ˆë‹¤.", "category": "geography", "difficulty": "medium"}
EOF
```

**JSONL í˜•ì‹ ì„¤ëª…**:
- **JSONL** = JSON Lines (ê° ì¤„ë§ˆë‹¤ í•˜ë‚˜ì˜ JSON ê°ì²´)
- ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹ì˜ í‘œì¤€ í˜•ì‹
- ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°€ëŠ¥

### 2. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±

```bash
mkdir -p datasets/manifests
cat > datasets/manifests/custom_benchmark_manifest.yaml << 'EOF'
name: "custom_benchmark"
version: "1.0"
description: "ì»¤ìŠ¤í…€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹"
license: "MIT"
language: ["ko", "en"]
domain: "general"
task_type: "text_generation"

source:
  type: "local"
  path: "datasets/raw/custom_benchmark"

splits:
  train:
    file: "raw/custom_benchmark/train.jsonl"
    size: 5
    sha256: "ê³„ì‚°ëœ_í•´ì‹œê°’"
  test:
    file: "raw/custom_benchmark/test.jsonl"
    size: 3
    sha256: "ê³„ì‚°ëœ_í•´ì‹œê°’"

schema:
  input_field: "question"
  output_field: "answer"
  context_field: "context"
  metadata_fields: ["category", "difficulty"]

evaluation:
  evalchemy:
    enabled: true
    tasks: ["custom_task_1", "custom_task_2"]
EOF
```

## ğŸ” ë²¤ì¹˜ë§ˆí¬ ìœ í˜•

### Deepeval ì í•© ë²¤ì¹˜ë§ˆí¬
- **RAG í‰ê°€**: ê²€ìƒ‰ ì¦ê°• ìƒì„± í’ˆì§ˆ ì¸¡ì •
- **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ì—…ë¬´/ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€
- **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ì¶˜ í‰ê°€
- **ì†Œê·œëª¨ ë°ì´í„°ì…‹**: < 1,000 ìƒ˜í”Œ

### Evalchemy ì í•© ë²¤ì¹˜ë§ˆí¬
- **í‘œì¤€ ë²¤ì¹˜ë§ˆí¬**: MMLU, ARC, HellaSwag ë“±
- **ëŒ€ê·œëª¨ í‰ê°€**: > 1,000 ìƒ˜í”Œ
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´
- **GPU ì§‘ì•½ì **: ëŒ€ìš©ëŸ‰ ëª¨ë¸ í‰ê°€

## ğŸ§ª Deepeval ë©”íŠ¸ë¦­ ì¶”ê°€

### 1. ë©”íŠ¸ë¦­ í´ë˜ìŠ¤ ìƒì„±

```bash
cat >> eval/deepeval_tests/metrics/custom_metric.py <<EOF
from deepeval.metrics import BaseMetric
from deepeval.test_case import LLMTestCase
from typing import Optional, List, Dict, Any
import asyncio

class CustomMetric(BaseMetric):
    """
    ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­
    
    Args:
        threshold (float): í†µê³¼ ê¸°ì¤€ ì ìˆ˜ (0.0 ~ 1.0)
        model (str): í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ëª…
        include_reason (bool): í‰ê°€ ì´ìœ  í¬í•¨ ì—¬ë¶€
    """
    
    def __init__(
        self,
        threshold: float = 0.7,
        model: Optional[str] = None,
        include_reason: bool = True,
        async_mode: bool = True
    ):
        self.threshold = threshold
        self.model = model
        self.include_reason = include_reason
        self.async_mode = async_mode
        
    def measure(self, test_case: LLMTestCase) -> float:
        """ë™ê¸° í‰ê°€ ì‹¤í–‰"""
        if self.async_mode:
            return asyncio.run(self.a_measure(test_case))
        return self._evaluate_sync(test_case)
    
    async def a_measure(self, test_case: LLMTestCase, _show_indicator: bool = True) -> float:
        """ë¹„ë™ê¸° í‰ê°€ ì‹¤í–‰"""
        return self._evaluate_sync(test_case)
    
    def _evaluate_sync(self, test_case: LLMTestCase) -> float:
        """ì‹¤ì œ í‰ê°€ ë¡œì§ êµ¬í˜„"""
        # ì—¬ê¸°ì— í‰ê°€ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤
        input_text = test_case.input
        actual_output = test_case.actual_output
        expected_output = test_case.expected_output
        
        # ì˜ˆì‹œ: ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°
        score = self._calculate_similarity(actual_output, expected_output)
        
        # ë©”íŠ¸ë¦­ ì†ì„± ì„¤ì •
        self.score = score
        self.success = score >= self.threshold
        
        if self.include_reason:
            self.reason = self._generate_reason(score, self.threshold)
        
        return score
    
    def _calculate_similarity(self, actual: str, expected: str) -> float:
        """ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë°©ë²•ì„ ì‚¬ìš©
        from difflib import SequenceMatcher
        return SequenceMatcher(None, actual, expected).ratio()
    
    def _generate_reason(self, score: float, threshold: float) -> str:
        """í‰ê°€ ì´ìœ  ìƒì„±"""
        if score >= threshold:
            return f"âœ… ì ìˆ˜ {score:.3f}ë¡œ ê¸°ì¤€ {threshold} ì´ìƒ ë‹¬ì„±"
        else:
            return f"âŒ ì ìˆ˜ {score:.3f}ë¡œ ê¸°ì¤€ {threshold} ë¯¸ë‹¬"
    
    def is_successful(self) -> bool:
        """í‰ê°€ ì„±ê³µ ì—¬ë¶€ ë°˜í™˜"""
        return self.success
    
    @property
    def __name__(self):
        return "Custom Metric"
EOF
```

### 2. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±

```bash
cat >> eval/deepeval_tests/test_custom_metric.py <<EOF
import pytest
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset
from .metrics.custom_metric import CustomMetric

class TestCustomMetric:
    """ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.parametrize("threshold", [0.5, 0.7, 0.9])
    def test_custom_metric_threshold(self, threshold):
        """ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
        test_case = LLMTestCase(
            input="í…ŒìŠ¤íŠ¸ ì…ë ¥",
            actual_output="ì‹¤ì œ ì¶œë ¥",
            expected_output="ì˜ˆìƒ ì¶œë ¥"
        )
        
        metric = CustomMetric(threshold=threshold)
        assert_test(test_case, [metric])
    
    def test_custom_metric_dataset(self):
        """ë°ì´í„°ì…‹ ê¸°ë°˜ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        dataset = EvaluationDataset(test_cases=[
            LLMTestCase(
                input="ì§ˆë¬¸ 1",
                actual_output="ë‹µë³€ 1",
                expected_output="ì •ë‹µ 1"
            ),
            LLMTestCase(
                input="ì§ˆë¬¸ 2", 
                actual_output="ë‹µë³€ 2",
                expected_output="ì •ë‹µ 2"
            )
        ])
        
        metric = CustomMetric(threshold=0.7)
        dataset.evaluate([metric])
        
        # ê²°ê³¼ ê²€ì¦
        assert len(dataset.test_cases) == 2
        for test_case in dataset.test_cases:
            assert hasattr(test_case, 'metrics_metadata')
EOF
```

### 3. ë©”íŠ¸ë¦­ ë“±ë¡

```bash
cat >> eval/deepeval_tests/metrics/__init__.py <<EOF
from .rag_precision import RAGPrecisionMetric
from .custom_metric import CustomMetric

# ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ ëª©ë¡
AVAILABLE_METRICS = {
    'rag_precision': RAGPrecisionMetric,
    'custom_metric': CustomMetric,
}

def get_metric(metric_name: str, **kwargs):
    """ë©”íŠ¸ë¦­ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    if metric_name not in AVAILABLE_METRICS:
        raise ValueError(f"Unknown metric: {metric_name}")
    
    return AVAILABLE_METRICS[metric_name](**kwargs)
EOF
```

## âš¡ Evalchemy ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€

Evalchemyì— ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì€ 3ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:
1.  **YAML íƒœìŠ¤í¬ ì •ì˜**: `lm-evaluation-harness`ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” YAML í˜•ì‹ìœ¼ë¡œ íƒœìŠ¤í¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
2.  **`eval_config.json` ë“±ë¡**: ìƒì„±í•œ íƒœìŠ¤í¬ë¥¼ ì¤‘ì•™ ì„¤ì • íŒŒì¼ì— ì¶”ê°€í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.
3.  **í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰**: `run_evalchemy.sh` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤.

### 1. íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ ë° ì´ˆê¸°í™” íŒŒì¼ ìƒì„±

ë¨¼ì € ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ë””ë ‰í† ë¦¬ì™€ `lm-evaluation-harness`ê°€ íƒœìŠ¤í¬ë¥¼ ì¸ì‹í•˜ëŠ” ë° í•„ìš”í•œ ì´ˆê¸°í™” íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ì²˜ìŒì— í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

```bash
# íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ ìƒì„± (eval/standard_evalchemy ë‚´ë¶€ì— ìœ„ì¹˜)
mkdir -p eval/standard_evalchemy/tasks

# __init__.py íŒŒì¼ ìƒì„± (íƒœìŠ¤í¬ ì¸ì‹ì„ ìœ„í•´ í•„ìš”)
cat > eval/standard_evalchemy/tasks/__init__.py << 'EOF'
"""
ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ ëª¨ë“ˆ

ì´ íŒŒì¼ì€ lm_evalì´ ì´ ë””ë ‰í† ë¦¬ë¥¼ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ê°€ í¬í•¨ëœ
íŒŒì´ì¬ íŒ¨í‚¤ì§€ë¡œ ì¸ì‹í•˜ë„ë¡ í•©ë‹ˆë‹¤. YAML ê¸°ë°˜ íƒœìŠ¤í¬ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•˜ë”ë¼ë„
ì´ íŒŒì¼ì€ í•„ìš”í•©ë‹ˆë‹¤.
"""
# íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ì„ì„ ëª…ì‹œ
__version__ = "1.0.0"
__author__ = "VLLM Eval Team"
EOF
```

### 2. YAML íƒœìŠ¤í¬ ì •ì˜ ìƒì„±

`lm-evaluation-harness` v0.4+ í‘œì¤€ì— ë”°ë¼ YAML íŒŒì¼ë¡œ ìƒˆë¡œìš´ íƒœìŠ¤í¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```bash
# custom_task_1.yaml ìƒì„±
cat > eval/standard_evalchemy/tasks/custom_task_1.yaml << 'EOF'
task: custom_task_1
test_split: test
fewshot_split: train
doc_to_text: "ì§ˆë¬¸: {{question}}\në‹µë³€:"
doc_to_target: "{{answer}}"
description: "ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ 1 - ê°„ë‹¨í•œ ì§ˆë¬¸ë‹µë³€"
dataset_path: json
dataset_kwargs:
  data_files:
    train: "../../../datasets/raw/custom_benchmark/train.jsonl"
    test: "../../../datasets/raw/custom_benchmark/test.jsonl"
output_type: generate_until
generation_kwargs:
  until: ["\n", "ì§ˆë¬¸:"]
  max_gen_toks: 256
filter_list:
  - name: "whitespace_cleanup"
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
metadata:
  version: 1.0
EOF
```

**ì£¼ìš” YAML ì„¤ì •**:
- `task`: íƒœìŠ¤í¬ì˜ ê³ ìœ  ì´ë¦„. `eval_config.json`ì—ì„œ ì´ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `dataset_path`: `json`ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ãƒ­ãƒ¼ã‚«ãƒ« JSON/JSONL íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `dataset_kwargs`: ë°ì´í„° íŒŒì¼ì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤. **ê²½ë¡œëŠ” `eval/standard_evalchemy/` ë””ë ‰í† ë¦¬ ê¸°ì¤€**ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
- `doc_to_text`/`doc_to_target`: ëª¨ë¸ì— ì…ë ¥ë  í”„ë¡¬í”„íŠ¸ í˜•ì‹ê³¼ ì •ë‹µ í•„ë“œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
- `metric_list`: í‰ê°€ì— ì‚¬ìš©í•  ë©”íŠ¸ë¦­ì„ ì •ì˜í•©ë‹ˆë‹¤.

### 3. `configs/eval_config.json`ì— íƒœìŠ¤í¬ ë“±ë¡

YAML íŒŒì¼ì„ ìƒì„±í•œ í›„, `run_evalchemy.sh` ìŠ¤í¬ë¦½íŠ¸ê°€ íƒœìŠ¤í¬ë¥¼ ì¸ì‹í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ `eval/standard_evalchemy/configs/eval_config.json` íŒŒì¼ì— ë“±ë¡í•©ë‹ˆë‹¤.

```bash
# eval/standard_evalchemy/configs/eval_config.json íŒŒì¼ì„ ì—´ì–´ "tasks" ì„¹ì…˜ì— ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
# "custom_task_1"ì€ yaml íŒŒì¼ì˜ 'task' í•„ë“œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
```

**`eval/standard_evalchemy/configs/eval_config.json` ìˆ˜ì • ì˜ˆì‹œ:**
```json
{
  "benchmarks": {
    // ...
    "custom_eval_group": {
      "enabled": true,
      "description": "ë‚´ê°€ ë§Œë“  ì»¤ìŠ¤í…€ ë²¤ì¹˜ë§ˆí¬ ê·¸ë£¹",
      "tasks": ["custom_task_1"]
    }
  },
  "tasks": {
    // ... ê¸°ì¡´ íƒœìŠ¤í¬ë“¤ ...
    "custom_task_1": {
      "enabled": true,
      "tasks": ["custom_task_1"],
      "num_fewshot": 0,
      "batch_size": 1,
      "limit": null,
      "description": "ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ 1 - ê°„ë‹¨í•œ ì§ˆë¬¸ë‹µë³€"
    }
  }
}
```
- **`tasks` ì„¹ì…˜**: `custom_task_1`ì´ë¼ëŠ” ìƒˆ í•­ëª©ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ í‚¤ ê°’(`"custom_task_1"`)ì€ `run_evalchemy.sh` ìŠ¤í¬ë¦½íŠ¸ ë‚´ì—ì„œ ì°¸ì¡°í•˜ëŠ” ì´ë¦„ì´ ë©ë‹ˆë‹¤. ë‚´ë¶€ì˜ `"tasks"` ë°°ì—´ì— ìˆëŠ” `custom_task_1`ì€ YAML íŒŒì¼ì— ì •ì˜ëœ `task` ì´ë¦„ì…ë‹ˆë‹¤.
- **`benchmarks` ì„¹ì…˜ (ì„ íƒ ì‚¬í•­)**: ì—¬ëŸ¬ íƒœìŠ¤í¬ë¥¼ `custom_eval_group`ê³¼ ê°™ì´ ë…¼ë¦¬ì ì¸ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ í•œ ë²ˆì— ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `enabled: true`ë¡œ ì„¤ì •í•´ì•¼ ì‹¤í–‰ë©ë‹ˆë‹¤.

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

ìƒˆë¡­ê²Œ ì¶”ê°€í•œ ë²¤ì¹˜ë§ˆí¬ê°€ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ëª¨ë“  í…ŒìŠ¤íŠ¸ëŠ” `eval/standard_evalchemy` ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.

### 1. ì„¤ì • íŒŒì¼ ë° íƒœìŠ¤í¬ ìœ íš¨ì„± ê²€ì‚¬

```bash
# ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd eval/standard_evalchemy

# ì„¤ì • íŒŒì¼(eval_config.json)ì´ ìœ íš¨í•œ JSONì¸ì§€ í™•ì¸
jq empty configs/eval_config.json && echo "âœ… eval_config.json is valid"

# ì¶”ê°€í•œ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ê°€ lm-evaluation-harnessì— ì˜í•´ ì¸ì‹ë˜ëŠ”ì§€ í™•ì¸
# --tasks list ì˜µì…˜ìœ¼ë¡œ ì „ì²´ íƒœìŠ¤í¬ ëª©ë¡ì„ í™•ì¸í•˜ê³  custom_task_1ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤.
./run_evalchemy.sh --tasks list | grep custom_task_1
```
> **ì°¸ê³ **: `run_evalchemy.sh`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ `lm_eval` ì‹¤í–‰ ì‹œ `--include_path tasks` ì˜µì…˜ì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ `tasks/` ë””ë ‰í† ë¦¬ì˜ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.

### 2. Dry Runìœ¼ë¡œ ì‹¤í–‰ ì¸ìˆ˜ í™•ì¸

ì‹¤ì œ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì—, `run_evalchemy.sh`ê°€ ìƒì„±í•˜ëŠ” `lm_eval` ëª…ë ¹ì–´ê°€ ì˜¬ë°”ë¥¸ì§€ `--dry-run` ì˜µì…˜ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.

```bash
# --dry-run ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ìƒì„±ë˜ëŠ” ëª…ë ¹ì–´ë§Œ ì¶œë ¥
# --run-idëŠ” ê²°ê³¼ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬ ì´ë¦„ì´ë¯€ë¡œ í…ŒìŠ¤íŠ¸ ëª©ì ì— ë§ê²Œ ì§€ì •í•©ë‹ˆë‹¤.
./run_evalchemy.sh --endpoint http://localhost/vllm/v1/completions --run-id test_custom_task_dry_run --dry-run
```

### 3. ì†ŒëŸ‰ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

`limit` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì ì€ ìˆ˜ì˜ ìƒ˜í”Œë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸ë¥¼ ì™„ë£Œí•˜ê³  ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

`configs/eval_config.json`ì—ì„œ `limit` ê°’ì„ `5` ì •ë„ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
```json
// ...
    "custom_task_1": {
      "enabled": true,
      "tasks": ["custom_task_1"],
      "limit": 5, // 5ê°œ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
// ...
```

í…ŒìŠ¤íŠ¸ ì‹¤í–‰:
```bash
# ì‹¤ì œ í‰ê°€ ì‹¤í–‰ (5ê°œ ìƒ˜í”Œ)
# --batch-size 1ì€ ì•ˆì •ì ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê¶Œì¥ë©ë‹ˆë‹¤.
./run_evalchemy.sh --endpoint http://localhost/vllm/v1/completions --run-id test_custom_task_limit5 --batch-size 1

# ì‹¤í–‰ í›„ ê²°ê³¼ í™•ì¸
cat results/test_custom_task_limit5/evalchemy_summary_test_custom_task_limit5.json | jq
```

### 4. ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤ì œ í‰ê°€ ì‹¤í–‰

í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´, `limit` ê°’ì„ `null`ë¡œ ë³€ê²½í•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

`configs/eval_config.json`ì—ì„œ `limit` ê°’ì„ `null`ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
```json
// ...
    "custom_task_1": {
      "enabled": true,
      "tasks": ["custom_task_1"],
      "limit": null, // ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
// ...
```

ì‹¤í–‰:
```bash
# í”„ë¡œë•ì…˜ìš© ì‹¤í–‰
./run_evalchemy.sh --endpoint http://your-vllm-server:8000/v1/completions --run-id custom_task_full_eval_$(date +%Y%m%d)
```
- `run_evalchemy.sh` ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ë¯¸ `--include_path tasks` ì˜µì…˜ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ìë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.

## ğŸ”§ ë¬¸ì œ í•´ê²° (Troubleshooting)

### 1. lm_evalì´ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°

**ì¦ìƒ**: `Task 'custom_task_1' not found` ì—ëŸ¬

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
# eval/standard_evalchemy ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
pwd

# 2ë‹¨ê³„: íƒœìŠ¤í¬ ëª©ë¡ í™•ì¸
./run_evalchemy.sh --tasks list | grep custom_task_1

# 3ë‹¨ê³„: YAML íŒŒì¼ êµ¬ë¬¸ ê²€ì‚¬
python3 -c "import yaml; print(yaml.safe_load(open('tasks/custom_task_1.yaml')))"

# 4ë‹¨ê³„: íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
ls -la tasks/
# ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìˆì–´ì•¼ í•¨:
# - __init__.py
# - custom_task_1.yaml
```

### 2. ë°ì´í„°ì…‹ ë¡œë”© ì—ëŸ¬

**ì¦ìƒ**: `Dataset 'custom_dataset' doesn't exist on the Hub` ë˜ëŠ” `FileNotFoundError`

**ì›ì¸**: YAML íŒŒì¼ì—ì„œ ì˜ëª»ëœ ë°ì´í„°ì…‹ ê²½ë¡œ ì°¸ì¡°

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: YAML íŒŒì¼ì˜ dataset_kwargs ê²½ë¡œ í™•ì¸
# ê²½ë¡œëŠ” eval/standard_evalchemy ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤.
cat tasks/custom_task_1.yaml | grep -A 2 data_files

# 2ë‹¨ê³„: ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸
ls -la ../../../datasets/raw/custom_benchmark/
# train.jsonlê³¼ test.jsonlì´ ìˆì–´ì•¼ í•¨
```

### 3. macOSì—ì„œ CUDA ì—ëŸ¬

**ì¦ìƒ**: `AssertionError: Torch not compiled with CUDA enabled`

**í•´ê²° ë°©ë²•**:
- `run_evalchemy.sh` ìŠ¤í¬ë¦½íŠ¸ëŠ” macOS í™˜ê²½ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  `--device cpu` ì˜µì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ë§Œì•½ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ `lm_eval`ì„ ì‹¤í–‰í•œë‹¤ë©´ `--device cpu` ì˜µì…˜ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
./run_evalchemy.sh --endpoint http://localhost/vllm/v1/completions

# ì§ì ‘ ì‹¤í–‰ ì‹œ
python3 -m lm_eval --device cpu ...
```

### 4. YAML íŒŒì¼ split ì—ëŸ¬

**ì¦ìƒ**: `KeyError: 'test'` ë˜ëŠ” `KeyError: 'validation'`

**ì›ì¸**: ë°ì´í„°ì…‹ì— í•´ë‹¹ splitì´ ì—†ê±°ë‚˜, JSONL íŒŒì¼ì˜ í‚¤ê°€ ì˜ëª» ì§€ì •ë¨

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: JSONL íŒŒì¼ ë‚´ìš© í™•ì¸ (í‚¤ê°€ 'train', 'test'ë¡œ ë˜ì–´ ìˆëŠ”ì§€)
# 2ë‹¨ê³„: YAML íŒŒì¼ì—ì„œ ì˜¬ë°”ë¥¸ split ì‚¬ìš© (test_split: test)
# 3ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥í•œ split í™•ì¸
python3 -c "
import datasets
ds = datasets.load_dataset('json', data_files={'train': '../../../datasets/raw/custom_benchmark/train.jsonl', 'test': '../../../datasets/raw/custom_benchmark/test.jsonl'})
print('Available splits:', list(ds.keys()))
"
# ìœ„ ì½”ë“œëŠ” eval/standard_evalchemy ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
```

### 5. ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì„ ë•Œ

**ì¦ìƒ**: `results/{run_id}` ë””ë ‰í† ë¦¬ëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ

**ì›ì¸**: VLLM ì—”ë“œí¬ì¸íŠ¸ ì—°ê²° ì‹¤íŒ¨, ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨ ë“±

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: ì—ëŸ¬ ë¡œê·¸ í™•ì¸
cat results/{run_id}/evalchemy_errors_{run_id}.log

# 2ë‹¨ê³„: VLLM ì„œë²„ ìƒíƒœ ë° ì—”ë“œí¬ì¸íŠ¸ URL í™•ì¸
curl http://your-vllm-server:8000/v1/models

# 3ë‹¨ê³„: --log-level DEBUG ì˜µì…˜ìœ¼ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸
./run_evalchemy.sh --endpoint ... --log-level DEBUG
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [lm-evaluation-harness v0.4+ Documentation](https://github.com/EleutherAI/lm-evaluation-harness)
- [Deepeval Documentation](https://docs.confident-ai.com/)
- [VLLM Documentation](https://docs.vllm.ai/)
- [Argo Workflows](https://argoproj.github.io/argo-workflows/)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•œ í›„ì—ëŠ”:

1. **ë¬¸ì„œ ì—…ë°ì´íŠ¸**: ì´ ê°€ì´ë“œì™€ README ì—…ë°ì´íŠ¸
2. **í…ŒìŠ¤íŠ¸ ì¶”ê°€**: ì¶©ë¶„í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
3. **PR ìƒì„±**: ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ Pull Request ìƒì„±
4. **ë¦¬ë·° ìš”ì²­**: íŒ€ ë©¤ë²„ë“¤ì˜ ì½”ë“œ ë¦¬ë·° ìš”ì²­

ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ë©´ ì´ìŠˆë¥¼ ìƒì„±í•˜ê±°ë‚˜ íŒ€ ì±„ë„ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”! ğŸš€

**ì£¼ì˜ì‚¬í•­**:
- ë°©ë²• 1: ì „ì²´ íƒœìŠ¤í¬ ëª©ë¡ì—ì„œ `custom_task_1`, `custom_task_2`ê°€ ë³´ì´ë©´ ì„±ê³µ
- ë°©ë²• 2: macOSì—ì„œëŠ” ë°˜ë“œì‹œ `--device cpu` ì˜µì…˜ í•„ìš” (CUDA ë¯¸ì§€ì›)
- `--limit 2`ëŠ” í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ì‹¤ì œ í‰ê°€ì—ì„œëŠ” ì œê±°í•´ì•¼ í•¨