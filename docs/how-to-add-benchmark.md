# ðŸŽ¯ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” VLLM í‰ê°€ ì‹œìŠ¤í…œì— ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ðŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ë°ì´í„°ì…‹ ì¤€ë¹„](#ë°ì´í„°ì…‹-ì¤€ë¹„)
- [ë²¤ì¹˜ë§ˆí¬ ìœ í˜•](#ë²¤ì¹˜ë§ˆí¬-ìœ í˜•)
- [Deepeval ë©”íŠ¸ë¦­ ì¶”ê°€](#deepeval-ë©”íŠ¸ë¦­-ì¶”ê°€)
- [Evalchemy ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€](#evalchemy-ë²¤ì¹˜ë§ˆí¬-ì¶”ê°€)
- [í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ðŸŽ¯ ê°œìš”

VLLM í‰ê°€ ì‹œìŠ¤í…œì€ ë‘ ê°€ì§€ í‰ê°€ í”„ë ˆìž„ì›Œí¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

- **Deepeval**: ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ë° RAG í‰ê°€
- **Evalchemy**: í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ ë° ëŒ€ê·œëª¨ í‰ê°€

ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•  ë•ŒëŠ” í‰ê°€ ëª©ì ê³¼ íŠ¹ì„±ì— ë”°ë¼ ì ì ˆí•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

## ðŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„

### 1. JSONL í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±

ë¨¼ì € ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ JSONL í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤:

```bash
# ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p datasets/raw/custom_benchmark

# í›ˆë ¨ ë°ì´í„° ìƒì„± (train.jsonl)
cat > datasets/raw/custom_benchmark/train.jsonl << 'EOF'
{"question": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?", "answer": "ì„œìš¸", "context": "í•œêµ­ì€ ë™ì•„ì‹œì•„ì— ìœ„ì¹˜í•œ êµ­ê°€ìž…ë‹ˆë‹¤.", "category": "geography", "difficulty": "easy"}
{"question": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?", "answer": "sort() ë©”ì„œë“œë‚˜ sorted() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", "context": "íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ì •ë ¬ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.", "category": "programming", "difficulty": "medium"}
{"question": "ì§€êµ¬ì—ì„œ ê°€ìž¥ í° ëŒ€ë¥™ì€?", "answer": "ì•„ì‹œì•„", "context": "ì§€êµ¬ëŠ” 7ê°œì˜ ëŒ€ë¥™ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.", "category": "geography", "difficulty": "easy"}
{"question": "HTTPì™€ HTTPSì˜ ì°¨ì´ì ì€?", "answer": "HTTPSëŠ” HTTPì— SSL/TLS ì•”í˜¸í™”ê°€ ì¶”ê°€ëœ í”„ë¡œí† ì½œìž…ë‹ˆë‹¤.", "context": "ì›¹ í†µì‹ ì—ì„œ ë³´ì•ˆì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.", "category": "technology", "difficulty": "medium"}
{"question": "ì…°ìµìŠ¤í”¼ì–´ì˜ ëŒ€í‘œìž‘ì€?", "answer": "í–„ë¦¿, ë¡œë¯¸ì˜¤ì™€ ì¤„ë¦¬ì—£, ë§¥ë² ìŠ¤ ë“±ì´ ìžˆìŠµë‹ˆë‹¤.", "context": "ì…°ìµìŠ¤í”¼ì–´ëŠ” ì˜êµ­ì˜ ëŒ€í‘œì ì¸ ê·¹ìž‘ê°€ìž…ë‹ˆë‹¤.", "category": "literature", "difficulty": "medium"}
EOF

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (test.jsonl)
cat > datasets/raw/custom_benchmark/test.jsonl << 'EOF'
{"question": "ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?", "answer": "ë„ì¿„", "context": "ì¼ë³¸ì€ ë™ì•„ì‹œì•„ì˜ ì„¬ë‚˜ë¼ìž…ë‹ˆë‹¤.", "category": "geography", "difficulty": "easy"}
{"question": "ìžë°”ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°°ì—´ì„ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?", "answer": "sort() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", "context": "ìžë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ê°œë°œì˜ í•µì‹¬ ì–¸ì–´ìž…ë‹ˆë‹¤.", "category": "programming", "difficulty": "medium"}
{"question": "ì„¸ê³„ì—ì„œ ê°€ìž¥ ê¸´ ê°•ì€?", "answer": "ë‚˜ì¼ê°•", "context": "ê°•ì€ ì§€êµ¬ì˜ ì¤‘ìš”í•œ ìˆ˜ìžì›ìž…ë‹ˆë‹¤.", "category": "geography", "difficulty": "medium"}
EOF
```

**JSONL í˜•ì‹ ì„¤ëª…**:
- **JSONL** = JSON Lines (ê° ì¤„ë§ˆë‹¤ í•˜ë‚˜ì˜ JSON ê°ì²´)
- ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹ì˜ í‘œì¤€ í˜•ì‹
- ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°€ëŠ¥

### 2. ë§¤ë‹ˆíŽ˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±

```bash
mkdir -p datasets/manifests
cat > datasets/manifests/custom_benchmark_manifest.yaml << 'EOF'
name: "custom_benchmark"
version: "1.0"
description: "ì»¤ìŠ¤í…€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹"
license: "MIT"
language: ["ko", "en"]
domain: "general"
task_type: "text_generation"

source:
  type: "local"
  path: "datasets/raw/custom_benchmark"

splits:
  train:
    file: "raw/custom_benchmark/train.jsonl"
    size: 5
    sha256: "ê³„ì‚°ëœ_í•´ì‹œê°’"
  test:
    file: "raw/custom_benchmark/test.jsonl"
    size: 3
    sha256: "ê³„ì‚°ëœ_í•´ì‹œê°’"

schema:
  input_field: "question"
  output_field: "answer"
  context_field: "context"
  metadata_fields: ["category", "difficulty"]

evaluation:
  evalchemy:
    enabled: true
    tasks: ["custom_task_1", "custom_task_2"]
EOF
```

## ðŸ” ë²¤ì¹˜ë§ˆí¬ ìœ í˜•

### Deepeval ì í•© ë²¤ì¹˜ë§ˆí¬
- **RAG í‰ê°€**: ê²€ìƒ‰ ì¦ê°• ìƒì„± í’ˆì§ˆ ì¸¡ì •
- **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ì—…ë¬´/ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€
- **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§žì¶˜ í‰ê°€
- **ì†Œê·œëª¨ ë°ì´í„°ì…‹**: < 1,000 ìƒ˜í”Œ

### Evalchemy ì í•© ë²¤ì¹˜ë§ˆí¬
- **í‘œì¤€ ë²¤ì¹˜ë§ˆí¬**: MMLU, ARC, HellaSwag ë“±
- **ëŒ€ê·œëª¨ í‰ê°€**: > 1,000 ìƒ˜í”Œ
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´
- **GPU ì§‘ì•½ì **: ëŒ€ìš©ëŸ‰ ëª¨ë¸ í‰ê°€

## ðŸ§ª Deepeval ë©”íŠ¸ë¦­ ì¶”ê°€

### 1. ë©”íŠ¸ë¦­ í´ëž˜ìŠ¤ ìƒì„±

```bash
cat >> eval/deepeval_tests/metrics/custom_metric.py <<EOF
from deepeval.metrics import BaseMetric
from deepeval.test_case import LLMTestCase
from typing import Optional, List, Dict, Any
import asyncio

class CustomMetric(BaseMetric):
    """
    ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­
    
    Args:
        threshold (float): í†µê³¼ ê¸°ì¤€ ì ìˆ˜ (0.0 ~ 1.0)
        model (str): í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ëª…
        include_reason (bool): í‰ê°€ ì´ìœ  í¬í•¨ ì—¬ë¶€
    """
    
    def __init__(
        self,
        threshold: float = 0.7,
        model: Optional[str] = None,
        include_reason: bool = True,
        async_mode: bool = True
    ):
        self.threshold = threshold
        self.model = model
        self.include_reason = include_reason
        self.async_mode = async_mode
        
    def measure(self, test_case: LLMTestCase) -> float:
        """ë™ê¸° í‰ê°€ ì‹¤í–‰"""
        if self.async_mode:
            return asyncio.run(self.a_measure(test_case))
        return self._evaluate_sync(test_case)
    
    async def a_measure(self, test_case: LLMTestCase, _show_indicator: bool = True) -> float:
        """ë¹„ë™ê¸° í‰ê°€ ì‹¤í–‰"""
        return self._evaluate_sync(test_case)
    
    def _evaluate_sync(self, test_case: LLMTestCase) -> float:
        """ì‹¤ì œ í‰ê°€ ë¡œì§ êµ¬í˜„"""
        # ì—¬ê¸°ì— í‰ê°€ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤
        input_text = test_case.input
        actual_output = test_case.actual_output
        expected_output = test_case.expected_output
        
        # ì˜ˆì‹œ: ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°
        score = self._calculate_similarity(actual_output, expected_output)
        
        # ë©”íŠ¸ë¦­ ì†ì„± ì„¤ì •
        self.score = score
        self.success = score >= self.threshold
        
        if self.include_reason:
            self.reason = self._generate_reason(score, self.threshold)
        
        return score
    
    def _calculate_similarity(self, actual: str, expected: str) -> float:
        """ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë°©ë²•ì„ ì‚¬ìš©
        from difflib import SequenceMatcher
        return SequenceMatcher(None, actual, expected).ratio()
    
    def _generate_reason(self, score: float, threshold: float) -> str:
        """í‰ê°€ ì´ìœ  ìƒì„±"""
        if score >= threshold:
            return f"âœ… ì ìˆ˜ {score:.3f}ë¡œ ê¸°ì¤€ {threshold} ì´ìƒ ë‹¬ì„±"
        else:
            return f"âŒ ì ìˆ˜ {score:.3f}ë¡œ ê¸°ì¤€ {threshold} ë¯¸ë‹¬"
    
    def is_successful(self) -> bool:
        """í‰ê°€ ì„±ê³µ ì—¬ë¶€ ë°˜í™˜"""
        return self.success
    
    @property
    def __name__(self):
        return "Custom Metric"
EOF
```

### 2. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìž‘ì„±

```bash
cat >> eval/deepeval_tests/test_custom_metric.py <<EOF
import pytest
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset
from .metrics.custom_metric import CustomMetric

class TestCustomMetric:
    """ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.parametrize("threshold", [0.5, 0.7, 0.9])
    def test_custom_metric_threshold(self, threshold):
        """ë‹¤ì–‘í•œ ìž„ê³„ê°’ì—ì„œ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
        test_case = LLMTestCase(
            input="í…ŒìŠ¤íŠ¸ ìž…ë ¥",
            actual_output="ì‹¤ì œ ì¶œë ¥",
            expected_output="ì˜ˆìƒ ì¶œë ¥"
        )
        
        metric = CustomMetric(threshold=threshold)
        assert_test(test_case, [metric])
    
    def test_custom_metric_dataset(self):
        """ë°ì´í„°ì…‹ ê¸°ë°˜ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        dataset = EvaluationDataset(test_cases=[
            LLMTestCase(
                input="ì§ˆë¬¸ 1",
                actual_output="ë‹µë³€ 1",
                expected_output="ì •ë‹µ 1"
            ),
            LLMTestCase(
                input="ì§ˆë¬¸ 2", 
                actual_output="ë‹µë³€ 2",
                expected_output="ì •ë‹µ 2"
            )
        ])
        
        metric = CustomMetric(threshold=0.7)
        dataset.evaluate([metric])
        
        # ê²°ê³¼ ê²€ì¦
        assert len(dataset.test_cases) == 2
        for test_case in dataset.test_cases:
            assert hasattr(test_case, 'metrics_metadata')
EOF
```

### 3. ë©”íŠ¸ë¦­ ë“±ë¡

```bash
cat >> eval/deepeval_tests/metrics/__init__.py <<EOF
from .rag_precision import RAGPrecisionMetric
from .custom_metric import CustomMetric

# ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ ëª©ë¡
AVAILABLE_METRICS = {
    'rag_precision': RAGPrecisionMetric,
    'custom_metric': CustomMetric,
}

def get_metric(metric_name: str, **kwargs):
    """ë©”íŠ¸ë¦­ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    if metric_name not in AVAILABLE_METRICS:
        raise ValueError(f"Unknown metric: {metric_name}")
    
    return AVAILABLE_METRICS[metric_name](**kwargs)
EOF
```

## âš¡ Evalchemy ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€

### 1. íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ ë° ì´ˆê¸°í™” íŒŒì¼ ìƒì„±

ë¨¼ì € ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ë””ë ‰í† ë¦¬ì™€ í•„ìš”í•œ íŒŒì¼ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤:

```bash
# íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p eval/evalchemy/tasks

# __init__.py íŒŒì¼ ìƒì„± (íƒœìŠ¤í¬ ì¸ì‹ì„ ìœ„í•´ í•„ìš”)
cat > eval/evalchemy/tasks/__init__.py << 'EOF'
"""
ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ ëª¨ë“ˆ

ì´ íŒŒì¼ì€ lm_evalì´ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ì¸ì‹í•  ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ì´ˆê¸°í™” íŒŒì¼ìž…ë‹ˆë‹¤.
YAML ê¸°ë°˜ íƒœìŠ¤í¬ ì •ì˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„ ì´ íŒŒì¼ì´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
"""

# íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ìž„ì„ ëª…ì‹œ
__version__ = "1.0.0"
__author__ = "VLLM Eval Team"
EOF
```

### 2. YAML íƒœìŠ¤í¬ ì •ì˜ ìƒì„±

ìµœì‹  lm_eval (v0.4+)ì—ì„œëŠ” YAML íŒŒì¼ë¡œ íƒœìŠ¤í¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:

```bash
# custom_task_1.yaml ìƒì„±
cat > eval/evalchemy/tasks/custom_task_1.yaml << 'EOF'
task: custom_task_1
test_split: test
fewshot_split: train
fewshot_config:
  sampler: first_n
doc_to_text: "ì§ˆë¬¸: {{question}}\në‹µë³€:"
doc_to_target: "{{answer}}"
description: "ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ 1 - ì§ˆë¬¸ë‹µë³€ í‰ê°€"
dataset_path: json
dataset_kwargs:
  data_files:
    train: "../../datasets/raw/custom_benchmark/train.jsonl"
    test: "../../datasets/raw/custom_benchmark/test.jsonl"
output_type: generate_until
generation_kwargs:
  until: ["\n"]
  max_gen_toks: 100
filter_list:
  - name: "whitespace_cleanup"
    filter:
      - function: "regex"
        regex_pattern: "^\\s*(.*)$"
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
metadata:
  version: 1.0
EOF

# custom_task_2.yaml ìƒì„± (ì„ íƒì‚¬í•­)
cat > eval/evalchemy/tasks/custom_task_2.yaml << 'EOF'
task: custom_task_2
test_split: test
fewshot_split: train
fewshot_config:
  sampler: first_n
doc_to_text: "ì§ˆë¬¸: {{question}}\në‹µë³€:"
doc_to_target: "{{answer}}"
description: "ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ 2 - ì§ˆë¬¸ë‹µë³€ í‰ê°€"
dataset_path: json
dataset_kwargs:
  data_files:
    train: "../../datasets/raw/custom_benchmark/train.jsonl"
    test: "../../datasets/raw/custom_benchmark/test.jsonl"
output_type: generate_until
generation_kwargs:
  until: ["\n"]
  max_gen_toks: 100
filter_list:
  - name: "whitespace_cleanup"
    filter:
      - function: "regex"
        regex_pattern: "^\\s*(.*)$"
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
metadata:
  version: 1.0
EOF
```

**ì£¼ìš” ì„¤ì • ì„¤ëª…**:
- `dataset_path: json`: JSON/JSONL íŒŒì¼ ì‚¬ìš©
- `dataset_kwargs`: ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì§€ì •
- `doc_to_text`: ìž…ë ¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- `doc_to_target`: ì •ë‹µ ì¶”ì¶œ ë°©ë²•
- `exact_match`: ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€í•˜ëŠ” ë©”íŠ¸ë¦­

## ðŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 1. ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ ì¸ì‹ í™•ì¸

```bash
cd eval/evalchemy
python3 -m lm_eval --include_path tasks --tasks list | grep custom
```

### 2. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# Deepeval ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸
cd ../..
python3 -m pytest eval/deepeval_tests/test_custom_metric.py -v

# Evalchemy íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ëª¨ë¸ í•„ìš”)
cd eval/evalchemy
python3 -m lm_eval --include_path tasks --model hf --model_args pretrained=gpt2 --tasks custom_task_1 --limit 2 --device cpu
```

### 3. ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‹¤í–‰

```bash
# VLLM ì„œë²„ì™€ í•¨ê»˜ ì‹¤í–‰ - root í´ë”ì—ì„œ ëª…ë ¹ì–´ ì‹¤í–‰ì„ ìœ„í•œ cd ../..
cd ../..
./run_evalchemy.sh --endpoint http://your-vllm-server:8000/v1/completions
```

`run_evalchemy.sh` ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ë¯¸ `--include_path tasks` ì˜µì…˜ì´ í¬í•¨ë˜ì–´ ìžˆì–´ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ìžë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.

### 6. ë¬¸ì œ í•´ê²°

#### ./run_evalchemy.sh ì‹¤í–‰ ì‹œ task ì¸ì‹ ì•ˆë¨
```bash
# ë£¨íŠ¸ í´ë”ë¡œ ì´ë™
```

#### íƒœìŠ¤í¬ ì¸ì‹ ì•ˆë¨
```bash
# í•´ê²°: --include_path í™•ì¸
python3 -m lm_eval --include_path tasks --tasks list

# YAML ë¬¸ë²• ê²€ì‚¬
python3 -c "import yaml; print(yaml.safe_load(open('tasks/custom_task_1.yaml')))"
```

#### macOS CUDA ì—ëŸ¬
```bash
# CPU ëª¨ë“œ ì‚¬ìš©
python3 -m lm_eval --include_path tasks --model hf --model_args pretrained=gpt2 --tasks custom_task_1 --device cpu
```

#### ë°ì´í„°ì…‹ ê²½ë¡œ ì—ëŸ¬
```bash
# í•´ê²°: ìƒëŒ€ ê²½ë¡œ í™•ì¸
ls -la ../../datasets/raw/custom_benchmark/
```

#### ë‹µë³€ ê³µë°± ë¬¸ì œ
```bash
# í•´ê²°: " ë„ì¿„"ì™€ ê°™ì´ ì•žì— ê³µë°±ì´ ì¶”ê°€ëœ ê²½ìš° yamlíŒŒì¼ì— í•´ë‹¹ ë‚´ìš© ì¶”ê°€
filter_list:
  - name: "whitespace_cleanup"
    filter:
      - function: "regex"
        regex_pattern: "^\\s*(.*)$"
```

## ðŸ”§ ë¬¸ì œ í•´ê²° (Troubleshooting)

### 1. lm_evalì´ ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°

**ì¦ìƒ**: `Task 'custom_task_1' not found` ì—ëŸ¬

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: --include_path í™•ì¸
cd eval/evalchemy
python3 -m lm_eval --include_path tasks --tasks list | grep custom

# 2ë‹¨ê³„: YAML íŒŒì¼ êµ¬ë¬¸ ê²€ì‚¬
python3 -c "import yaml; print(yaml.safe_load(open('tasks/custom_task_1.yaml')))"

# 3ë‹¨ê³„: íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
ls -la tasks/
# ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìžˆì–´ì•¼ í•¨:
# - custom_task_1.yaml
# - custom_task_2.yaml
```

### 2. ë°ì´í„°ì…‹ ë¡œë”© ì—ëŸ¬

**ì¦ìƒ**: `Dataset 'custom_dataset' doesn't exist on the Hub`

**ì›ì¸**: YAML íŒŒì¼ì—ì„œ ìž˜ëª»ëœ ë°ì´í„°ì…‹ ê²½ë¡œ ì°¸ì¡°

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: ë°ì´í„° íŒŒì¼ ì¡´ìž¬ í™•ì¸
ls -la ../../datasets/raw/custom_benchmark/
# train.jsonlê³¼ test.jsonlì´ ìžˆì–´ì•¼ í•¨

# 2ë‹¨ê³„: YAML ì„¤ì • í™•ì¸
cat tasks/custom_task_1.yaml | grep -A 5 dataset_path
# ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •ë˜ì–´ì•¼ í•¨:
# dataset_path: json
# dataset_kwargs:
#   data_files:
#     train: "../../datasets/raw/custom_benchmark/train.jsonl"
#     test: "../../datasets/raw/custom_benchmark/test.jsonl"
```

### 3. macOSì—ì„œ CUDA ì—ëŸ¬

**ì¦ìƒ**: `AssertionError: Torch not compiled with CUDA enabled`

**í•´ê²° ë°©ë²•**:
```bash
# í•­ìƒ --device cpu ì˜µì…˜ ì‚¬ìš©
python3 -m lm_eval --device cpu

# 2ë‹¨ê³„: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python3 -m lm_eval --batch_size 1

# 3ë‹¨ê³„: ìž‘ì€ ëª¨ë¸ ì‚¬ìš©
python3 -m lm_eval --model hf --model_args pretrained=distilgpt2
```

### 4. YAML íŒŒì¼ split ì—ëŸ¬

**ì¦ìƒ**: `KeyError: 'test'` ë˜ëŠ” `KeyError: 'validation'`

**ì›ì¸**: ë°ì´í„°ì…‹ì— í•´ë‹¹ splitì´ ì—†ìŒ

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥í•œ split í™•ì¸
python3 -c "
import datasets
ds = datasets.load_dataset('json', data_files={'train': '../../datasets/raw/custom_benchmark/train.jsonl', 'test': '../../datasets/raw/custom_benchmark/test.jsonl'})
print('Available splits:', list(ds.keys()))
"

# 2ë‹¨ê³„: YAML íŒŒì¼ì—ì„œ ì˜¬ë°”ë¥¸ split ì‚¬ìš©
# test_split: test  (ë˜ëŠ” validation)
```

### 5. ë©”íŠ¸ë¦­ ê³„ì‚° ì—ëŸ¬

**ì¦ìƒ**: `TypeError: unsupported operand type(s) for +: 'int' and 'list'`

**ì›ì¸**: BLEU ê°™ì€ ë³µìž¡í•œ ë©”íŠ¸ë¦­ì—ì„œ ë°ì´í„° íƒ€ìž… ë¶ˆì¼ì¹˜

**í•´ê²° ë°©ë²•**:
```yaml
# ê°„ë‹¨í•œ ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
# BLEU ë©”íŠ¸ë¦­ ì œê±°
```

### 6. ê²½ë¡œ ë¬¸ì œ

**ì¦ìƒ**: `FileNotFoundError: [Errno 2] No such file or directory`

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: í˜„ìž¬ ìž‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
pwd
# /path/to/vllm-eval/eval/evalchemy ì´ì–´ì•¼ í•¨

# 2ë‹¨ê³„: ìƒëŒ€ ê²½ë¡œ í™•ì¸
ls -la ../../datasets/raw/custom_benchmark/

# 3ë‹¨ê³„: ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© (í•„ìš”ì‹œ)
# YAML íŒŒì¼ì—ì„œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
```

### 7. ê¶Œí•œ ë¬¸ì œ

**ì¦ìƒ**: `PermissionError: [Errno 13] Permission denied`

**í•´ê²° ë°©ë²•**:
```bash
# íŒŒì¼ ê¶Œí•œ í™•ì¸ ë° ìˆ˜ì •
chmod 644 datasets/raw/custom_benchmark/*.jsonl
chmod 644 eval/evalchemy/tasks/*.yaml
```

### 8. ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: `RuntimeError: CUDA out of memory` ë˜ëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: CPU ëª¨ë“œ ì‚¬ìš©
python3 -m lm_eval --device cpu

# 2ë‹¨ê³„: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python3 -m lm_eval --batch_size 1

# 3ë‹¨ê³„: ìž‘ì€ ëª¨ë¸ ì‚¬ìš©
python3 -m lm_eval --model hf --model_args pretrained=distilgpt2
```

### 9. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**í•´ê²° ë°©ë²•**:
```bash
# 1ë‹¨ê³„: ì¸í„°ë„· ì—°ê²° í™•ì¸
ping huggingface.co

# 2ë‹¨ê³„: í”„ë¡ì‹œ ì„¤ì • (í•„ìš”ì‹œ)
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port

# 3ë‹¨ê³„: ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
export HF_HOME=/path/to/cache
```

### 10. ì¼ë°˜ì ì¸ ë””ë²„ê¹… íŒ

```bash
# 1. ìƒì„¸ ë¡œê·¸ ì¶œë ¥
export LOGLEVEL=DEBUG
python3 -m lm_eval --include_path tasks --tasks custom_task_1 --limit 1

# 2. Python ê²½ë¡œ í™•ì¸
python3 -c "import sys; print('\n'.join(sys.path))"

# 3. íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸
pip list | grep -E "(lm-eval|datasets|transformers|torch)"

# 4. ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸
# 4-1. íƒœìŠ¤í¬ ëª©ë¡ í™•ì¸
python3 -m lm_eval --include_path tasks --tasks list

# 4-2. ì„¤ì • í™•ì¸
python3 -m lm_eval --include_path tasks --tasks custom_task_1 --help

# 4-3. ìµœì†Œ ì‹¤í–‰
python3 -m lm_eval --include_path tasks --model hf --model_args pretrained=gpt2 --tasks custom_task_1 --limit 1 --device cpu
```

## ðŸ“š ì°¸ê³  ìžë£Œ

- [lm-evaluation-harness v0.4+ Documentation](https://github.com/EleutherAI/lm-evaluation-harness)
- [Deepeval Documentation](https://docs.confident-ai.com/)
- [VLLM Documentation](https://docs.vllm.ai/)
- [Argo Workflows](https://argoproj.github.io/argo-workflows/)

## ðŸ¤ ê¸°ì—¬í•˜ê¸°

ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶”ê°€í•œ í›„ì—ëŠ”:

1. **ë¬¸ì„œ ì—…ë°ì´íŠ¸**: ì´ ê°€ì´ë“œì™€ README ì—…ë°ì´íŠ¸
2. **í…ŒìŠ¤íŠ¸ ì¶”ê°€**: ì¶©ë¶„í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸ ìž‘ì„±
3. **PR ìƒì„±**: ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ Pull Request ìƒì„±
4. **ë¦¬ë·° ìš”ì²­**: íŒ€ ë©¤ë²„ë“¤ì˜ ì½”ë“œ ë¦¬ë·° ìš”ì²­

ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ë©´ ì´ìŠˆë¥¼ ìƒì„±í•˜ê±°ë‚˜ íŒ€ ì±„ë„ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”! ðŸš€

**ì£¼ì˜ì‚¬í•­**:
- ë°©ë²• 1: ì „ì²´ íƒœìŠ¤í¬ ëª©ë¡ì—ì„œ `custom_task_1`, `custom_task_2`ê°€ ë³´ì´ë©´ ì„±ê³µ
- ë°©ë²• 2: macOSì—ì„œëŠ” ë°˜ë“œì‹œ `--device cpu` ì˜µì…˜ í•„ìš” (CUDA ë¯¸ì§€ì›)
- `--limit 2`ëŠ” í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ì‹¤ì œ í‰ê°€ì—ì„œëŠ” ì œê±°í•´ì•¼ í•¨