# VLLM Official Benchmark Container - Using Pre-built VLLM Image
FROM vllm/vllm-openai:latest

# ë©”íƒ€ë°ì´í„°
LABEL maintainer="ThakiCloud <tech@company.com>"
LABEL description="VLLM official benchmark_serving.py container using pre-built image"
LABEL version="2.0.0"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    DEBIAN_FRONTEND=noninteractive

# ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
WORKDIR /app

# VLLM ì†ŒìŠ¤ì½”ë“œ í´ë¡  (benchmark_serving.py í¬í•¨)
RUN git clone https://github.com/vllm-project/vllm.git /app/vllm-source && \
    cd /app/vllm-source && \
    git checkout main

# VLLM benchmarks ë””ë ‰í† ë¦¬ë¥¼ ë©”ì¸ ìœ„ì¹˜ë¡œ ë³µì‚¬
RUN cp -r /app/vllm-source/benchmarks /app/benchmarks && \
    chmod +x /app/benchmarks/benchmark_serving.py

# ë²¤ì¹˜ë§ˆí‚¹ì— í•„ìš”í•œ ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜
# hadolint ignore=DL3013
RUN pip install --no-cache-dir \
    datasets==3.2.0 \
    matplotlib==3.9.2 \
    seaborn==0.13.2 \
    memory-profiler==0.61.0 \
    jsonlines==4.0.0 \
    asyncio-throttle==1.0.2 \
    PyYAML==6.0.2

# ì„¤ì • íŒŒì¼ ë° ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬ (ë¨¼ì € ë³µì‚¬)
COPY configs/ /app/configs/
COPY scripts/ /app/scripts/

# ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ì—…ë°ì´íŠ¸ëœ ë²„ì „)
RUN cp /app/scripts/run_vllm_benchmark.sh /app/run_vllm_benchmark.sh && \
    chmod +x /app/run_vllm_benchmark.sh
RUN cp /app/scripts/standardize_vllm_benchmark.py /app/standardize_vllm_benchmark.py && \
    chmod +x /app/standardize_vllm_benchmark.py

# ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
RUN cat > /app/create_sample_dataset.py << 'EOF' && chmod +x /app/create_sample_dataset.py
#!/usr/bin/env python3
"""
VLLM ë²¤ì¹˜ë§ˆí¬ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± (VLLM í˜•ì‹)
"""
import json
import os

def create_vllm_sample_dataset(output_path: str, num_samples: int = 100):
    """VLLM benchmark_serving.py í˜¸í™˜ ë°ì´í„°ì…‹ ìƒì„±"""
    
    # ë‹¤ì–‘í•œ ê¸¸ì´ì™€ ë³µì¡ë„ì˜ í”„ë¡¬í”„íŠ¸ë“¤
    sample_prompts = [
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
        "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì´ ì‚¬íšŒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ë…¼ì˜í•´ì£¼ì„¸ìš”.",
        "ê¸°í›„ ë³€í™”ì˜ ì£¼ìš” ì›ì¸ê³¼ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
        "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì˜ ì›ë¦¬ì™€ í™œìš© ë¶„ì•¼ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”.",
        "ì§€ì† ê°€ëŠ¥í•œ ë°œì „ì„ ìœ„í•œ ì¬ìƒ ì—ë„ˆì§€ì˜ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¹…ë°ì´í„° ë¶„ì„ì˜ ê³¼ì •ê³¼ í™œìš© ì‚¬ë¡€ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.",
        "ì‚¬ë¬¼ì¸í„°ë„·(IoT)ì´ ì¼ìƒìƒí™œì— ë¯¸ì¹˜ëŠ” ë³€í™”ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì–‘ìì»´í“¨íŒ…ì˜ ì›ë¦¬ì™€ ë¯¸ë˜ ì „ë§ì— ëŒ€í•´ ë…¼ì˜í•´ì£¼ì„¸ìš”."
    ]
    
    dataset = []
    for i in range(num_samples):
        prompt = sample_prompts[i % len(sample_prompts)]
        # í”„ë¡¬í”„íŠ¸ì— ë²ˆí˜¸ ì¶”ê°€ë¡œ ë‹¤ì–‘ì„± í™•ë³´
        if i >= len(sample_prompts):
            prompt = f"[{i+1}] {prompt}"
        
        # VLLM benchmark_serving.py í˜¸í™˜ í˜•ì‹
        dataset.append({
            "prompt": prompt,
            "output_len": 128  # ì˜ˆìƒ ì¶œë ¥ ê¸¸ì´
        })
    
    # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… VLLM í˜¸í™˜ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {output_path}")
    print(f"ğŸ“Š ì´ {len(dataset)}ê°œ í”„ë¡¬í”„íŠ¸")

if __name__ == "__main__":
    create_vllm_sample_dataset("/app/sample_dataset.jsonl", 100)
EOF

# VLLM ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (ì™¸ë¶€ íŒŒì¼ ì‚¬ìš©)
RUN cp /app/scripts/analyze_vllm_results.py /app/analyze_vllm_results.py && \
    chmod +x /app/analyze_vllm_results.py


# ì„¤ì • íŒŒì¼ ë° ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ë³µì‚¬ë¨

# í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
RUN printf '#!/bin/bash\npython -c "import requests; print(\\"VLLM Benchmark ready\\")"' > /app/healthcheck.sh && \
    chmod +x /app/healthcheck.sh

# ë¹„root ì‚¬ìš©ì ìƒì„± (ë³´ì•ˆ)
RUN useradd --create-home --shell /bin/bash benchuser && \
    chown -R benchuser:benchuser /app

USER benchuser

# ê¸°ë³¸ í™˜ê²½ ë³€ìˆ˜ (ìµœì‹  íŒŒë¼ë¯¸í„° ë°˜ì˜)
ENV VLLM_ENDPOINT="http://vllm:8000" \
    ENDPOINT_PATH="/v1/chat/completions" \
    MODEL_NAME="Qwen/Qwen3-8B" \
    SERVED_MODEL_NAME="qwen3-8b" \
    OUTPUT_DIR="/results" \
    PARSED_DIR="/parsed" \
    NUM_PROMPTS="100" \
    REQUEST_RATE="1.0" \
    MAX_CONCURRENCY="1" \
    RANDOM_INPUT_LEN="512" \
    RANDOM_OUTPUT_LEN="128" \
    BACKEND="openai-chat" \
    DATASET_TYPE="random" \
    PERCENTILE_METRICS="ttft,tpot,itl,e2el" \
    METRIC_PERCENTILES="25,50,75,90,95,99" \
    LOG_LEVEL="INFO"

# ë³¼ë¥¨ ë§ˆìš´íŠ¸ í¬ì¸íŠ¸
VOLUME ["/results", "/parsed"]

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD /app/healthcheck.sh

# ê¸°ë³¸ ì‹¤í–‰ ëª…ë ¹
ENTRYPOINT ["/app/run_vllm_benchmark.sh"]

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì˜ˆì‹œ:
# ê¸°ë³¸ ì‹¤í–‰
# docker run -v $(pwd)/vllm-eval/results:/results -v $(pwd)/vllm-eval/parsed:/parsed vllm-benchmark:latest
#
# ê³ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
# docker run -v $(pwd)/vllm-eval/results:/results -v $(pwd)/vllm-eval/parsed:/parsed \
#   -e VLLM_ENDPOINT=http://gpu-cluster:8000 \
#   -e MODEL_NAME=Qwen/Qwen3-8B \
#   -e SERVED_MODEL_NAME=qwen3-8b \
#   -e MAX_CONCURRENCY=10 \
#   -e RANDOM_INPUT_LEN=2048 \
#   -e RANDOM_OUTPUT_LEN=512 \
#   vllm-benchmark:latest
#
# ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
# docker run -v $(pwd)/vllm-eval/results:/results -v $(pwd)/vllm-eval/parsed:/parsed \
#   -e MAX_CONCURRENCY=50 \
#   -e METRIC_PERCENTILES=50,90,95,99,99.9 \
#   vllm-benchmark:latest