#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ Deepeval í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import logging
import os

from deepeval.models import DeepEvalBaseLLM
from deepeval.test_case import LLMTestCase

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleModel(DeepEvalBaseLLM):
    """ê°„ë‹¨í•œ Mock ëª¨ë¸"""

    def __init__(self):
        self.model_name = "simple-mock"

    def load_model(self):
        return self.model_name

    def generate(self, prompt: str, schema=None) -> str:  # noqa: ARG002
        """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±"""
        if "í•œêµ­" in prompt:
            return "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."
        elif "íŒŒì´ì¬" in prompt:
            return "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ë ¤ë©´ sort() ë©”ì„œë“œë‚˜ sorted() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        elif "ì§€êµ¬" in prompt:
            return "ì§€êµ¬ì˜ ë‘˜ë ˆëŠ” ì•½ 40,075kmì…ë‹ˆë‹¤."
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    async def a_generate(self, prompt: str, schema=None) -> str:
        return self.generate(prompt, schema)

    def get_model_name(self) -> str:
        return self.model_name


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš€ ê°„ë‹¨í•œ Deepeval í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # ëª¨ë¸ ìƒì„±
    model = SimpleModel()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
    test_cases = [
        LLMTestCase(
            input="í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            actual_output=model.generate("í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"),
            expected_output="í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.",
        ),
        LLMTestCase(
            input="íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬ ë°©ë²•ì€?",
            actual_output=model.generate("íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬ ë°©ë²•ì€?"),
            expected_output="íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ë ¤ë©´ sort() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        ),
    ]

    # ê²°ê³¼ ì €ì¥
    output_dir = os.getenv("OUTPUT_DIR", "./test_results")
    os.makedirs(output_dir, exist_ok=True)

    results = {
        "test_results": [],
        "summary": {
            "total_tests": len(test_cases),
            "model_name": model.get_model_name(),
            "status": "success",
        },
    }

    print("\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    for i, tc in enumerate(test_cases):
        print(f"\nTest {i+1}:")
        print(f"  Input: {tc.input}")
        print(f"  Expected: {tc.expected_output}")
        print(f"  Actual: {tc.actual_output}")

        # ì •í™•ë„ ê³„ì‚°
        accuracy = 1.0 if tc.actual_output == tc.expected_output else 0.8
        match = "âœ…" if accuracy == 1.0 else "âŒ"

        print(f"  Accuracy: {accuracy:.2f} {match}")

        # ê²°ê³¼ ì €ì¥
        results["test_results"].append(
            {
                "test_id": i + 1,
                "input": tc.input,
                "expected_output": tc.expected_output,
                "actual_output": tc.actual_output,
                "accuracy": accuracy,
                "match": accuracy == 1.0,
            }
        )

    # JSON íŒŒì¼ë¡œ ì €ì¥
    results_file = f"{output_dir}/simple_deepeval_results.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")

    # ìš”ì•½ ì¶œë ¥
    total_tests = len(test_cases)
    passed_tests = sum(1 for result in results["test_results"] if result["match"])
    success_rate = (passed_tests / total_tests) * 100

    print("\nğŸ“Š ìš”ì•½:")
    print(f"  ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
    print(f"  í†µê³¼: {passed_tests}")
    print(f"  ì„±ê³µë¥ : {success_rate:.1f}%")

    return results


if __name__ == "__main__":
    main()
