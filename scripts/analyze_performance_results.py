#!/usr/bin/env python3
"""
VLLM ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""
import json
import os
import sys
import glob
from pathlib import Path
from typing import Dict, List, Any
import statistics

def load_benchmark_results(results_dir: str) -> Dict[str, Any]:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¡œë“œ"""
    results = {}
    
    for scenario_dir in glob.glob(f"{results_dir}/*_*"):
        if not os.path.isdir(scenario_dir):
            continue
            
        scenario_name = os.path.basename(scenario_dir).split('_')[0]
        
        # JSON ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
        json_files = glob.glob(f"{scenario_dir}/*.json")
        if not json_files:
            print(f"âš ï¸  {scenario_name}: JSON ê²°ê³¼ íŒŒì¼ ì—†ìŒ")
            continue
            
        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
        latest_file = max(json_files, key=os.path.getctime)
        
        try:
            with open(latest_file, 'r') as f:
                data = json.load(f)
                results[scenario_name] = data
                print(f"âœ… {scenario_name}: ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {scenario_name}: ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨ - {e}")
    
    return results

def analyze_performance(results: Dict[str, Any]) -> None:
    """ì„±ëŠ¥ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "="*60)
    print("ğŸ“Š VLLM ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ê²°ê³¼")
    print("="*60)
    
    summary = []
    
    for scenario_name, data in results.items():
        print(f"\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤: {scenario_name}")
        print("-" * 40)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ (ì˜¬ë°”ë¥¸ í•„ë“œëª… ì‚¬ìš©)
        completed_requests = data.get('completed', 0)
        total_requests = data.get('num_prompts', completed_requests)
        success_rate = completed_requests / total_requests if total_requests > 0 else 0
        
        print(f"âœ… ì„±ê³µë¥ : {success_rate:.1%} ({completed_requests}/{total_requests})")
        print(f"â±ï¸  ì´ ì†Œìš”ì‹œê°„: {data.get('duration', 0):.1f}ì´ˆ")
        print(f"ğŸ”„ ìš”ì²­ ì²˜ë¦¬ëŸ‰: {data.get('request_throughput', 0):.2f} req/s")
        print(f"ğŸ“¤ í† í° ì²˜ë¦¬ëŸ‰: {data.get('output_throughput', 0):.1f} tok/s")
        
        # TTFT (Time to First Token)
        ttft_mean = data.get('mean_ttft_ms', 0)
        ttft_p95 = data.get('p95_ttft_ms', 0)
        print(f"ğŸš€ TTFT í‰ê· : {ttft_mean:.1f}ms")
        print(f"ğŸš€ TTFT P95: {ttft_p95:.1f}ms")
        
        # TPOT (Time per Output Token)
        tpot_mean = data.get('mean_tpot_ms', 0)
        tpot_p95 = data.get('p95_tpot_ms', 0)
        print(f"âš¡ TPOT í‰ê· : {tpot_mean:.1f}ms")
        print(f"âš¡ TPOT P95: {tpot_p95:.1f}ms")
        
        # E2E Latency
        e2el_mean = data.get('mean_e2el_ms', 0)
        e2el_p95 = data.get('p95_e2el_ms', 0)
        print(f"ğŸ”„ E2E í‰ê· : {e2el_mean:.1f}ms")
        print(f"ğŸ”„ E2E P95: {e2el_p95:.1f}ms")
        
        # ìš”ì•½ ë°ì´í„° ìˆ˜ì§‘
        summary.append({
            'scenario': scenario_name,
            'success_rate': success_rate,
            'throughput': data.get('output_throughput', 0),
            'ttft_p95': ttft_p95,
            'tpot_mean': tpot_mean,
            'e2el_p95': e2el_p95
        })
    
    # ì „ì²´ ìš”ì•½
    if summary:
        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½")
        print("-" * 40)
        
        avg_success_rate = statistics.mean([s['success_rate'] for s in summary])
        avg_throughput = statistics.mean([s['throughput'] for s in summary])
        avg_ttft_p95 = statistics.mean([s['ttft_p95'] for s in summary])
        avg_tpot_mean = statistics.mean([s['tpot_mean'] for s in summary])
        
        print(f"í‰ê·  ì„±ê³µë¥ : {avg_success_rate:.1%}")
        print(f"í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_throughput:.1f} tok/s")
        print(f"í‰ê·  TTFT P95: {avg_ttft_p95:.1f}ms")
        print(f"í‰ê·  TPOT: {avg_tpot_mean:.1f}ms")
        
        # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
        grade = "A"
        issues = []
        
        if avg_success_rate < 0.95:
            grade = "C"
            issues.append(f"ë‚®ì€ ì„±ê³µë¥  ({avg_success_rate:.1%})")
        
        if avg_ttft_p95 > 200:
            grade = "B" if grade == "A" else grade
            issues.append(f"ë†’ì€ TTFT P95 ({avg_ttft_p95:.1f}ms)")
        
        if avg_tpot_mean > 50:
            grade = "B" if grade == "A" else grade
            issues.append(f"ë†’ì€ TPOT ({avg_tpot_mean:.1f}ms)")
        
        if avg_throughput < 10:
            grade = "C"
            issues.append(f"ë‚®ì€ ì²˜ë¦¬ëŸ‰ ({avg_throughput:.1f} tok/s)")
        
        print(f"\nğŸ† ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        if issues:
            print("âš ï¸  ê°œì„  í•„ìš” ì‚¬í•­:")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print("âœ¨ ëª¨ë“  ì„±ëŠ¥ ì§€í‘œê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤!")

def main():
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python3 analyze_performance_results.py <results_directory>")
        sys.exit(1)
    
    results_dir = sys.argv[1]
    
    if not os.path.exists(results_dir):
        print(f"âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {results_dir}")
        sys.exit(1)
    
    print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {results_dir}")
    
    # ê²°ê³¼ ë¡œë“œ ë° ë¶„ì„
    results = load_benchmark_results(results_dir)
    
    if not results:
        print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    analyze_performance(results)

if __name__ == "__main__":
    main() 