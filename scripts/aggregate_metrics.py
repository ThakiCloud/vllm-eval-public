#!/usr/bin/env python3
"""
Aggregate Metrics Script

Deepevalê³¼ Evalchemy í‰ê°€ ê²°ê³¼ë¥¼ ClickHouseì— ì €ì¥í•˜ê³ 
Teams Webhookìœ¼ë¡œ ì•Œë¦¼ì„ ì „ì†¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import argparse
import json
import logging
import os
import sys
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional

import requests
from clickhouse_driver import Client
from jinja2 import Template

# Constants
REGRESSION_THRESHOLD = 0.1
SIGNIFICANT_CHANGE_THRESHOLD = 0.2
HTTP_TIMEOUT = 30
DEFAULT_HISTORICAL_LIMIT = 5
RUN_ID_DISPLAY_LENGTH = 8

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class MetricsAggregator:
    """ë©”íŠ¸ë¦­ ì§‘ê³„ ë° ì €ì¥ í´ë˜ìŠ¤."""

    def __init__(
        self,
        clickhouse_host: str = "localhost",
        clickhouse_port: int = 9000,
        clickhouse_database: str = "vllm_eval",
        teams_webhook_url: Optional[str] = None,
    ):
        """
        Args:
            clickhouse_host: ClickHouse í˜¸ìŠ¤íŠ¸
            clickhouse_port: ClickHouse í¬íŠ¸
            clickhouse_database: ë°ì´í„°ë² ì´ìŠ¤ëª…
            teams_webhook_url: Teams Webhook URL
        """
        self.clickhouse_host = clickhouse_host
        self.clickhouse_port = clickhouse_port
        self.clickhouse_database = clickhouse_database
        self.teams_webhook_url = teams_webhook_url

        # ClickHouse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.clickhouse_client = Client(
                host=clickhouse_host, port=clickhouse_port, database=clickhouse_database
            )
            logger.info(f"ClickHouse ì—°ê²° ì„±ê³µ: {clickhouse_host}:{clickhouse_port}")
        except Exception as e:
            logger.error(f"ClickHouse ì—°ê²° ì‹¤íŒ¨: {e}")
            self.clickhouse_client = None

    def ensure_table_exists(self) -> None:
        """ClickHouse í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)."""
        if not self.clickhouse_client:
            logger.warning("ClickHouse í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ í…Œì´ë¸” ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        create_table_sql = """
        CREATE TABLE IF NOT EXISTS vllm_eval.results (
            run_id String,
            model_tag String,
            metric_name String,
            metric_value Float64,
            metric_category String,
            evaluation_framework String,
            dataset_name String,
            dataset_version String,
            timestamp DateTime64(3),
            metadata String
        ) ENGINE = MergeTree()
        ORDER BY (model_tag, timestamp, metric_name)
        PARTITION BY toYYYYMM(timestamp)
        """

        try:
            self.clickhouse_client.execute(create_table_sql)
            logger.info("ClickHouse í…Œì´ë¸” í™•ì¸/ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")

    def load_evaluation_results(self, results_file: str) -> dict:
        """í‰ê°€ ê²°ê³¼ íŒŒì¼ ë¡œë“œ."""
        logger.info(f"í‰ê°€ ê²°ê³¼ ë¡œë“œ: {results_file}")

        try:
            results_path = Path(results_file)
            results = json.loads(results_path.read_text(encoding="utf-8"))
            return results
        except Exception as e:
            logger.error(f"ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def parse_deepeval_results(self, results: dict, run_id: str, model_tag: str) -> list[dict]:
        """Deepeval ê²°ê³¼ íŒŒì‹±."""
        parsed_results = []

        if "deepeval_results" not in results:
            return parsed_results

        deepeval_data = results["deepeval_results"]
        timestamp = datetime.utcnow()

        # ë©”íŠ¸ë¦­ë³„ ê²°ê³¼ íŒŒì‹±
        for metric_name, metric_value in deepeval_data.get("metrics", {}).items():
            if isinstance(metric_value, (int, float)):
                parsed_results.append(
                    {
                        "run_id": run_id,
                        "model_tag": model_tag,
                        "metric_name": metric_name,
                        "metric_value": float(metric_value),
                        "metric_category": self._get_metric_category(metric_name),
                        "evaluation_framework": "deepeval",
                        "dataset_name": deepeval_data.get("dataset_name", "unknown"),
                        "dataset_version": deepeval_data.get("dataset_version", "v1.0.0"),
                        "timestamp": timestamp,
                        "metadata": json.dumps(
                            {
                                "execution_time": deepeval_data.get("execution_time"),
                                "sample_count": deepeval_data.get("sample_count"),
                                "success_rate": deepeval_data.get("success_rate"),
                            }
                        ),
                    }
                )

        logger.info(f"Deepeval ê²°ê³¼ íŒŒì‹± ì™„ë£Œ: {len(parsed_results)}ê°œ ë©”íŠ¸ë¦­")
        return parsed_results

    def parse_evalchemy_results(self, results: dict, run_id: str, model_tag: str) -> list[dict]:
        """Evalchemy ê²°ê³¼ íŒŒì‹±."""
        parsed_results = []

        if "evalchemy_results" not in results:
            return parsed_results

        evalchemy_data = results["evalchemy_results"]
        timestamp = datetime.utcnow()

        # ë²¤ì¹˜ë§ˆí¬ë³„ ê²°ê³¼ íŒŒì‹±
        for benchmark_name, benchmark_data in evalchemy_data.get("benchmarks", {}).items():
            if isinstance(benchmark_data, dict):
                for metric_name, metric_value in benchmark_data.items():
                    if isinstance(metric_value, (int, float)):
                        parsed_results.append(
                            {
                                "run_id": run_id,
                                "model_tag": model_tag,
                                "metric_name": f"{benchmark_name}_{metric_name}",
                                "metric_value": float(metric_value),
                                "metric_category": "benchmark",
                                "evaluation_framework": "evalchemy",
                                "dataset_name": benchmark_name,
                                "dataset_version": evalchemy_data.get("dataset_versions", {}).get(
                                    benchmark_name, "v1.0.0"
                                ),
                                "timestamp": timestamp,
                                "metadata": json.dumps(
                                    {
                                        "shots": evalchemy_data.get("shots", 0),
                                        "batch_size": evalchemy_data.get("batch_size", 1),
                                        "model_args": evalchemy_data.get("model_args", {}),
                                    }
                                ),
                            }
                        )

        logger.info(f"Evalchemy ê²°ê³¼ íŒŒì‹± ì™„ë£Œ: {len(parsed_results)}ê°œ ë©”íŠ¸ë¦­")
        return parsed_results

    def _get_metric_category(self, metric_name: str) -> str:
        """ë©”íŠ¸ë¦­ ì´ë¦„ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ."""
        category_mapping = {
            "rag_precision": "rag",
            "rag_recall": "rag",
            "rag_f1": "rag",
            "hallucination_rate": "hallucination",
            "context_relevance": "context",
            "answer_accuracy": "accuracy",
            "faithfulness": "faithfulness",
        }

        for key, category in category_mapping.items():
            if key in metric_name.lower():
                return category

        return "general"

    def store_results(self, results: list[dict]) -> None:
        """ClickHouseì— ê²°ê³¼ ì €ì¥."""
        if not self.clickhouse_client:
            logger.warning("ClickHouse í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        if not results:
            logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            # ë°°ì¹˜ ì‚½ì…
            self.clickhouse_client.execute(
                "INSERT INTO vllm_eval.results VALUES",
                [
                    (
                        r["run_id"],
                        r["model_tag"],
                        r["metric_name"],
                        r["metric_value"],
                        r["metric_category"],
                        r["evaluation_framework"],
                        r["dataset_name"],
                        r["dataset_version"],
                        r["timestamp"],
                        r["metadata"],
                    )
                    for r in results
                ],
            )
            logger.info(f"ClickHouseì— {len(results)}ê°œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ClickHouse ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_historical_results(
        self, model_tag: str, limit: int = DEFAULT_HISTORICAL_LIMIT
    ) -> list[dict]:
        """ê³¼ê±° ê²°ê³¼ ì¡°íšŒ."""
        if not self.clickhouse_client:
            return []

        query = """
        SELECT metric_name, AVG(metric_value) as avg_value
        FROM vllm_eval.results
        WHERE model_tag = %(model_tag)s
        AND timestamp >= now() - INTERVAL 30 DAY
        GROUP BY metric_name
        ORDER BY metric_name
        LIMIT %(limit)s
        """

        try:
            results = self.clickhouse_client.execute(
                query, {"model_tag": model_tag, "limit": limit}
            )
            return [{"metric_name": row[0], "avg_value": row[1]} for row in results]
        except Exception as e:
            logger.error(f"ê³¼ê±° ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def detect_regression(self, current_results: list[dict], model_tag: str) -> list[dict]:
        """í’ˆì§ˆ í‡´í™” íƒì§€ (ìµœê·¼ NíšŒ Rolling Mean ëŒ€ë¹„ 10% í•˜ë½)."""
        regressions = []
        historical_results = self.get_historical_results(model_tag)

        # ì´ë ¥ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        historical_dict = {
            result["metric_name"]: result["avg_value"] for result in historical_results
        }

        for current in current_results:
            metric_name = current["metric_name"]
            current_value = current["metric_value"]

            if metric_name in historical_dict:
                historical_avg = historical_dict[metric_name]
                decline_rate = (historical_avg - current_value) / historical_avg

                if decline_rate > REGRESSION_THRESHOLD:  # 10% ì´ìƒ í•˜ë½
                    regressions.append(
                        {
                            "metric_name": metric_name,
                            "current_value": current_value,
                            "historical_avg": historical_avg,
                            "decline_rate": decline_rate * 100,
                            "severity": (
                                "high" if decline_rate > SIGNIFICANT_CHANGE_THRESHOLD else "medium"
                            ),
                        }
                    )

        if regressions:
            logger.warning(f"í’ˆì§ˆ í‡´í™” ê°ì§€: {len(regressions)}ê°œ ë©”íŠ¸ë¦­")

        return regressions

    def send_teams_notification(
        self, run_id: str, model_tag: str, results_summary: dict, regressions: list[dict]
    ) -> None:
        """Microsoft Teamsì— ì•Œë¦¼ ì „ì†¡."""
        if not self.teams_webhook_url:
            logger.info("Teams Webhook URLì´ ì—†ì–´ ì•Œë¦¼ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # Adaptive Card í…œí”Œë¦¿
        card_template = Template(
            """
        {
            "type": "message",
            "attachments": [
                {
                    "contentType": "application/vnd.microsoft.card.adaptive",
                    "content": {
                        "type": "AdaptiveCard",
                        "body": [
                            {
                                "type": "TextBlock",
                                "size": "Medium",
                                "weight": "Bolder",
                                "text": "ğŸš€ VLLM í‰ê°€ ì™„ë£Œ: {{ model_tag }}"
                            },
                            {
                                "type": "FactSet",
                                "facts": [
                                    {
                                        "title": "Run ID",
                                        "value": "{{ run_id }}"
                                    },
                                    {
                                        "title": "ì‹¤í–‰ ì‹œê°„",
                                        "value": "{{ execution_time }}"
                                    },
                                    {
                                        "title": "ì´ ë©”íŠ¸ë¦­",
                                        "value": "{{ total_metrics }}"
                                    },
                                    {
                                        "title": "ìƒíƒœ",
                                        "value": "{{ status }}"
                                    }
                                ]
                            }
                            {% if regressions %}
                            ,{
                                "type": "TextBlock",
                                "size": "Medium",
                                "weight": "Bolder",
                                "text": "âš ï¸ í’ˆì§ˆ í‡´í™” ê°ì§€",
                                "color": "Attention"
                            }
                            {% for regression in regressions %}
                            ,{
                                "type": "FactSet",
                                "facts": [
                                    {
                                        "title": "ë©”íŠ¸ë¦­",
                                        "value": "{{ regression.metric_name }}"
                                    },
                                    {
                                        "title": "í˜„ì¬ê°’",
                                        "value": "{{ regression.current_value | round(3) }}"
                                    },
                                    {
                                        "title": "ì´ì „ í‰ê· ",
                                        "value": "{{ regression.historical_avg | round(3) }}"
                                    },
                                    {
                                        "title": "í•˜ë½ë¥ ",
                                        "value": "{{ regression.decline_rate | round(1) }}%"
                                    }
                                ]
                            }
                            {% endfor %}
                            {% endif %}
                        ],
                        "actions": [
                            {
                                "type": "Action.OpenUrl",
                                "title": "Grafana ëŒ€ì‹œë³´ë“œ",
                                "url": "{{ grafana_url }}"
                            },
                            {
                                "type": "Action.OpenUrl",
                                "title": "Argo Workflows",
                                "url": "{{ argo_url }}"
                            }
                        ],
                        "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                        "version": "1.2"
                    }
                }
            ]
        }
        """
        )

        # í…œí”Œë¦¿ ë³€ìˆ˜ ì¤€ë¹„
        template_vars = {
            "run_id": run_id[:RUN_ID_DISPLAY_LENGTH],  # ì§§ê²Œ í‘œì‹œ
            "model_tag": model_tag,
            "execution_time": results_summary.get("execution_time", "N/A"),
            "total_metrics": results_summary.get("total_metrics", 0),
            "status": "âš ï¸ í‡´í™” ê°ì§€" if regressions else "âœ… ì„±ê³µ",
            "regressions": regressions,
            "grafana_url": "http://grafana.company.com/d/vllm-eval",
            "argo_url": "http://argo.company.com/workflows",
        }

        # Adaptive Card ìƒì„±
        try:
            card_json = card_template.render(**template_vars)
            card_data = json.loads(card_json)

            # Teamsì— ì „ì†¡
            response = requests.post(self.teams_webhook_url, json=card_data, timeout=HTTP_TIMEOUT)

            if response.status_code == 200:
                logger.info("Teams ì•Œë¦¼ ì „ì†¡ ì„±ê³µ")
            else:
                logger.error(f"Teams ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")

        except Exception as e:
            logger.error(f"Teams ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")

    def aggregate(
        self,
        deepeval_results_file: str,
        evalchemy_results_file: str,
        model_tag: str,
        run_id: Optional[str] = None,
    ) -> None:
        """ì „ì²´ ì§‘ê³„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰."""
        if not run_id:
            run_id = str(uuid.uuid4())

        logger.info(f"ë©”íŠ¸ë¦­ ì§‘ê³„ ì‹œì‘: {run_id}")

        # í…Œì´ë¸” í™•ì¸/ìƒì„±
        self.ensure_table_exists()

        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        all_results = {}

        deepeval_path = Path(deepeval_results_file)
        if deepeval_path.exists():
            deepeval_results = self.load_evaluation_results(deepeval_results_file)
            all_results["deepeval_results"] = deepeval_results

        evalchemy_path = Path(evalchemy_results_file)
        if evalchemy_path.exists():
            evalchemy_results = self.load_evaluation_results(evalchemy_results_file)
            all_results["evalchemy_results"] = evalchemy_results

        # ê²°ê³¼ íŒŒì‹±
        parsed_results = []
        parsed_results.extend(self.parse_deepeval_results(all_results, run_id, model_tag))
        parsed_results.extend(self.parse_evalchemy_results(all_results, run_id, model_tag))

        # ClickHouseì— ì €ì¥
        self.store_results(parsed_results)

        # í’ˆì§ˆ í‡´í™” ê°ì§€
        regressions = self.detect_regression(parsed_results, model_tag)

        # ê²°ê³¼ ìš”ì•½
        results_summary = {
            "run_id": run_id,
            "model_tag": model_tag,
            "total_metrics": len(parsed_results),
            "execution_time": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
            "regressions_count": len(regressions),
        }

        # Teams ì•Œë¦¼ ì „ì†¡
        self.send_teams_notification(run_id, model_tag, results_summary, regressions)

        logger.info(f"ë©”íŠ¸ë¦­ ì§‘ê³„ ì™„ë£Œ: {run_id}")

        # ê²°ê³¼ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ìš©)
        return {
            "results_summary": results_summary,
            "parsed_results": parsed_results,
            "regressions": regressions,
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description="í‰ê°€ ê²°ê³¼ ì§‘ê³„ ë° ì €ì¥")
    parser.add_argument("--deepeval-results", required=True, help="Deepeval ê²°ê³¼ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--evalchemy-results", required=True, help="Evalchemy ê²°ê³¼ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model-tag", required=True, help="ëª¨ë¸ íƒœê·¸ (ì˜ˆ: release-0.3.1)")
    parser.add_argument("--run-id", help="ì‹¤í–‰ ID (ë¯¸ì§€ì •ì‹œ ìë™ ìƒì„±)")
    parser.add_argument("--clickhouse-host", default="localhost", help="ClickHouse í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--clickhouse-port", type=int, default=9000, help="ClickHouse í¬íŠ¸")
    parser.add_argument("--teams-webhook-url", help="Teams Webhook URL")

    args = parser.parse_args()

    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Teams Webhook URL ê°€ì ¸ì˜¤ê¸°
    teams_webhook_url = args.teams_webhook_url or os.getenv("TEAMS_WEBHOOK_URL")

    try:
        aggregator = MetricsAggregator(
            clickhouse_host=args.clickhouse_host,
            clickhouse_port=args.clickhouse_port,
            teams_webhook_url=teams_webhook_url,
        )

        result = aggregator.aggregate(
            deepeval_results_file=args.deepeval_results,
            evalchemy_results_file=args.evalchemy_results,
            model_tag=args.model_tag,
            run_id=args.run_id,
        )

        # ê²°ê³¼ ì¶œë ¥
        print(json.dumps(result["results_summary"], indent=2))

    except Exception as e:
        logger.error(f"ì§‘ê³„ ì‹¤íŒ¨: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
