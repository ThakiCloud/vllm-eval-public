#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ Evalchemy ë²¤ì¹˜ë§ˆí¬ ëŒ€ì•ˆ (ì§ì ‘ API í˜¸ì¶œ ë°©ì‹)
"""

import json
import logging
import os
import re
import time
from typing import Any

import requests

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleEvalchemy:
    """ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í´ë˜ìŠ¤"""

    def __init__(self, base_url: str, model_name: str):
        self.base_url = base_url
        self.model_name = model_name
        self.session = requests.Session()

    def call_model(self, prompt: str, max_tokens: int = 50) -> str:
        """VLLM API í˜¸ì¶œ"""
        try:
            response = self.session.post(
                f"{self.base_url}/chat/completions",
                headers={"Content-Type": "application/json"},
                json={
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": max_tokens,
                    "stream": False,
                },
                timeout=30,
            )

            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
            else:
                logger.error(f"API Error: {response.status_code} - {response.text}")
                return ""

        except Exception as e:
            logger.error(f"Request failed: {e}")
            return ""

    def extract_answer(self, response: str) -> str:
        """ëª¨ë¸ ì‘ë‹µì—ì„œ A, B, C, D ë‹µë³€ ì¶”ì¶œ"""
        # deepseek-r1 ëª¨ë¸ì˜ ê²½ìš° <think> íƒœê·¸ ì´í›„ì˜ ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ ì°¾ê¸°
        if "</think>" in response:
            # </think> ì´í›„ì˜ ë‚´ìš©ì—ì„œ ë‹µë³€ ì¶”ì¶œ
            after_think = response.split("</think>")[-1].strip()
            if after_think:
                response = after_think

        # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ë¡œ ë‹µë³€ ì¶”ì¶œ
        # íŒ¨í„´ 1: "ë‹µ: A" ë˜ëŠ” "ë‹µë³€: B" ë“±
        answer_pattern = r"ë‹µ\s*:\s*([A-D])"
        match = re.search(answer_pattern, response)
        if match:
            return match.group(1)

        # íŒ¨í„´ 2: "ì •ë‹µì€ A" ë˜ëŠ” "ë‹µì€ B" ë“±
        answer_pattern2 = r"(?:ì •ë‹µ|ë‹µ)ì€?\s*([A-D])"
        match = re.search(answer_pattern2, response)
        if match:
            return match.group(1)

        # íŒ¨í„´ 3: ë§ˆì§€ë§‰ì— ë‚˜ì˜¤ëŠ” A, B, C, D
        letters = re.findall(r"\b([A-D])\b", response)
        if letters:
            return letters[-1]  # ë§ˆì§€ë§‰ì— ë‚˜ì˜¤ëŠ” ê²ƒì„ ë‹µìœ¼ë¡œ ê°„ì£¼

        # íŒ¨í„´ 4: ì²« ë²ˆì§¸ë¡œ ë‚˜ì˜¤ëŠ” A, B, C, D
        for char in response.upper():
            if char in ["A", "B", "C", "D"]:
                return char

        return ""  # ë‹µë³€ì„ ì°¾ì§€ ëª»í•¨

    def run_arc_easy_style_test(self) -> dict[str, Any]:
        """ARC Easy ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        questions = [
            {
                "question": "ë‹¤ìŒ ì¤‘ ì§€êµ¬ì˜ ìœ„ì„±ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
                "choices": ["A) íƒœì–‘", "B) ë‹¬", "C) í™”ì„±", "D) ê¸ˆì„±"],
                "correct": "B",
                "id": "q1",
            },
            {
                "question": "ë¬¼ì˜ í™”í•™ ê¸°í˜¸ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?",
                "choices": ["A) H2O", "B) CO2", "C) O2", "D) NaCl"],
                "correct": "A",
                "id": "q2",
            },
            {
                "question": "1 + 1ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
                "choices": ["A) 1", "B) 2", "C) 3", "D) 4"],
                "correct": "B",
                "id": "q3",
            },
            {
                "question": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?",
                "choices": ["A) ë¶€ì‚°", "B) ì„œìš¸", "C) ëŒ€êµ¬", "D) ê´‘ì£¼"],
                "correct": "B",
                "id": "q4",
            },
            {
                "question": "ì»´í“¨í„°ì˜ CPUëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•©ë‹ˆê¹Œ?",
                "choices": ["A) ì¤‘ì•™ì²˜ë¦¬ì¥ì¹˜", "B) í•˜ë“œë””ìŠ¤í¬", "C) ë©”ëª¨ë¦¬", "D) ëª¨ë‹ˆí„°"],
                "correct": "A",
                "id": "q5",
            },
        ]

        results = []
        correct_count = 0

        print("ğŸ§ª ARC Easy ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")

        for i, q in enumerate(questions):
            print(f"\n--- ì§ˆë¬¸ {i+1}/{len(questions)} ---")
            print(f"ì§ˆë¬¸: {q['question']}")

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë” ëª…í™•í•œ ì§€ì‹œ)
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ì ì ˆí•œ ë‹µì„ A, B, C, D ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {q['question']}

ì„ íƒì§€:
{chr(10).join(q['choices'])}

ìœ„ ì„ íƒì§€ ì¤‘ì—ì„œ ì •ë‹µì„ A, B, C, D ì¤‘ í•˜ë‚˜ì˜ ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”.
ë‹µ:"""

            # ëª¨ë¸ í˜¸ì¶œ
            response = self.call_model(prompt, max_tokens=100)
            print(f"ëª¨ë¸ ì‘ë‹µ: {response}")

            # ë‹µë³€ ì¶”ì¶œ
            predicted = self.extract_answer(response)

            # ì •ë‹µ í™•ì¸
            is_correct = predicted == q["correct"]
            if is_correct:
                correct_count += 1
                status = "âœ…"
            else:
                status = "âŒ"

            print(f"ì˜ˆì¸¡: {predicted}, ì •ë‹µ: {q['correct']} {status}")

            results.append(
                {
                    "question_id": q["id"],
                    "question": q["question"],
                    "choices": q["choices"],
                    "correct_answer": q["correct"],
                    "predicted_answer": predicted,
                    "model_response": response,
                    "is_correct": is_correct,
                }
            )

            time.sleep(1)  # API í˜¸ì¶œ ê°„ê²©

        accuracy = correct_count / len(questions)

        return {
            "task": "arc_easy_style",
            "total_questions": len(questions),
            "correct_answers": correct_count,
            "accuracy": accuracy,
            "details": results,
        }

    def run_hellaswag_style_test(self) -> dict[str, Any]:
        """HellaSwag ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        scenarios = [
            {
                "context": "ì‚¬ëŒì´ ê¸¸ì„ ê±·ê³  ìˆë‹¤",
                "endings": [
                    "A) ê°‘ìê¸° ë‚ ì•„ê°”ë‹¤",
                    "B) ê³„ì†í•´ì„œ ì•ìœ¼ë¡œ ê±¸ì—ˆë‹¤",
                    "C) ë°”ë‹¥ì— ë¿Œë¦¬ë¥¼ ë‚´ë ¸ë‹¤",
                    "D) íˆ¬ëª…í•´ì¡Œë‹¤",
                ],
                "correct": "B",
                "id": "h1",
            },
            {
                "context": "í•™ìƒì´ ì±…ì„ ì½ê³  ìˆë‹¤",
                "endings": [
                    "A) ì±…ì´ ë§ì„ í•˜ê¸° ì‹œì‘í–ˆë‹¤",
                    "B) í˜ì´ì§€ë¥¼ ë„˜ê²¼ë‹¤",
                    "C) ì±…ì´ ë‚ ì•„ê°”ë‹¤",
                    "D) í•™ìƒì´ ì‚¬ë¼ì¡Œë‹¤",
                ],
                "correct": "B",
                "id": "h2",
            },
            {
                "context": "ìš”ë¦¬ì‚¬ê°€ ìŒì‹ì„ ì¤€ë¹„í•˜ê³  ìˆë‹¤",
                "endings": [
                    "A) ì¬ë£Œë¥¼ ì†ì§ˆí–ˆë‹¤",
                    "B) ì£¼ë°©ì´ í­ë°œí–ˆë‹¤",
                    "C) ìŒì‹ì´ ì €ì ˆë¡œ ë§Œë“¤ì–´ì¡Œë‹¤",
                    "D) ìš”ë¦¬ì‚¬ê°€ ì¶¤ì„ ì¶”ì—ˆë‹¤",
                ],
                "correct": "A",
                "id": "h3",
            },
        ]

        results = []
        correct_count = 0

        print("\nğŸ§ª HellaSwag ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")

        for i, scenario in enumerate(scenarios):
            print(f"\n--- ì‹œë‚˜ë¦¬ì˜¤ {i+1}/{len(scenarios)} ---")
            print(f"ìƒí™©: {scenario['context']}")

            prompt = f"""ë‹¤ìŒ ìƒí™©ì—ì„œ ê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  í˜„ì‹¤ì ì¸ ë‹¤ìŒ í–‰ë™ì„ A, B, C, D ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.

ìƒí™©: {scenario['context']}

ë‹¤ìŒ í–‰ë™ ì„ íƒì§€:
{chr(10).join(scenario['endings'])}

ìœ„ ì„ íƒì§€ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ê²ƒì„ A, B, C, D ì¤‘ í•˜ë‚˜ì˜ ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”.
ë‹µ:"""

            response = self.call_model(prompt, max_tokens=100)
            print(f"ëª¨ë¸ ì‘ë‹µ: {response}")

            # ë‹µë³€ ì¶”ì¶œ
            predicted = self.extract_answer(response)

            is_correct = predicted == scenario["correct"]
            if is_correct:
                correct_count += 1
                status = "âœ…"
            else:
                status = "âŒ"

            print(f"ì˜ˆì¸¡: {predicted}, ì •ë‹µ: {scenario['correct']} {status}")

            results.append(
                {
                    "scenario_id": scenario["id"],
                    "context": scenario["context"],
                    "endings": scenario["endings"],
                    "correct_answer": scenario["correct"],
                    "predicted_answer": predicted,
                    "model_response": response,
                    "is_correct": is_correct,
                }
            )

            time.sleep(1)

        accuracy = correct_count / len(scenarios)

        return {
            "task": "hellaswag_style",
            "total_scenarios": len(scenarios),
            "correct_answers": correct_count,
            "accuracy": accuracy,
            "details": results,
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê°„ë‹¨í•œ Evalchemy ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    # ì„¤ì •
    base_url = os.getenv("VLLM_MODEL_ENDPOINT", "http://localhost:1234/v1")
    output_dir = os.getenv("OUTPUT_DIR", "./test_results")

    # ëª¨ë¸ í™•ì¸
    try:
        response = requests.get(f"{base_url}/models", timeout=5)
        models_data = response.json()
        available_models = [model["id"] for model in models_data.get("data", [])]

        if not available_models:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        model_name = available_models[0]
        print(f"âœ… ì‚¬ìš© ëª¨ë¸: {model_name}")

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    evalchemy = SimpleEvalchemy(base_url, model_name)

    # ARC Easy ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸
    arc_results = evalchemy.run_arc_easy_style_test()

    # HellaSwag ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸
    hellaswag_results = evalchemy.run_hellaswag_style_test()

    # ê²°ê³¼ í†µí•©
    final_results = {
        "model": model_name,
        "server_url": base_url,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "benchmarks": {"arc_easy_style": arc_results, "hellaswag_style": hellaswag_results},
        "summary": {
            "arc_easy_accuracy": arc_results["accuracy"],
            "hellaswag_accuracy": hellaswag_results["accuracy"],
            "overall_accuracy": (arc_results["accuracy"] + hellaswag_results["accuracy"]) / 2,
        },
    }

    # ê²°ê³¼ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    results_file = f"{output_dir}/simple_evalchemy_results.json"

    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 50)
    print(f"ëª¨ë¸: {model_name}")
    print(
        f"ARC Easy ìŠ¤íƒ€ì¼ ì •í™•ë„: {arc_results['accuracy']:.1%} ({arc_results['correct_answers']}/{arc_results['total_questions']})"
    )
    print(
        f"HellaSwag ìŠ¤íƒ€ì¼ ì •í™•ë„: {hellaswag_results['accuracy']:.1%} ({hellaswag_results['correct_answers']}/{hellaswag_results['total_scenarios']})"
    )
    print(f"ì „ì²´ í‰ê·  ì •í™•ë„: {final_results['summary']['overall_accuracy']:.1%}")
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")


if __name__ == "__main__":
    main()
