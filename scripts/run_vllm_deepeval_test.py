#!/usr/bin/env python3
"""
ì‹¤ì œ VLLM ì„œë²„ì™€ ì—°ë™í•˜ëŠ” Deepeval í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import logging
import os
from typing import Optional

import requests
from deepeval.models import DeepEvalBaseLLM

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VLLMModel(DeepEvalBaseLLM):
    """VLLM OpenAI í˜¸í™˜ API ëª¨ë¸"""

    def __init__(self, base_url: str = "http://localhost:8000/v1", model_name: str = "qwen3-8b"):
        self.base_url = base_url
        self.model_name = model_name

    def load_model(self):
        return self.model_name

    def generate(self, prompt: str, schema=None) -> str:  # noqa: ARG002
        """VLLM APIë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers={"Content-Type": "application/json"},
                json={
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 150,
                    "stream": False,
                },
                timeout=30,
            )

            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
            else:
                logger.error(f"API Error: {response.status_code} - {response.text}")
                return "API í˜¸ì¶œ ì‹¤íŒ¨"

        except Exception as e:
            logger.error(f"Request failed: {e}")
            return "ìš”ì²­ ì‹¤íŒ¨"

    async def a_generate(self, prompt: str, schema=None) -> str:
        return self.generate(prompt, schema)

    def get_model_name(self) -> str:
        return self.model_name


def check_vllm_server(base_url: str) -> bool:
    """VLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        health_url = base_url.replace("/v1", "/health")
        response = requests.get(health_url, timeout=5)
        return response.status_code == 200
    except requests.RequestException:
        return False


def load_test_questions(file_path: Optional[str] = None):
    """í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ"""
    if file_path and os.path.exists(file_path):
        with open(file_path, encoding="utf-8") as f:
            return [json.loads(line) for line in f if line.strip()]

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    return [
        {
            "input": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "expected_keywords": ["ì„œìš¸"],
            "context": "í•œêµ­ ì§€ë¦¬",
        },
        {
            "input": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?",
            "expected_keywords": ["sort", "sorted"],
            "context": "í”„ë¡œê·¸ë˜ë°",
        },
        {
            "input": "ì§€êµ¬ì˜ ë‘˜ë ˆëŠ” ì–¼ë§ˆë‚˜ ë©ë‹ˆê¹Œ?",
            "expected_keywords": ["40", "075", "km"],
            "context": "ì§€êµ¬ê³¼í•™",
        },
    ]


def evaluate_response(actual: str, expected_keywords: list) -> tuple:
    """ì‘ë‹µ í‰ê°€"""
    actual_lower = actual.lower()
    matches = sum(1 for keyword in expected_keywords if keyword.lower() in actual_lower)
    score = matches / len(expected_keywords) if expected_keywords else 0.0
    return score, matches, len(expected_keywords)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš€ VLLM Deepeval í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # VLLM ì„œë²„ ì„¤ì •
    base_url = os.getenv("VLLM_MODEL_ENDPOINT", "http://localhost:8000/v1")
    model_name = os.getenv("MODEL_NAME", "qwen3-8b")

    # ì„œë²„ ìƒíƒœ í™•ì¸
    print(f"ğŸ“¡ VLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘... ({base_url})")
    if not check_vllm_server(base_url):
        print("âŒ VLLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”:")
        print("docker run -d --name vllm-server --gpus all -p 8000:8000 \\")
        print("  vllm/vllm-openai:latest --model Qwen/Qwen2-7B-Instruct \\")
        print(f"  --served-model-name {model_name}")
        return

    print("âœ… VLLM ì„œë²„ ì—°ê²° ì„±ê³µ")

    # ëª¨ë¸ ìƒì„±
    model = VLLMModel(base_url, model_name)

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ
    test_questions = load_test_questions("eval/deepeval_tests/datasets/test_local_dataset.jsonl")

    # ê²°ê³¼ ì €ì¥
    output_dir = os.getenv("OUTPUT_DIR", "./test_results")
    os.makedirs(output_dir, exist_ok=True)

    results = {
        "test_results": [],
        "summary": {
            "total_tests": len(test_questions),
            "model_name": model.get_model_name(),
            "server_url": base_url,
            "status": "success",
        },
    }

    print(f"\nğŸ§ª {len(test_questions)}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")

    total_score = 0.0
    for i, question in enumerate(test_questions):
        print(f"\n--- Test {i+1}/{len(test_questions)} ---")
        print(f"ì§ˆë¬¸: {question['input']}")

        # VLLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        actual_output = model.generate(question["input"])
        print(f"ì‘ë‹µ: {actual_output}")

        # í‰ê°€
        if "expected_keywords" in question:
            score, matches, total_keywords = evaluate_response(
                actual_output, question["expected_keywords"]
            )
            print(f"í‰ê°€: {matches}/{total_keywords} í‚¤ì›Œë“œ ë§¤ì¹˜ (ì ìˆ˜: {score:.2f})")
        else:
            # expected_outputê³¼ ë‹¨ìˆœ ë¹„êµ
            expected = question.get("expected_output", "")
            score = 1.0 if expected.lower() in actual_output.lower() else 0.5
            print(f"í‰ê°€: ìœ ì‚¬ë„ ì ìˆ˜ {score:.2f}")

        total_score += score

        # ê²°ê³¼ ì €ì¥
        results["test_results"].append(
            {
                "test_id": i + 1,
                "input": question["input"],
                "actual_output": actual_output,
                "expected_info": question.get(
                    "expected_keywords", question.get("expected_output", "")
                ),
                "score": score,
                "context": question.get("context", ""),
            }
        )

    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    avg_score = total_score / len(test_questions)
    results["summary"]["average_score"] = avg_score
    results["summary"]["total_score"] = total_score

    # JSON íŒŒì¼ë¡œ ì €ì¥
    results_file = f"{output_dir}/vllm_deepeval_results.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")

    # ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  ì´ í…ŒìŠ¤íŠ¸: {len(test_questions)}")
    print(f"  í‰ê·  ì ìˆ˜: {avg_score:.2f}")
    print(f"  ì´í•© ì ìˆ˜: {total_score:.2f}/{len(test_questions)}")
    print(f"  ì„±ê³µë¥ : {(avg_score * 100):.1f}%")

    # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
    print("\nğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for result in results["test_results"]:
        status = "âœ…" if result["score"] >= 0.7 else "âš ï¸" if result["score"] >= 0.4 else "âŒ"
        print(f"  Test {result['test_id']}: {result['score']:.2f} {status}")

    return results


if __name__ == "__main__":
    main()
